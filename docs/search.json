[
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Blog",
    "section": "",
    "text": "Gestire file CSV grandi, brutti e cattivi\n\n\n\n\n\n\n\nduckdb\n\n\ncsv\n\n\nparquet\n\n\n\n\nTips & tricks, ispirati da DuckDB, file Parquet e OpenCoesione\n\n\n\n\n\n\n21 ago 2023\n\n\nAndrea Borruso\n\n\n\n\n\n\nNessun risultato\n\n Torna in cima"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Andrea Borruso",
    "section": "",
    "text": "Ciao\nGrazie per essere qui. Questo è uno spazio dove inserirò alcuni appunti sulle cose che imparo, sui progetti che mi piacciono e/o che sto sviluppando, sugli strumenti con cui lavoro e gioco e sulle persone che incontro.\nSono socio dell’associazione onData.\n\n\nQuando hai dato lo stesso consiglio 3 volte, scrivi un post (David Robinson).\n\n\n\n\n Torna in cima"
  },
  {
    "objectID": "posts/duckdb-intro-csv/index.html",
    "href": "posts/duckdb-intro-csv/index.html",
    "title": "Gestire file CSV grandi, brutti e cattivi",
    "section": "",
    "text": "Il formato CSV, con tutti i suoi difetti, è ancora uno dei formati più diffusi per lo scambio di dati. Ci sono modalità di gestire questi file ed eccezionali strumenti, che possono rendere molto semplice, efficace e rapido il loro utilizzo. Anche quando sono di grandi dimensioni. E per fortuna ci sono banche dati “grosse” e importanti come OpenCoesione che danno l’opportunità di fare pratica con questi strumenti.\nIn questo lungo post farò una carrelata della compressione dei file CSV, dell’importanza di descriverli, dell’utilizzo di DuckDB per analizzarli e del formato Parquet come alternativa al CSV.\n⚠️ QUESTO POST NON È UN TUTORIAL (ma doping omeopatico per i tuoi criceti e per chi pubblica dati).\n\n\n\n\n\n\n\n\nPrima di continuare a leggere\n\n\n\nIn questo post faccio riferimento a diversi strumenti, tra cui DuckDB, Miller e VisiData. Se vuoi replicare alcune delle procedere descritte, è necessario installarli. Soprattutto DuckDB."
  },
  {
    "objectID": "posts/duckdb-intro-csv/index.html#tldr-una-breve-introduzione",
    "href": "posts/duckdb-intro-csv/index.html#tldr-una-breve-introduzione",
    "title": "Gestire file CSV grandi, brutti e cattivi",
    "section": "",
    "text": "Il formato CSV, con tutti i suoi difetti, è ancora uno dei formati più diffusi per lo scambio di dati. Ci sono modalità di gestire questi file ed eccezionali strumenti, che possono rendere molto semplice, efficace e rapido il loro utilizzo. Anche quando sono di grandi dimensioni. E per fortuna ci sono banche dati “grosse” e importanti come OpenCoesione che danno l’opportunità di fare pratica con questi strumenti.\nIn questo lungo post farò una carrelata della compressione dei file CSV, dell’importanza di descriverli, dell’utilizzo di DuckDB per analizzarli e del formato Parquet come alternativa al CSV.\n⚠️ QUESTO POST NON È UN TUTORIAL (ma doping omeopatico per i tuoi criceti e per chi pubblica dati).\n\n\n\n\n\n\n\n\nPrima di continuare a leggere\n\n\n\nIn questo post faccio riferimento a diversi strumenti, tra cui DuckDB, Miller e VisiData. Se vuoi replicare alcune delle procedere descritte, è necessario installarli. Soprattutto DuckDB."
  },
  {
    "objectID": "posts/duckdb-intro-csv/index.html#i-csv-sono-brutti-e-cattivi",
    "href": "posts/duckdb-intro-csv/index.html#i-csv-sono-brutti-e-cattivi",
    "title": "Gestire file CSV grandi, brutti e cattivi",
    "section": "I CSV sono brutti e cattivi",
    "text": "I CSV sono brutti e cattivi\nIl formato CSV è uno dei formati più diffusi per lo scambio di dati. È un formato testuale, che può essere letto e scritto da quasi tutti i linguaggi di programmazione e da quasi tutti gli strumenti di analisi dati (i “quasi” si potrebbero levare). Non è consigliabile utilizzarlo come formato di lavoro, perché le operazioni di lettura e scrittura sono lente e costose in termini di risorse (tempo e memoria) e perché è in generale un formato “povero”.\nUn file CSV è “brutto e cattivo” ad esempio per queste ragioni:\n\nnon consente di definire il tipo di campo (stringa, numero, data, ecc.). Tipicamente si è costretti a fare l’inferenza, che può essere errata;\nnon consente di definire il separatore di campo (virgola, punto e virgola, tabulazione, ecc.). Si può fare l’inferenza e/o si può leggere (e si possono fare errori);\nnon consente di definire il separatore dei decimali (virgola, punto, ecc.). Si può fare l’inferenza e/o si può leggere (e si possono fare errori);\nnon consente di definire l’encoding (UTF-8, ISO-8859-1, ecc.). Tipicamente si è costretti a fare l’inferenza, che può essere errata.\n\nE per questo chi pubblica dati in questo formato, dovrebbe a maggior forza descriverli con dei metadati. Ma purtroppo nella gran parte dei casi, non ci resta che “morire di inferenza”.\nE si potrebbero aggiungere altre brutture sul formato e di come spesso viene reso disponibile. Ma non è il tema del post."
  },
  {
    "objectID": "posts/duckdb-intro-csv/index.html#compressione-dei-file-csv",
    "href": "posts/duckdb-intro-csv/index.html#compressione-dei-file-csv",
    "title": "Gestire file CSV grandi, brutti e cattivi",
    "section": "Compressione dei file CSV",
    "text": "Compressione dei file CSV\nMolti siti che pubblicano file CSV - specie quando sono “grandi” - li pubblicano in formato compresso. Questo - come nel caso del file dei progetti di OpenCoesione - è un ottimo modo per ridurre il tempo di download: il file reso disponibile in formato compresso ZIP pesa circa (nella versione di aprile 2023) 240 MB, mentre il file CSV contenuto al suo interno pesa circa 4,4 GB.\nScegliendo un altro formato di compressione, diverso dallo ZIP, si possono dare ulteriori vantaggi a chi utilizzerà il file.\nUno è quello di renderlo subito pronto all’uso, come se fosse già decompresso. Che è una cosa molto comoda specie nelle prime fasi di lavoro “esplorative”, in cui si fanno un po’ di “ispezioni”, le prime prove di analisi, di verifica qualità, di adeguatezza dei dati, ecc..\n\nFormato GZIP\nÈ un formato introdotto nel 1992, che ha una buona compressione e che è veloce da decomprimere. È un formato lossless, non perde informazioni, e streaming, può essere decompresso man mano che i dati sono letti, senza dover attendere il completamento dell’estrazione dell’intero archivio. La gran parte delle applicazioni e linguaggi di scripting sono in grado di accedere nativamente a un file gz; un po’ meno con il formato zip. Ed è compatibile con tutti i sistemi operativi e tutte le utility di compressione e decompressione.\nA seguire alcuni esempi di quanto è pronto all’uso, basati sul CSV dei Progetti del PNRR, compresso da me in formato gz.\n\n\n\n\n\n\nAmbiente di lavoro utilizzato\n\n\n\n\n\nLa gran parte degli esempi di comandi e di codice inseriti in questo articolo, sono pensati per essere eseguiti in ambiente Linux. Sono replicabili quindi in quasi tutti i sistemi operativi - compresi Mac e Windows - perché Linux o è disponibile “nativamente”, o lo è installando applicativi (su Windows ad esempio Windows Subsystem for Linux, WSL).\n\n\n\nLo posso esplorare con l’utility zcat, che “stampa” sulla shell il contenuto del file gz:\nzcat PNRR_Progetti-Universo_REGIS_v2.1.csv.gz | head -n 5\n\n\nhead estrae le prime 5 righe.\nMi verrà restituito l’output di Lista 1, e potrò constatare che c’è una riga di intestazione (non è obbligatoria nei CSV), che il separatore di campo è il ;, che il separatore dei decimali è la , e soprattutto farmi un’idea dei contenuti.\n\nLista 1: Prime righe del file\nProgramma;Missione;Descrizione Missione;Componente;Descrizione Componente;ID Misura;Codice Univoco Misura;Descrizione Misura;ID Submisura;Codice CID;Codice Univoco Submisura;Descrizione Submisura;Amministrazione Titolare;Codice Identificativo Procedura di Attivazione;Titolo Procedura;Tipologia Procedura di Attivazione;CUP;Codice Locale Progetto;Stato CUP;CUP Codice Natura;CUP Descrizione Natura;CUP Codice Tipologia;CUP Descrizione Tipologia;CUP Codice Settore;CUP Descrizione Settore;CUP Codice Sottosettore;CUP Descrizione Sottosettore;CUP Codice Categoria;CUP Descrizione Categoria;Titolo Progetto;Sintesi Progetto;Descrizione Tipo Aiuto;Finanziamento - Stato;Finanziamento Stato - FOI;Finanziamento UE (Diverso da PNRR);Finanziamento Regione;Finanziamento Provincia;Finanziamento Comune;Finanziamento Altro Pubblico;Finanziamento Privato;Finanziamento Da Reperire;Finanziamento PNRR;Finanziamento PNC;Altri Fondi;Finanziamento Totale;Finanziamento Totale Pubblico;Finanziamento Totale Pubblico Netto;Soggetto Attuatore;Codice Fiscale Soggetto Attuatore;Flag Progetti in Essere;Data di Estrazione\nPNRR;M1;Digitalizzazione, innovazione, competitività e cultura;M1C1;Digitalizzazione, innovazione e sicurezza nella PA;M1C1I1.2;M1C1I1.02;Abilitazione al cloud per le PA locali;M1C1I1.2;M1C1I1.2;M1C1I1.02.00;Abilitazione al cloud per le PA locali;PCM - DIPARTIM. TRASFORMAZIONE DIGITALE;1000000237;AVVISO AB. CLOUD COMUNI DEL 15/04/22;Bando;G61C22000240006;G61C22000240006;Attivo;02;ACQUISTO O REALIZZAZIONE DI SERVIZI;19;APPLICATIVI E PIATTAFORME WEB;10;SERVIZI PER LA P.A. E PER LA COLLETTIVITA';01;SERVIZI E TECNOLOGIE PER L'INFORMAZIONE E LE COMUNICAZIONI;007;SISTEMI INFORMATIVI PER LA P.A.;1.2. Ab.Cloud Com Cosoleto;MIGRAZIONE AL CLOUD DEI SERVIZI DIGITALI DELL'AMMINISTRAZIONE*TERRITORIO COMUNALE*N. 9 SERVIZI;INTERVENTO CHE NON COSTITUISCE AIUTO DI STATO;0,00;0,00;0,00;0,00;0,00;0,00;0,00;0,00;0,00;47427,00;0,00;0,00;47427,00;47427,00;47427,00;COMUNE DI COSOLETO;01234470803;No;13/06/2023\nPNRR;M1;Digitalizzazione, innovazione, competitività e cultura;M1C1;Digitalizzazione, innovazione e sicurezza nella PA;M1C1I1.2;M1C1I1.02;Abilitazione al cloud per le PA locali;M1C1I1.2;M1C1I1.2;M1C1I1.02.00;Abilitazione al cloud per le PA locali;PCM - DIPARTIM. TRASFORMAZIONE DIGITALE;1000000237;AVVISO AB. CLOUD COMUNI DEL 15/04/22;Bando;F71C22000160006;F71C22000160006;Attivo;02;ACQUISTO O REALIZZAZIONE DI SERVIZI;19;APPLICATIVI E PIATTAFORME WEB;10;SERVIZI PER LA P.A. E PER LA COLLETTIVITA';01;SERVIZI E TECNOLOGIE PER L'INFORMAZIONE E LE COMUNICAZIONI;007;SISTEMI INFORMATIVI PER LA P.A.;1.2. Ab.Cloud Com Bompensiere;MIGRAZIONE AL CLOUD DEI SERVIZI DIGITALI DELL'AMMINISTRAZIONE*TERRITORIO COMUNALE*N. 9 SERVIZI DA MIGRARE;INTERVENTO CHE NON COSTITUISCE AIUTO DI STATO;0,00;0,00;0,00;0,00;0,00;0,00;0,00;0,00;0,00;47427,00;0,00;0,00;47427,00;47427,00;47427,00;COMUNE DI BOMPENSIERE;80005060852;No;13/06/2023\nPNRR;M1;Digitalizzazione, innovazione, competitività e cultura;M1C1;Digitalizzazione, innovazione e sicurezza nella PA;M1C1I1.2;M1C1I1.02;Abilitazione al cloud per le PA locali;M1C1I1.2;M1C1I1.2;M1C1I1.02.00;Abilitazione al cloud per le PA locali;PCM - DIPARTIM. TRASFORMAZIONE DIGITALE;1000000237;AVVISO AB. CLOUD COMUNI DEL 15/04/22;Bando;B61C22000190006;B61C22000190006;Attivo;02;ACQUISTO O REALIZZAZIONE DI SERVIZI;19;APPLICATIVI E PIATTAFORME WEB;10;SERVIZI PER LA P.A. E PER LA COLLETTIVITA';01;SERVIZI E TECNOLOGIE PER L'INFORMAZIONE E LE COMUNICAZIONI;007;SISTEMI INFORMATIVI PER LA P.A.;1.2. Ab.Cloud Com Castelgrande;MIGRAZIONE AL CLOUD DEI SERVIZI DIGITALI DELL'AMMINISTRAZIONE*TERRITORIO COMUNALE*9 SERVIZI DA MIGRARE;INTERVENTO CHE NON COSTITUISCE AIUTO DI STATO;0,00;0,00;0,00;0,00;0,00;0,00;0,00;0,00;0,00;47427,00;0,00;0,00;47427,00;47427,00;47427,00;COMUNE DI CASTELGRANDE;80004060762;No;13/06/2023\nPNRR;M1;Digitalizzazione, innovazione, competitività e cultura;M1C1;Digitalizzazione, innovazione e sicurezza nella PA;M1C1I1.2;M1C1I1.02;Abilitazione al cloud per le PA locali;M1C1I1.2;M1C1I1.2;M1C1I1.02.00;Abilitazione al cloud per le PA locali;PCM - DIPARTIM. TRASFORMAZIONE DIGITALE;1000000237;AVVISO AB. CLOUD COMUNI DEL 15/04/22;Bando;F31C22000020006;F31C22000020006;Attivo;02;ACQUISTO O REALIZZAZIONE DI SERVIZI;19;APPLICATIVI E PIATTAFORME WEB;10;SERVIZI PER LA P.A. E PER LA COLLETTIVITA';01;SERVIZI E TECNOLOGIE PER L'INFORMAZIONE E LE COMUNICAZIONI;007;SISTEMI INFORMATIVI PER LA P.A.;1.2. Ab.Cloud Com Ittireddu;MIGRAZIONE AL CLOUD DEI SERVIZI DIGITALI DELL'AMMINISTRAZIONE*TERRITORIO COMUNALE*N. 9 SERVIZI DA MIGRARE;INTERVENTO CHE NON COSTITUISCE AIUTO DI STATO;0,00;0,00;0,00;0,00;0,00;0,00;0,00;0,00;0,00;47427,00;0,00;0,00;47427,00;47427,00;47427,00;COMUNE DI ITTIREDDU;00283910909;No;13/06/2023\n\nE posso usare in modo diretto strumenti di analisi, trasformazione e filtro di file CSV come lo straordinario Miller. Ad esempio avere restituito il conteggio dei record, i valori nulli di ogni campo e anche i valori distinti. Se voglio farlo soltanto per i campi Missione, Codice Univoco Misura e CUP Codice Natura, utilizzerò il verbo cut e summary, direttamente sul file compresso:\nmlr --csv --ifs \";\" \\\ncut -f Missione,\"Codice Univoco Misura\",\"CUP Codice Natura\" then \\\nsummary -a count,null_count,distinct_count PNRR_Progetti-Universo_REGIS_v2.1.csv.gz\nIn output avrò:\n\n\n\nTabella 1: Output di summary di Miller\n\n\nfield_name\ncount\nnull_count\ndistinct_count\n\n\n\n\nMissione\n197546\n0\n6\n\n\nCodice Univoco Misura\n197546\n0\n109\n\n\nCUP Codice Natura\n197546\n17683\n7\n\n\n\n\n\nE con questo file compresso potrò usare anche strumenti di analisi e trasformazione che mi consentono di eseguire una più “standard” query SQL, come DuckDB. Per avere il totale del finanziamento PNRR per ogni missione, posso usare questo comando:\nduckdb -csv -c '\nSELECT Missione, SUM(\"Finanziamento PNRR\") AS total_PNRR\nFROM  read_csv_auto(\"PNRR_Progetti-Universo_REGIS_v2.1.csv.gz\"\nGROUP BY Missione\n'\n\n\n\n\n\n\nInformazioni utili\n\n\n\nIl parametro -csv è per avere l’output del comando in formato CSV, mentre -c per definire la query SQL. Il FROM ha come input direttamente il file CSV compresso, letto tramite la funzione read_csv_auto di DuckDB.\n\n\nNota bene: la query di sopra non ti funzionerà.\nPerché purtroppo “i CSV sono brutti e cattivi” e senza conoscere il separatore dei campi, il separatore dei decimali, i tipi di campo, sapere se c’è o no la riga di intestazione, ecc., è difficile riuscire a interrogare un CSV, anche con DuckDB. Per questo motivo è molto raccomandato a chi rende disponibili file CSV:\n\npubblicare CSV standard;\ndescriverli.\n\n\n\nSono essenziali allo scopo, le operazioni di esplorazione descritte sopra.\nE allora è meglio (a volte necessario) aggiungere un po’ di parametri al comando precedente (vedi Lista 2): per fare in modo che le colonne possano essere correttamente distinte (fissando con il parametro delim il separatore dei campi), che siano mappati i nomi dei campi (fissando il parametro header), che i numeri decimali siano elaborabili come tali (fissando il parametro decimal_separator e specificando il tipo FLOAT per il campo Finanziamento PNRR).\n\n\ndelim e header sono riconosciuti automaticamente da DuckDB. Sono inseriti a scopo didattico.\n\nLista 2: Query SQL tramite DuckDB su un CSV compresso\nduckdb -csv -c '\nSELECT Missione, SUM(\"Finanziamento PNRR\") AS total_PNRR\nFROM  read_csv_auto(\n    \"PNRR_Progetti-Universo_REGIS_v2.1.csv.gz\",\n1    delim=\";\",\n2    decimal_separator=\",\",\n3    header=True,\n4    types={\"Finanziamento PNRR\":\"FLOAT\"}\n)\nGROUP BY Missione\n'\n\n\n1\n\nimposta il separatore di campi a ;\n\n2\n\nimposta il separatore di decimali a ,\n\n3\n\nla prima riga è la riga di intestazione\n\n4\n\nil campo Finanziamento PNRR è di tipo FLOAT\n\n\n\n\n\n\n\n\nIl vantaggio di un CSV standard\n\n\n\nCon un CSV standard, questi parametri aggiuntivi non sarebbero necessari e si potrebbe usare la prima versione della query SQL.\n\n\nIn output, in mezzo secondo (di fatto, non per dire “in breve tempo”), queste righe di output (per un CSV di circa 200.000 righe per 50 colonne, che è pure compresso):\n\n\nOutput della query SQL\n\n\nMissione\ntotal_PNRR\n\n\n\n\nM1\n19245334466.82313\n\n\nM2\n22688516407.240803\n\n\nM3\n22563735313.390625\n\n\nM4\n20129269029.00844\n\n\nM5\n12998257338.579052\n\n\nM6\n8059988416.008047\n\n\n\n\n\n\nFormato ZSTD\nSi può scegliere un algoritmo di compressione alternativo allo ZIP, anche per ragioni di velocità di decompressione e compressione e per la percentuale di compressione.\nPer questa accoppiata di caratteristiche il formato ZSTD è sempre più diffuso tra le applicazioni e le librerie software di accesso ai dati.\nPer dare qualche numero di confronto, basato sul file CSV da 4 GB di OpenCoesione e sul mio notebook con Pentium i7 di 12esima generazione con 16 GB di RAM:\n\nla compressione (standard) con gzip richiede 47 secondi, e il file compresso di output pesa 253 MB;\nla compressione (standard) con zstd richiede 13 secondi, e il file compresso di output pesa 186 MB;\nla decompressione del file gzip richiede 19 secondi;\nla decompressione del file zstd richiede 5 secondi.\n\nL’utility a riga di comando per gestire i file ZSTD si chiama zstd e si può installare in (quasi) tutti i sistemi operativi. Un’applicazione open source, multipiattaforma, con interfaccia grafica, che supporta questa compressione è PeaZip. Molte applicazioni e librerie software di accesso ai dati, come DuckDB, Apache Arrow e Apache Spark, lo supportano nativamente.\nIn DuckDB, si può leggere un file CSV compresso in formato ZSTD, semplicemente puntando ad esso (l’estensione .zst fa in modo che venga interpretato come file compresso in formato ZSTD):\n\nLista 3: Esempio di conteggio delle righe\nduckdb -csv -c '\nSELECT count(*) numero_righe\nFROM  read_csv_auto(\n        \"PNRR_Progetti-Universo_REGIS_v2.1.csv.zst\",\n        delim=\";\",\n        decimal_separator=\",\",\n        header=True,\n        types={\"Finanziamento PNRR\":\"FLOAT\"}\n)\n'\n\n\n\n\n\n\n\nFile di esempio\n\n\n\nQui un file compresso ZSTD da usare come esempio.\n\n\nRestituisce il numero di righe in circa 0,4 secondi per il CSV (compresso ZSTD) di circa 200.000 righe per 50 colonne, usato nell’esempio di sopra. E in 5 secondi per il CSV (compresso ZSTD) di circa 2.000.000 di righe per 200 colonne di OpenCoesione.\nSi può esplorare analogamente un file CSV compresso ZSTD, “stampandolo” a schermo, con l’utility zstdcat. Per leggere le prime 5 righe del file:\nzstdcat PNRR_Progetti-Universo_REGIS_v2.1.csv.zst | head -n 5\nO sempre in accoppiata con Miller, per avere dei dati di sintesi (in 0.6 secondi) come quelli di Tabella 1:\nzstdcat PNRR_Progetti-Universo_REGIS_v2.1.csv.zst | \\\nmlr --csv --ifs \";\" \\\ncut -f Missione,\"Codice Univoco Misura\",\"CUP Codice Natura\" then \\\nsummary -a count,null_count,distinct_count"
  },
  {
    "objectID": "posts/duckdb-intro-csv/index.html#csv-standard",
    "href": "posts/duckdb-intro-csv/index.html#csv-standard",
    "title": "Gestire file CSV grandi, brutti e cattivi",
    "section": "CSV “standard”",
    "text": "CSV “standard”\n\n\n\n\n\n\nA proposito di “standard”\n\n\n\n\n\nIl formato CSV non è uno standard, ma una convenzione non ufficiale che è stata ampiamente adotatta. È descritta nella RFC 4180.\n\n\n\nQui utilizzo l’aggettivo “standard” per alcune caratteristiche che rendono (nella gran parte dei casi) un CSV “subito pronto”, per essere letto da applicazioni e linguaggi di scripting per l’analisi dei dati. Queste sono quelle consigliate e più tipiche:\n\nUTF-8 come codifica dei caratteri;\n, come separatore di campi;\n. come separatore dei decimali. In Italia il separatore è la ,, e ci può stare usarla, ma usando il . e documentandolo, si mette a disposizione un file che sarà più pronto per una lettura automatica;\npresenza della riga di intestazione (una sola);\ndate nello standard ISO 8601 (ad esempio la data “8 marzo 2023” rappresentata come “2023-03-08”);\nogni colonna un solo tipo di campo (es. solo numeri, solo testo, solo date, ecc.). Questo sembra inutile evidenziarlo, ma capita spesso di trovare colonne che contengono valori di tipo diverso (es. numeri e testo) nella stessa colonna;\nevitare l’utilizzo di spazi, virgolette o altri caratteri speciali nei nomi dei campi;\nnon inserire il separatore delle migliaia nei valori delle celle dei campi numerici.\n\n\n\n\n\n\n\nLinee Guida open data\n\n\n\nDiversi di questi punti sono raccomandati anche nelle “Linee Guida recanti regole tecniche per l’apertura dei dati e il riutilizzo dell’informazione del settore pubblico”, purtroppo al momento leggibili soltanto in PDF.\n\n\n\nStandardizzare il CSV di OpenCoesione\nNella sezione sui dati di OpenCoesione (leggila prima di proseguire) è riportato come sia necessario descrivere il file CSV descritto (quello dei progetti), per poterlo leggere al meglio con qualsiasi applicazione. È necessario farlo, perché ha alcune problematiche e perché non è appunto “standard”.\nFatto lo sforzo di descriverlo, è possibile usare DuckDB per convertire un CSV “brutto, sporco e cattivo”, in un CSV molto più pronto all’uso.\n\n\n\n\n\n\nNota bene\n\n\n\nAnche un CSV creato come descritto a seguire potrebbe dare qualche problema, perché i programmi di analisi - con un file CSV - sono costretti a fare l’inferenza dei contenuti. E l’inferenza può essere errata. Qui inoltre siamo di fronte a un file particolarmente “ricco” e “variegato”.\n\n\nIl comando DuckDB da usare è in modo schematico COPY INPUT TO OUTPUT (documentazione COPY):\n\nl’INPUT è il “SELEZIONA TUTTO” del file CSV compresso originario, applicando le dovute operazioni di casting dei tipi di campo;\npoi è necessario descrivere il CSV di input (i separatori, il formato dei campi con date, l’intestazione, i tipi di campo, ecc.);\ninfine, si definisce il TO scegliendo un nome per il file di output e alcune opzioni di formato.\n\nDando al file di output l’estensione .csv.gz il formato sarà appunto un CSV e sarà compresso con algoritmo GZIP. Con (HEADER, timestampformat '%Y-%m-%d') si fa in modo che l’output contenga la riga di intestazione e che le date siano rappresentate nel formato ISO 8601 (YYYY-MM-DD).\n\n\n\n\n\n\nComando da usare per la conversione\n\n\n\n\n\necho \"COPY (SELECT *\nREPLACE(\nCAST(PROGRAMMATO_INDICATORE_1 AS FLOAT) AS PROGRAMMATO_INDICATORE_1,\nCAST(PROGRAMMATO_INDICATORE_2 AS FLOAT) AS PROGRAMMATO_INDICATORE_2,\nCAST(PROGRAMMATO_INDICATORE_3 AS FLOAT) AS PROGRAMMATO_INDICATORE_3,\nCAST(PROGRAMMATO_INDICATORE_4 AS FLOAT) AS PROGRAMMATO_INDICATORE_4,\nCAST(REALIZZATO_INDICATORE_1 AS FLOAT) AS REALIZZATO_INDICATORE_1,\nCAST(REALIZZATO_INDICATORE_2 AS FLOAT) AS REALIZZATO_INDICATORE_2,\nCAST(REALIZZATO_INDICATORE_3 AS FLOAT) AS REALIZZATO_INDICATORE_3,\nCAST(REALIZZATO_INDICATORE_4 AS FLOAT) AS REALIZZATO_INDICATORE_4,\n)\nfrom read_csv_auto(\n'progetti_esteso_20230430.csv.gz',SEP=';',dateformat='%Y%m%d',decimal_separator=',',sample_size=100000,\ntypes={'OC_COD_ARTICOLAZ_PROGRAMMA':'VARCHAR','FINANZ_UE':'FLOAT','FINANZ_UE_FESR':'FLOAT','FINANZ_UE_FSE':'FLOAT','FINANZ_UE_FEASR':'FLOAT','FINANZ_UE_FEAMP':'FLOAT','FINANZ_UE_IOG':'FLOAT','FINANZ_STATO_FONDO_DI_ROTAZIONE':'FLOAT','FINANZ_STATO_FSC':'FLOAT','FINANZ_STATO_PAC':'FLOAT','FINANZ_STATO_COMPLETAMENTI':'FLOAT','FINANZ_STATO_ALTRI_PROVVEDIMENTI':'FLOAT','FINANZ_REGIONE':'FLOAT','FINANZ_PROVINCIA':'FLOAT','FINANZ_COMUNE':'FLOAT','FINANZ_RISORSE_LIBERATE':'FLOAT','FINANZ_ALTRO_PUBBLICO':'FLOAT','FINANZ_STATO_ESTERO':'FLOAT','FINANZ_PRIVATO':'FLOAT','FINANZ_DA_REPERIRE':'FLOAT','FINANZ_TOTALE_PUBBLICO':'FLOAT','ECONOMIE_TOTALI':'FLOAT','ECONOMIE_TOTALI_PUBBLICHE':'FLOAT','OC_FINANZ_UE_NETTO':'FLOAT','OC_FINANZ_UE_FESR_NETTO':'FLOAT','OC_FINANZ_UE_FSE_NETTO':'FLOAT','OC_FINANZ_UE_FEASR_NETTO':'FLOAT','OC_FINANZ_UE_FEAMP_NETTO':'FLOAT','OC_FINANZ_UE_IOG_NETTO':'FLOAT','OC_FINANZ_STATO_FONDO_ROT_NETTO':'FLOAT','OC_FINANZ_STATO_FSC_NETTO':'FLOAT','OC_FINANZ_STATO_PAC_NETTO':'FLOAT','OC_FINANZ_STATO_COMPL_NETTO':'FLOAT','OC_FINANZ_STATO_ALTRI_PROV_NETTO':'FLOAT','OC_FINANZ_REGIONE_NETTO':'FLOAT','OC_FINANZ_PROVINCIA_NETTO':'FLOAT','OC_FINANZ_COMUNE_NETTO':'FLOAT','OC_FINANZ_RISORSE_LIBERATE_NETTO':'FLOAT','OC_FINANZ_ALTRO_PUBBLICO_NETTO':'FLOAT','OC_FINANZ_STATO_ESTERO_NETTO':'FLOAT','OC_FINANZ_PRIVATO_NETTO':'FLOAT','OC_FINANZ_TOT_PUB_NETTO':'FLOAT','OC_COSTO_COESIONE':'FLOAT','IMPEGNI':'FLOAT','OC_IMPEGNI_GIURID_VINCOLANTI':'FLOAT','OC_IMPEGNI_TRASFERIMENTI':'FLOAT','OC_IMPEGNI_COESIONE':'FLOAT','TOT_PAGAMENTI':'FLOAT','OC_TOT_PAGAMENTI_BENEFICIARI':'FLOAT','OC_TOT_PAGAMENTI_TRASFERIMENTI':'FLOAT','COSTO_REALIZZATO':'FLOAT','COSTO_RENDICONTABILE_UE':'FLOAT','OC_TOT_PAGAMENTI_RENDICONTAB_UE':'FLOAT','OC_TOT_PAGAMENTI_FSC':'FLOAT','OC_TOT_PAGAMENTI_PAC':'FLOAT','OC_PAGAMENTI_COESIONE':'FLOAT','OC_DATA_INIZIO_PROGETTO':'DATE','OC_DATA_FINE_PROGETTO_PREVISTA':'DATE','OC_DATA_FINE_PROGETTO_EFFETTIVA':'DATE','DATA_INIZIO_PREV_STUDIO_FATT':'DATE','DATA_INIZIO_EFF_STUDIO_FATT':'DATE','DATA_FINE_PREV_STUDIO_FATT':'DATE','DATA_FINE_EFF_STUDIO_FATT':'DATE','DATA_INIZIO_PREV_PROG_PREL':'DATE','DATA_INIZIO_EFF_PROG_PREL':'DATE','DATA_FINE_PREV_PROG_PREL':'DATE','DATA_FINE_EFF_PROG_PREL':'DATE','DATA_INIZIO_PREV_PROG_DEF':'DATE','DATA_INIZIO_EFF_PROG_DEF':'DATE','DATA_FINE_PREV_PROG_DEF':'DATE','DATA_FINE_EFF_PROG_DEF':'DATE','DATA_INIZIO_PREV_PROG_ESEC':'DATE','DATA_INIZIO_EFF_PROG_ESEC':'DATE','DATA_FINE_PREV_PROG_ESEC':'DATE','DATA_FINE_EFF_PROG_ESEC':'DATE','DATA_INIZIO_PREV_AGG_BANDO':'DATE','DATA_INIZIO_EFF_AGG_BANDO':'DATE','DATA_FINE_PREV_AGG_BANDO':'DATE','DATA_FINE_EFF_AGG_BANDO':'DATE','DATA_INIZIO_PREV_STIP_ATTRIB':'DATE','DATA_INIZIO_EFF_STIP_ATTRIB':'DATE','DATA_FINE_PREV_STIP_ATTRIB':'DATE','DATA_FINE_EFF_STIP_ATTRIB':'DATE','DATA_INIZIO_PREV_ESECUZIONE':'DATE','DATA_INIZIO_EFF_ESECUZIONE':'DATE','DATA_FINE_PREV_ESECUZIONE':'DATE','DATA_FINE_EFF_ESECUZIONE':'DATE','DATA_INIZIO_PREV_COLLAUDO':'DATE','DATA_INIZIO_EFF_COLLAUDO':'DATE','DATA_FINE_PREV_COLLAUDO':'DATE','DATA_FINE_EFF_COLLAUDO':'DATE','COD_TIPO_PROCED_ATTIVAZIONE':'VARCHAR','OC_FLAG_REGIONE_UNICA':'INT','DATA_AGGIORNAMENTO':'DATE','OC_FLAG_CUP':'INT','PROGRAMMATO_INDICATORE_1':'VARCHAR','REALIZZATO_INDICATORE_1':'VARCHAR','PROGRAMMATO_INDICATORE_2':'VARCHAR','REALIZZATO_INDICATORE_2':'VARCHAR','PROGRAMMATO_INDICATORE_3':'VARCHAR','REALIZZATO_INDICATORE_3':'VARCHAR','PROGRAMMATO_INDICATORE_4':'VARCHAR','REALIZZATO_INDICATORE_4':'VARCHAR'}\n)) TO 'progetti_esteso_20230430_standard.csv.gz' (HEADER, timestampformat '%Y-%m-%d');\" |  duckdb\n\n\n\nCosì facendo, il CSV di output avrà queste caratteristiche:\n\nnei campi con le date, ci saranno delle stringhe nel formato ISO 8601 (YYYY-MM-DD) e non numeri interi;\ni campi numerici avranno il . come separatore dei decimali e non ci sarà difformità nel modo di rappresentarlo;\nil separatore di campo sarà la ,."
  },
  {
    "objectID": "posts/duckdb-intro-csv/index.html#descrivere-i-csv",
    "href": "posts/duckdb-intro-csv/index.html#descrivere-i-csv",
    "title": "Gestire file CSV grandi, brutti e cattivi",
    "section": "Descrivere i CSV",
    "text": "Descrivere i CSV\n\nCSV is one of the most popular formats for publishing data on the web. It is concise, easy to understand by both humans and computers, and aligns nicely to the tabular nature of most data.\nBut CSV is also a poor format for data. There is no mechanism within CSV to indicate the type of data in a particular column, or whether values in a particular column must be unique. It is therefore hard to validate and prone to errors such as missing values or differing data types within a column.\n\nQueste frasi sono presenti all’inizio di un lavoro di riferimento, per comprendere come si potrebbe descrivere un file in questo formato, in modo che sia utile, efficace e leggibile da umani, personal computer e applicazioni: CSV on the Web: A Primer. Se non lo conosci, leggilo. Io qui non entrerò nei dettagli, perché non vale la pena ripetere quello che è già stato scritto molto bene.\nUn altro progetto di riferimento per la descrizione di un file CSV è Frictionless Data e le sue specifiche per i dati tabellari. E ancora una volta, per le stesse ragioni non entrerò nei dettagli.\nMolti dei dati pubblicati nei portali Open Data italiani in questo formato, sono accompagnati soltanto da un titolo e una descrizione. È un vuoto informativo e molto spesso una barriera a un pieno uso dei dati. Alle volte, come nel caso di OpenCoesione, è presente un file con i nomi dei campi, la loro tipologia e altre note. Nella gran parte dei casi sono però file descrittivi leggibili soltanto a schermo da chi dovrà analizzare i file. Non sono leggibili da un personal computer o da un’applicazione, non sono machine readable.\nE quando si hanno dati ricchi e complessi, questa diventa un’ulteriore barriera all’uso dei dati.\n\n\n\n\n\n\n📌 Importante\n\n\n\nLa pubblicazione non si dovrebbe limitare a rendere disponibile dei file, ma essere accompagnata da una descrizione del file stesso, leggibile dalle persone e dalle applicazioni.\n\n\nFarlo in una delle due modalità descritte sopra sarebbe perfetto. Non è a costo zero, ma se è uno dei processi (“automatici”) di un progetto di pubblicazione, è un costo che si può sostenere, che si ripaga in termini di usabilità e riusabilità dei dati.\n\nAssociare schema SQL\nUna “buona pratica” da suggerire a chi pubblica dati, basata su uno standard che è sia formale che de facto, è quella di accompagnare al file CSV anche la query SQL per la creazione della tabella. Sotto l’esempio per la creazione della tabella dei progetti di OpenCoesione descritta qui (qui il file SQL da prendere soltanto come spunto).\n\n\n\n\n\n\nSchema SQL tabella progetti di OpenCoesione\n\n\n\n\n\nCREATE TABLE progetti(COD_LOCALE_PROGETTO VARCHAR, CUP VARCHAR, OC_TITOLO_PROGETTO VARCHAR, OC_SINTESI_PROGETTO VARCHAR, OC_LINK VARCHAR, OC_COD_CICLO BIGINT, OC_DESCR_CICLO VARCHAR, OC_COD_TEMA_SINTETICO VARCHAR, OC_TEMA_SINTETICO VARCHAR, COD_GRANDE_PROGETTO VARCHAR, DESCRIZIONE_GRANDE_PROGETTO VARCHAR, OC_COD_FONTE VARCHAR, OC_DESCR_FONTE VARCHAR, FONDO_COMUNITARIO VARCHAR, OC_CODICE_PROGRAMMA VARCHAR, OC_DESCRIZIONE_PROGRAMMA VARCHAR, COD_OB_TEMATICO VARCHAR, DESCR_OB_TEMATICO VARCHAR, COD_PRIORITA_INVEST VARCHAR, DESCR_PRIORITA_INVEST VARCHAR, OC_COD_CATEGORIA_SPESA VARCHAR, OC_DESCR_CATEGORIA_SPESA VARCHAR, OC_ARTICOLAZIONE_PROGRAMMA VARCHAR, OC_SUBARTICOLAZIONE_PROGRAMMA VARCHAR, OC_COD_ARTICOLAZ_PROGRAMMA VARCHAR, OC_DESCR_ARTICOLAZ_PROGRAMMA VARCHAR, OC_COD_SUBARTICOLAZ_PROGRAMMA VARCHAR, OC_DESCR_SUBARTICOLAZ_PROGRAMMA VARCHAR, COD_STRUMENTO VARCHAR, DESCR_STRUMENTO VARCHAR, DESCR_TIPO_STRUMENTO VARCHAR, CUP_COD_NATURA VARCHAR, CUP_DESCR_NATURA VARCHAR, CUP_COD_TIPOLOGIA VARCHAR, CUP_DESCR_TIPOLOGIA VARCHAR, CUP_COD_SETTORE VARCHAR, CUP_DESCR_SETTORE VARCHAR, CUP_COD_SOTTOSETTORE VARCHAR, CUP_DESCR_SOTTOSETTORE VARCHAR, CUP_COD_CATEGORIA VARCHAR, CUP_DESCR_CATEGORIA VARCHAR, COD_ATECO VARCHAR, DESCRIZIONE_ATECO VARCHAR, OC_COD_TIPO_AIUTO VARCHAR, OC_DESCR_TIPO_AIUTO VARCHAR, COD_REGIONE VARCHAR, DEN_REGIONE VARCHAR, COD_PROVINCIA VARCHAR, DEN_PROVINCIA VARCHAR, COD_COMUNE VARCHAR, DEN_COMUNE VARCHAR, OC_MACROAREA VARCHAR, OC_COD_SLL BIGINT, OC_DENOMINAZIONE_SLL VARCHAR, FINANZ_UE FLOAT, FINANZ_UE_FESR FLOAT, FINANZ_UE_FSE FLOAT, FINANZ_UE_FEASR FLOAT, FINANZ_UE_FEAMP FLOAT, FINANZ_UE_IOG FLOAT, FINANZ_STATO_FONDO_DI_ROTAZIONE FLOAT, FINANZ_STATO_FSC FLOAT, FINANZ_STATO_PAC FLOAT, FINANZ_STATO_COMPLETAMENTI FLOAT, FINANZ_STATO_ALTRI_PROVVEDIMENTI FLOAT, FINANZ_REGIONE FLOAT, FINANZ_PROVINCIA FLOAT, FINANZ_COMUNE FLOAT, FINANZ_RISORSE_LIBERATE FLOAT, FINANZ_ALTRO_PUBBLICO FLOAT, FINANZ_STATO_ESTERO FLOAT, FINANZ_PRIVATO FLOAT, FINANZ_DA_REPERIRE FLOAT, FINANZ_TOTALE_PUBBLICO FLOAT, ECONOMIE_TOTALI FLOAT, ECONOMIE_TOTALI_PUBBLICHE FLOAT, OC_FINANZ_UE_NETTO FLOAT, OC_FINANZ_UE_FESR_NETTO FLOAT, OC_FINANZ_UE_FSE_NETTO FLOAT, OC_FINANZ_UE_FEASR_NETTO FLOAT, OC_FINANZ_UE_FEAMP_NETTO FLOAT, OC_FINANZ_UE_IOG_NETTO FLOAT, OC_FINANZ_STATO_FONDO_ROT_NETTO FLOAT, OC_FINANZ_STATO_FSC_NETTO FLOAT, OC_FINANZ_STATO_PAC_NETTO FLOAT, OC_FINANZ_STATO_COMPL_NETTO FLOAT, OC_FINANZ_STATO_ALTRI_PROV_NETTO FLOAT, OC_FINANZ_REGIONE_NETTO FLOAT, OC_FINANZ_PROVINCIA_NETTO FLOAT, OC_FINANZ_COMUNE_NETTO FLOAT, OC_FINANZ_RISORSE_LIBERATE_NETTO FLOAT, OC_FINANZ_ALTRO_PUBBLICO_NETTO FLOAT, OC_FINANZ_STATO_ESTERO_NETTO FLOAT, OC_FINANZ_PRIVATO_NETTO FLOAT, OC_FINANZ_TOT_PUB_NETTO FLOAT, OC_COSTO_COESIONE FLOAT, IMPEGNI FLOAT, OC_IMPEGNI_GIURID_VINCOLANTI FLOAT, OC_IMPEGNI_TRASFERIMENTI FLOAT, OC_IMPEGNI_COESIONE FLOAT, TOT_PAGAMENTI FLOAT, OC_TOT_PAGAMENTI_BENEFICIARI FLOAT, OC_TOT_PAGAMENTI_TRASFERIMENTI FLOAT, COSTO_REALIZZATO FLOAT, COSTO_RENDICONTABILE_UE FLOAT, OC_TOT_PAGAMENTI_RENDICONTAB_UE FLOAT, OC_TOT_PAGAMENTI_FSC FLOAT, OC_TOT_PAGAMENTI_PAC FLOAT, OC_PAGAMENTI_COESIONE FLOAT, OC_DATA_INIZIO_PROGETTO DATE, OC_DATA_FINE_PROGETTO_PREVISTA DATE, OC_DATA_FINE_PROGETTO_EFFETTIVA DATE, DATA_INIZIO_PREV_STUDIO_FATT DATE, DATA_INIZIO_EFF_STUDIO_FATT DATE, DATA_FINE_PREV_STUDIO_FATT DATE, DATA_FINE_EFF_STUDIO_FATT DATE, DATA_INIZIO_PREV_PROG_PREL DATE, DATA_INIZIO_EFF_PROG_PREL DATE, DATA_FINE_PREV_PROG_PREL DATE, DATA_FINE_EFF_PROG_PREL DATE, DATA_INIZIO_PREV_PROG_DEF DATE, DATA_INIZIO_EFF_PROG_DEF DATE, DATA_FINE_PREV_PROG_DEF DATE, DATA_FINE_EFF_PROG_DEF DATE, DATA_INIZIO_PREV_PROG_ESEC DATE, DATA_INIZIO_EFF_PROG_ESEC DATE, DATA_FINE_PREV_PROG_ESEC DATE, DATA_FINE_EFF_PROG_ESEC DATE, DATA_INIZIO_PREV_AGG_BANDO DATE, DATA_INIZIO_EFF_AGG_BANDO DATE, DATA_FINE_PREV_AGG_BANDO DATE, DATA_FINE_EFF_AGG_BANDO DATE, DATA_INIZIO_PREV_STIP_ATTRIB DATE, DATA_INIZIO_EFF_STIP_ATTRIB DATE, DATA_FINE_PREV_STIP_ATTRIB DATE, DATA_FINE_EFF_STIP_ATTRIB DATE, DATA_INIZIO_PREV_ESECUZIONE DATE, DATA_INIZIO_EFF_ESECUZIONE DATE, DATA_FINE_PREV_ESECUZIONE DATE, DATA_FINE_EFF_ESECUZIONE DATE, DATA_INIZIO_PREV_COLLAUDO DATE, DATA_INIZIO_EFF_COLLAUDO DATE, DATA_FINE_PREV_COLLAUDO DATE, DATA_FINE_EFF_COLLAUDO DATE, OC_STATO_FINANZIARIO VARCHAR, OC_STATO_PROGETTO VARCHAR, OC_STATO_PROCEDURALE VARCHAR, OC_COD_FASE_CORRENTE VARCHAR, OC_DESCR_FASE_CORRENTE VARCHAR, COD_PROCED_ATTIVAZIONE VARCHAR, DESCR_PROCED_ATTIVAZIONE VARCHAR, COD_TIPO_PROCED_ATTIVAZIONE VARCHAR, DESCR_TIPO_PROCED_ATTIVAZIONE VARCHAR, OC_CODFISC_PROGRAMMATORE VARCHAR, OC_DENOM_PROGRAMMATORE VARCHAR, OC_COD_FORMA_GIU_PROGRAMMATORE VARCHAR, OC_DESCR_FORMA_GIU_PROGRAMMATORE VARCHAR, OC_TOTALE_PROGRAMMATORI BIGINT, OC_CODFISC_ATTUATORE VARCHAR, OC_DENOM_ATTUATORE VARCHAR, OC_COD_FORMA_GIU_ATTUATORE VARCHAR, OC_DESCR_FORMA_GIU_ATTUATORE VARCHAR, OC_TOTALE_ATTUATORI BIGINT, OC_CODFISC_BENEFICIARIO VARCHAR, OC_DENOM_BENEFICIARIO VARCHAR, OC_COD_FORMA_GIU_BENEFICIARIO VARCHAR, OC_DESCR_FORMA_GIU_BENEFICIARIO VARCHAR, OC_TOTALE_BENEFICIARI BIGINT, OC_CODFISC_REALIZZATORE VARCHAR, OC_DENOM_REALIZZATORE VARCHAR, OC_COD_FORMA_GIU_REALIZZATORE VARCHAR, OC_DESCR_FORMA_GIU_REALIZZATORE VARCHAR, OC_TOTALE_REALIZZATORI BIGINT, OC_TOTALE_INDICATORI BIGINT, COD_INDICATORE_1 VARCHAR, DESCR_INDICATORE_1 VARCHAR, UNITA_MISURA_INDICATORE_1 VARCHAR, PROGRAMMATO_INDICATORE_1 FLOAT, REALIZZATO_INDICATORE_1 FLOAT, COD_INDICATORE_2 VARCHAR, DESCR_INDICATORE_2 VARCHAR, UNITA_MISURA_INDICATORE_2 VARCHAR, PROGRAMMATO_INDICATORE_2 FLOAT, REALIZZATO_INDICATORE_2 FLOAT, COD_INDICATORE_3 VARCHAR, DESCR_INDICATORE_3 VARCHAR, UNITA_MISURA_INDICATORE_3 VARCHAR, PROGRAMMATO_INDICATORE_3 FLOAT, REALIZZATO_INDICATORE_3 FLOAT, COD_INDICATORE_4 VARCHAR, DESCR_INDICATORE_4 VARCHAR, UNITA_MISURA_INDICATORE_4 VARCHAR, PROGRAMMATO_INDICATORE_4 FLOAT, REALIZZATO_INDICATORE_4 FLOAT, OC_FLAG_REGIONE_UNICA INTEGER, OC_FLAG_VISUALIZZAZIONE BIGINT, OC_FLAG_PAC BIGINT, DATA_AGGIORNAMENTO DATE, OC_FLAG_CUP INTEGER);\n\n\n\nCon una tabella così ricca, di queste dimensioni e pubblicata in questo formato, avere lo schema SQL di creazione renderebbe l’uso dei dati da subito molto più efficace. Perché il CSV è principalmente un formato di scambio, e poterlo importare subito in un database dà una marcia in più."
  },
  {
    "objectID": "posts/duckdb-intro-csv/index.html#parquet",
    "href": "posts/duckdb-intro-csv/index.html#parquet",
    "title": "Gestire file CSV grandi, brutti e cattivi",
    "section": "Parquet",
    "text": "Parquet\nIl formato CSV - con i suoi pregi e difetti - è molto diffuso. Un formato tabellare degno di nota che sta emergendo, è il Parquet.\nÈ un formato open source progettato per l’archiviazione e l’analisi di grandi dataset. Le principali caratteristiche sono:\n\nStruttura a colonne: i dati sono memorizzati per colonna e non per riga, per una migliore compressione;\nCompressione efficace: supporta compressione gzip, snappy e zstd;\nPrestazioni elevate: consente analisi molto veloci grazie alla struttura a colonne;\nFormato descritto: contiene metadati sullo schema;\nPartizionabile: un unico dataset di dimensioni molto molto grandi, può essere suddiviso in più file e cartelle, partizionate (ad esempio per anno, per regione, per provincia, ecc.);\nAccessibile in stream: invece di leggere o scrivere l’intero file in una volta sola, consente di lavorare con il file suddividendo i dati in pacchetti più piccoli;\nSupportato da molti data engine e da tutti i principali linguaggi di programmazione e librerie di elaborazione dati.\n\nTra le più interessanti, specie per chi è nuovo al concetto e al formato, è la struttura di archiviazione a colonne.\n\nFormato colonnare\nIn molti dei formati tabellari più noti e comuni, i dati sono archiviati come sequenze di righe. Avviene per i CSV, ma anche per importanti database relazionali come PostgreSQL e MySQL.\n\n\n\nTabella 2: Tabella dati di esempio\n\n\nPizza\nCliente\nRegione\nData\nCosto\n\n\n\n\nMargherita\nMarco Stretto\nSicilia\n2023-01-01\n5\n\n\nParmigiana\nMarco Stretto\nSicilia\n2023-01-02\n9\n\n\nRomana\nGuido La Vespa\nCalabria\n2023-01-01\n7\n\n\nRomana\nChiara Mente\nSicilia\n2023-01-03\n7\n\n\nParmigiana\nGuido La Vespa\nCalabria\n2023-01-02\n9\n\n\nRomana\nMarco Stretto\nSicilia\n2023-01-05\n7\n\n\n\n\n\nA partire da una tabella con dei dati sulle vendite di una pizzeria, come Tabella 2, le domande che si potrebbero fare sono:\n\nquante pizze Margherita sono state vendute?\nquanti utenti che vengono dalla Sicilia hanno ordinato una pizza Parmigiana?\nquanto ha speso in totale Guido La Vespa?\nquante vendite sono state fatte il 2 gennaio 2023?\n\nPer rispondere a queste domande, un motore di “tabelle” basato su righe, dovrà leggere tutte le righe della tabella, dall’inizio alla fine. Per sapere quanti utenti che vengono dalla Sicilia hanno ordinato una pizza Parmigiana, si dovrà scorrere la tabella come in Figura 1.\n\n\n\nFigura 1: Flusso lettura dati, in archivi basati su righe\n\n\nBasterebbe leggere soltanto i valori delle colonne Pizza e Regione, ma in uno schema di archiviazione basato su righe vengono lette anche le altre colonne.\nPer rispondere a questa query è molto più efficiente un’archiviazione a colonne. In questo caso, ogni colonna è un’entità, il che significa che ogni colonna è fisicamente separata dalle altre.  Tornando alla nostra precedente domanda: in uno schema a colonne il motore di analisi può leggere soltanto le colonne necessarie per la query (Pizza e Regione). E, nella maggior parte dei casi, questo migliorerà le prestazioni delle query analitiche.\n\n\n\n\nPizza\nCliente\nRegione\nData\nCosto\n\n\n\n\nMargherita\nMarco Stretto\nSicilia\n2023-01-01\n5\n\n\nParmigiana\nMarco Stretto\nSicilia\n2023-01-02\n9\n\n\nRomana\nGuido La Vespa\nCalabria\n2023-01-01\n7\n\n\nRomana\nChiara Mente\nSicilia\n2023-01-03\n7\n\n\nParmigiana\nGuido La Vespa\nCalabria\n2023-01-02\n9\n\n\nRomana\nMarco Stretto\nSicilia\n2023-01-05\n7\n\n\n\nIl formato Parquet è proprio un formato di archiviazione a colonne, anzi è più correttamente un formato di archiviazione a colonne, per gruppi di righe.\n\n\n\nFigura 2: Struttura a colonne per gruppi di righe\n\n\nLe colonne sono memorizzate sempre come unità separate, ma Parquet introduce delle strutture aggiuntive chiamate “Row group” (vedi Figura 2).\nPerché sono un vantaggio? Nella query di esempio di sopra, non solo bastano due colonne, ma il Gruppo 2 di righe è inutile (non c’è la “Parmigiana”). Con questa struttura a gruppi di righe (qui semplificata per fini didattici) sarà possibile non tenere conto del “Gruppo 2” e la query sarà più rapida.\n\n\n\nFigura 3: Per la query di esempio, il “Gruppo 2” è inutile\n\n\n\n\nConvertire il file di OpenCoesione in Parquet\nPer convertire il file di OpenCoesione in formato Parquet si può usare ancora una volta un comando DuckDB. Richiederà una ricca descrizione del file di input, e di base sarà COPY INPUT TO OUTPUT (documentazione COPY):\n\nl’INPUT è il “SELEZIONA TUTTO” del file CSV compresso originario, applicando le dovute operazioni di casting dei tipi di campo;\npoi è necessario descrivere il CSV di input (i separatori, il formato dei campi con date, l’intestazione, i tipi di campo, ecc.);\ninfine, si definisce il TO scegliendo un nome per il file di output e alcune opzioni di formato.\n\n\n\n\n\n\n\nComando da usare per la conversione da CSV a Parquet\n\n\n\n\n\necho \"COPY (SELECT *\nREPLACE(\nCAST(PROGRAMMATO_INDICATORE_1 AS FLOAT) AS PROGRAMMATO_INDICATORE_1,\nCAST(PROGRAMMATO_INDICATORE_2 AS FLOAT) AS PROGRAMMATO_INDICATORE_2,\nCAST(PROGRAMMATO_INDICATORE_3 AS FLOAT) AS PROGRAMMATO_INDICATORE_3,\nCAST(PROGRAMMATO_INDICATORE_4 AS FLOAT) AS PROGRAMMATO_INDICATORE_4,\nCAST(REALIZZATO_INDICATORE_1 AS FLOAT) AS REALIZZATO_INDICATORE_1,\nCAST(REALIZZATO_INDICATORE_2 AS FLOAT) AS REALIZZATO_INDICATORE_2,\nCAST(REALIZZATO_INDICATORE_3 AS FLOAT) AS REALIZZATO_INDICATORE_3,\nCAST(REALIZZATO_INDICATORE_4 AS FLOAT) AS REALIZZATO_INDICATORE_4,\n)\nfrom read_csv_auto(\n'progetti_esteso_20230430.csv.gz',SEP=';',dateformat='%Y%m%d',decimal_separator=',',sample_size=100000,\ntypes={'OC_COD_ARTICOLAZ_PROGRAMMA':'VARCHAR','FINANZ_UE':'FLOAT','FINANZ_UE_FESR':'FLOAT','FINANZ_UE_FSE':'FLOAT','FINANZ_UE_FEASR':'FLOAT','FINANZ_UE_FEAMP':'FLOAT','FINANZ_UE_IOG':'FLOAT','FINANZ_STATO_FONDO_DI_ROTAZIONE':'FLOAT','FINANZ_STATO_FSC':'FLOAT','FINANZ_STATO_PAC':'FLOAT','FINANZ_STATO_COMPLETAMENTI':'FLOAT','FINANZ_STATO_ALTRI_PROVVEDIMENTI':'FLOAT','FINANZ_REGIONE':'FLOAT','FINANZ_PROVINCIA':'FLOAT','FINANZ_COMUNE':'FLOAT','FINANZ_RISORSE_LIBERATE':'FLOAT','FINANZ_ALTRO_PUBBLICO':'FLOAT','FINANZ_STATO_ESTERO':'FLOAT','FINANZ_PRIVATO':'FLOAT','FINANZ_DA_REPERIRE':'FLOAT','FINANZ_TOTALE_PUBBLICO':'FLOAT','ECONOMIE_TOTALI':'FLOAT','ECONOMIE_TOTALI_PUBBLICHE':'FLOAT','OC_FINANZ_UE_NETTO':'FLOAT','OC_FINANZ_UE_FESR_NETTO':'FLOAT','OC_FINANZ_UE_FSE_NETTO':'FLOAT','OC_FINANZ_UE_FEASR_NETTO':'FLOAT','OC_FINANZ_UE_FEAMP_NETTO':'FLOAT','OC_FINANZ_UE_IOG_NETTO':'FLOAT','OC_FINANZ_STATO_FONDO_ROT_NETTO':'FLOAT','OC_FINANZ_STATO_FSC_NETTO':'FLOAT','OC_FINANZ_STATO_PAC_NETTO':'FLOAT','OC_FINANZ_STATO_COMPL_NETTO':'FLOAT','OC_FINANZ_STATO_ALTRI_PROV_NETTO':'FLOAT','OC_FINANZ_REGIONE_NETTO':'FLOAT','OC_FINANZ_PROVINCIA_NETTO':'FLOAT','OC_FINANZ_COMUNE_NETTO':'FLOAT','OC_FINANZ_RISORSE_LIBERATE_NETTO':'FLOAT','OC_FINANZ_ALTRO_PUBBLICO_NETTO':'FLOAT','OC_FINANZ_STATO_ESTERO_NETTO':'FLOAT','OC_FINANZ_PRIVATO_NETTO':'FLOAT','OC_FINANZ_TOT_PUB_NETTO':'FLOAT','OC_COSTO_COESIONE':'FLOAT','IMPEGNI':'FLOAT','OC_IMPEGNI_GIURID_VINCOLANTI':'FLOAT','OC_IMPEGNI_TRASFERIMENTI':'FLOAT','OC_IMPEGNI_COESIONE':'FLOAT','TOT_PAGAMENTI':'FLOAT','OC_TOT_PAGAMENTI_BENEFICIARI':'FLOAT','OC_TOT_PAGAMENTI_TRASFERIMENTI':'FLOAT','COSTO_REALIZZATO':'FLOAT','COSTO_RENDICONTABILE_UE':'FLOAT','OC_TOT_PAGAMENTI_RENDICONTAB_UE':'FLOAT','OC_TOT_PAGAMENTI_FSC':'FLOAT','OC_TOT_PAGAMENTI_PAC':'FLOAT','OC_PAGAMENTI_COESIONE':'FLOAT','OC_DATA_INIZIO_PROGETTO':'DATE','OC_DATA_FINE_PROGETTO_PREVISTA':'DATE','OC_DATA_FINE_PROGETTO_EFFETTIVA':'DATE','DATA_INIZIO_PREV_STUDIO_FATT':'DATE','DATA_INIZIO_EFF_STUDIO_FATT':'DATE','DATA_FINE_PREV_STUDIO_FATT':'DATE','DATA_FINE_EFF_STUDIO_FATT':'DATE','DATA_INIZIO_PREV_PROG_PREL':'DATE','DATA_INIZIO_EFF_PROG_PREL':'DATE','DATA_FINE_PREV_PROG_PREL':'DATE','DATA_FINE_EFF_PROG_PREL':'DATE','DATA_INIZIO_PREV_PROG_DEF':'DATE','DATA_INIZIO_EFF_PROG_DEF':'DATE','DATA_FINE_PREV_PROG_DEF':'DATE','DATA_FINE_EFF_PROG_DEF':'DATE','DATA_INIZIO_PREV_PROG_ESEC':'DATE','DATA_INIZIO_EFF_PROG_ESEC':'DATE','DATA_FINE_PREV_PROG_ESEC':'DATE','DATA_FINE_EFF_PROG_ESEC':'DATE','DATA_INIZIO_PREV_AGG_BANDO':'DATE','DATA_INIZIO_EFF_AGG_BANDO':'DATE','DATA_FINE_PREV_AGG_BANDO':'DATE','DATA_FINE_EFF_AGG_BANDO':'DATE','DATA_INIZIO_PREV_STIP_ATTRIB':'DATE','DATA_INIZIO_EFF_STIP_ATTRIB':'DATE','DATA_FINE_PREV_STIP_ATTRIB':'DATE','DATA_FINE_EFF_STIP_ATTRIB':'DATE','DATA_INIZIO_PREV_ESECUZIONE':'DATE','DATA_INIZIO_EFF_ESECUZIONE':'DATE','DATA_FINE_PREV_ESECUZIONE':'DATE','DATA_FINE_EFF_ESECUZIONE':'DATE','DATA_INIZIO_PREV_COLLAUDO':'DATE','DATA_INIZIO_EFF_COLLAUDO':'DATE','DATA_FINE_PREV_COLLAUDO':'DATE','DATA_FINE_EFF_COLLAUDO':'DATE','COD_TIPO_PROCED_ATTIVAZIONE':'VARCHAR','OC_FLAG_REGIONE_UNICA':'INT','DATA_AGGIORNAMENTO':'DATE','OC_FLAG_CUP':'INT','PROGRAMMATO_INDICATORE_1':'VARCHAR','REALIZZATO_INDICATORE_1':'VARCHAR','PROGRAMMATO_INDICATORE_2':'VARCHAR','REALIZZATO_INDICATORE_2':'VARCHAR','PROGRAMMATO_INDICATORE_3':'VARCHAR','REALIZZATO_INDICATORE_3':'VARCHAR','PROGRAMMATO_INDICATORE_4':'VARCHAR','REALIZZATO_INDICATORE_4':'VARCHAR'}\n)) TO 'progetti_esteso_20230430.parquet' (FORMAT 'PARQUET', CODEC 'ZSTD');\" |  duckdb\n\n\n\nCon (FORMAT 'PARQUET', CODEC 'ZSTD') si imposta il PARQUET come formato di output e ZSTD come algoritmo di compressione.\n\n\nÈ come avere delle API\nIl formato parquet - accoppiato a client come DuckDB - rende facile e universale l’accesso ai dati. Questo perché, una volta che un file è pubblico sul web (in HTTPS, come S3, ecc.), si ha a disposizione un’interfaccia SQL per interrogarlo, in modalità streaming, ovvero rendendo disponibili solo le porzioni di dati necessarie per rispondere alle query, invece che tutto il file.\nÈ un po’ come avere delle API, ma senza doverle creare. E, da utente, senza dovere studiare una nuova documentazione di API, perché basta conoscere l’“universale” SQL e accedere al file parquet dal linguaggio di scripting e/o client preferiti.\nMi spiego con un esempio, basato sui dati sui progetti del PNRR. È un dataset “piccolo” (200.000 righe per 50 colonne), ma mi è comodo qui a fini didattici. Voglio sapere per ogni Missione, il numero dei progetti e il valore del finanziamento PNRR (in euro). Usando come client DuckDB, e abilitando l’estensione httpfs (vedi sezione estensioni), basta conoscere l’URL del file e lanciare questa query:\nduckdb -c '\nSELECT \"Descrizione Missione\" missione,COUNT(*) numero_progetti,\nSUM(\"Finanziamento PNRR\") finanziamento_pnrr\nFROM \"https://raw.githubusercontent.com/aborruso/aborruso.github.io//main/posts/duckdb-intro-csv/file/PNRR_Progetti-Universo_REGIS_v2.1.parquet\"\nGROUP by \"Descrizione Missione\"\nORDER BY finanziamento_pnrr desc;\n'\nE in circa 1 secondo (al primo lancio e in molto meno dal secondo in poi) ottengo:\n\n\n\nTabella 3: Output della query\n\n\n\n\n\n\n\nmissione\nnumero_progetti\nfinanziamento_pnrr\n\n\n\n\nRivoluzione verde e transizione ecologica\n49621\n22688516407.240803\n\n\nInfrastrutture per una mobilità sostenibile\n206\n22563735313.390625\n\n\nIstruzione e ricerca\n65403\n20129269029.008453\n\n\nDigitalizzazione, innovazione, competitività e cultura\n63025\n19245334466.823135\n\n\nInclusione e coesione\n11764\n12998257338.579054\n\n\nSalute\n7527\n8059988416.008047\n\n\n\n\n\nE posso farlo anche da una pagina web che includa DuckDb come WebAssembly (vedi Figura 4).\n\n\n\nFigura 4: Il client DuckDB come modulo WASM\n\n\n\n\n\n\n\n\nNon è la soluzione definitiva\n\n\n\nQuesta dei file statici in formato parquet non è la soluzione definitiva per abilitare l’accesso ai dati in modo programmatico, in applicazioni web. Fino ad alcuni milioni di righe ha delle performance buone, poi diventa un po’ lento. E ci sono modalità più ottimizzate per richieste transazionali o atomiche, per dati relazionali con schemi complessi, per dataset “privati” con controllo granulare degli accessi, e per dati in rapido cambiamento.\n\n\n\n\nAffiancarlo ai file CSV\nIl formato parquet - per alcune delle sue caratteristiche - potrebbe affiancare (e alla lunga rimpiazzare?) il CSV, come formato di pubblicazione di dati aperti.\nIn contesti applicativi in cui le performance, lo spazio di archiviazione, le modalità di accesso sono parametri da tenere in alta considerazione, è già molto usato. Un esempio per tutti è quello dei dataset di Hugging Face.\nI dati aperti globali sugli edifici del progetto GlobalMLBuildingFootprints (1.2 miliardi di edifici), sono in formato parquet, nella sua estensione spaziale. Anche i dataset geografici del progetto Overture Maps Foundation, sono in formato parquet. I dati aperti sulle corse di taxi e veicoli con conducente di New York, sono anch’essi in questo formato."
  },
  {
    "objectID": "posts/duckdb-intro-csv/index.html#duckdb",
    "href": "posts/duckdb-intro-csv/index.html#duckdb",
    "title": "Gestire file CSV grandi, brutti e cattivi",
    "section": "DuckDB",
    "text": "DuckDB\nDuckDB è lo strumento più usato in questo post, perché riesce a fare utilizzare comodamente dati CSV brutti, sporchi, cattivi e “grossi”, e anche renderli belli (con una conversione in formato parquet).\nÈ un database relazionale embedded e open source leggero e veloce, di facilissima installazione e gestione. È progettato per l’analisi rapida di dati ed è basato su uno schema di archiviazione a colonne (vedi spiegazione correlata per il formato parquet).\nÈ scritto in C++, ha un’interfaccia SQL standard, ed è integrabile facilmente in qualsiasi ambiente di lavoro (API per Python, R, Java, Julia, Swift, ecc.).\nPer le sue caratteristiche e per l’uso che si fa tipicamente dei dati, ha contribuito a far emergere la frase “Big data is dead” (leggilo è molto interessante):\n\nI sistemi moderni possono gestire anche dati molto grandi in modo efficace ed economico.\nLa maggior parte delle aziende ha pochi GB/TB di dati, non i PB prospettati dal marketing di alcune soluzioni.\nStorage e computing sono separati, ma lo storage tende ad aumentare più velocemente del computing richiesto.\nLe query analitiche usano di solito solo una piccola parte dei dataset, i record più recenti.\nAlcuni dati invecchiano velocemente, dopo poco tempo sono interrogati raramente e possono diventare un onere.\nBisognerebbe dimostrare che i dataset rimangono realmente rilevanti anziché accumularli senza motivo.\n\nDuckDB è “tanta roba” e sul sito ufficiale è tutto molto ben documentato.\nQui voglio sottolineare alcune cose che, da piccolo utente, mi sono piaciute molto.\n\nFacilità di installazione e utilizzo\nLa pagina dedicata è molto comoda:\n\nsi sceglie la versione;\nl’ambiente e le modalità di lavoro (io lo utilizzo soprattutto a riga di comando, come CLI);\nla piattaforma;\nvengono restituite le istruzioni per l’installazione.\n\nE in pochissimo tempo si è pronti all’uso.\n\n\n\nFigura 5: Pagina di installazione di DuckDB\n\n\n\n\nLe estensioni\nDuckBD ha delle estensioni che ne estendono in modo molto interessante l’utilizzo.\nTre che ho trovato subito utilissime sono:\n\nhttpfs, per leggere file CSV e parquet direttamente da indirizzi HTTP(S) (è come avere delle API) o tramite S3;\njson, che implementa funzioni per lavorare con dati in formato JSON;\nspatial, per usare DuckDB come motore di analisi spaziali/geografiche.\n\nE con l’estensione httpfs e con dati CSV pubblicati in modo “standard” (come quelli sulla COVID-19 pubblicati dalla Protezione Civile), è ad esempio possibile avere restituito comodamente e rapidamente la media mobile settimanale dei nuovi casi COVID-19 per regione, puntando semplicemente all’URL di un file statico (DuckBD supporta una sintassi SQL, dove il FROM può essere inserito all’inizio):\nduckdb  --csv -c \"FROM read_csv_auto('https://raw.githubusercontent.com/pcm-dpc/COVID-19/master/dati-regioni/dpc-covid19-ita-regioni.csv')\nSELECT denominazione_regione,data,nuovi_positivi,AVG(nuovi_positivi)\nOVER (\n  PARTITION BY denominazione_regione\n  ORDER BY data\n  RANGE BETWEEN INTERVAL 7 DAYS PRECEDING\n  AND INTERVAL 0 DAYS FOLLOWING\n  ) AS media_mobile\nORDER BY 1,2;\"\nCome se fosse un’API.\nE per dare un’idea dell’estensione spaziale, un piccolo esempio, per creare due punti, unirli in una linea, calcolare il centroide di questa ed estrarne la coordinata x:\n1duckdb -c 'load spatial;\nSELECT ST_X(\n  ST_Centroid(\n    ST_MakeLine(\n      ST_POINT(42.3471, 14.8454),ST_POINT(42.1471, 13.8454)\n      )\n    )\n) test;'\n\n1\n\ncarica l’estensione spaziale\n\n\nA schermo si avrà 42.2471.\n\n\nUn SQL più comodo\nDuckDB supporta un SQL che mette di buon umore. Alcuni esempi.\nSe ho una tabella con 199 colonne, come quella dei progetti di OpenCoesione, e voglio selezionarle tutte tranne due, dovrei scrivere per esteso le 196 che voglio. Qui posso usare EXCLUDE e scrivere soltanto le due che non voglio:\nSELECT * EXCLUDE (codice_progetto, codice_progetto_originale) FROM progetti;\nLo stesso avviene nel comune SQL, quando devo applicare delle modifiche soltanto a una parte dei campi di una tabella. Qui ho REPLACE:\nSELECT * REPLACE (FINANZ_TOTALE_PUBBLICO/100 AS FINANZ_NORM) FROM miatabella;\nMa il SQL “speciale” di DuckDB che mi ha fatto più piacere usare è il GROUP BY ALL. Normalmente, infatti specificare le colonne sia nella clausola SELECT che nella clausola GROUP BY. Qui, con GROUP BY ALL, il raggruppamento viene fatto per tutte le colonne nella clausola SELECT che non sono incluse in una funzione di aggregazione.\nSELECT missione,regione, COUNT(*) numero_progetti\nFROM progetti\nGROUP BY ALL\nE in modo simile funziona l’ORDER BY ALL.\nIn molti dialetti SQL, non è possibile utilizzare un alias definito nella clausola SELECT in nessun altro punto, tranne che nella clausola ORDER BY di quella stessa query. Questo spesso porta a lunghe subquery.In DuckDB, un alias non di aggregazione nella clausola SELECT può essere immediatamente utilizzato nelle clausole WHERE e GROUP BY, mentre gli alias di aggregazione possono essere utilizzati nella clausola HAVING. Nessuna subquery necessaria!\nÈ possibile fare lo slicing delle stringhe in modo simile ai linguaggi di scripting, senza necessariamente usare SUBSTRING:\nSELECT 'Quanto è bello DuckDB'[:-7] as slice;\n┌────────────────┐\n│     slice      │\n├────────────────┤\n│ Quanto è bello │\n└────────────────┘\nE si potrebbero fare tanti altri esempi.\n\n\nIntegrazione con altri ambienti\nCon l’estensione sqlite, legge un in modo diretto un database sqlite:\nSELECT * FROM sqlite_scan('test.db', 'tbl_name');\nCon l’estensione spaziale legge file in formato EXCEL:\nSELECT * FROM st_read('test_excel.xlsx', layer='Sheet1');\nCon l’estensione postgres, legge un database PostgreSQL:\n-- usando la stringa di connessione vuota, di default\nSELECT * FROM postgres_scan('', 'public', 'mytable');\nLe API Python portano la potenza di fuoco di DuckDB in modo molto comodo ed efficace dentro questo ambiente di lavoro. E DuckBD può nativamente interrogare Pandas DataFrames, Polars DataFrames e tabelle Arrow.\nimport duckdb\n\n# directly query a Pandas DataFrame\nimport pandas as pd\npandas_df = pd.DataFrame({'a': [42]})\nduckdb.sql('SELECT * FROM pandas_df')\n\n# directly query a Polars DataFrame\nimport polars as pl\npolars_df = pl.DataFrame({'a': [42]})\nduckdb.sql('SELECT * FROM polars_df')\n\n# directly query a pyarrow table\nimport pyarrow as pa\narrow_table = pa.Table.from_pydict({'a':[42]})\nduckdb.sql('SELECT * FROM arrow_table')\nEd è possibile usare DuckDB in R, Julia, tramite ODBC e con tanti altri client.\n\n\nDuckDB e Observable\nDuckDb è nativamente integrato in Observable. E questo gli consente di leggere in modo diretto file in formato CSV, JSON, Apache Arrow e Apache Parquet, e di sfruttare un potente motore di query SQL.\nPrima si attiva il client, puntando alla risorsa di interesse:\npnrrdb = DuckDBClient.of({\n  progetti: FileAttachment(\"PNRR_Progetti-Universo_REGIS_v2.1.parquet\")\n})\n\npnrrdb = DuckDBClient.of({\n  progetti: FileAttachment(\"file/PNRR_Progetti-Universo_REGIS_v2.1.parquet\")\n})\n\n\n\n\n\n\nE poi si può usare il client Javascript DuckDB per fare una query SQL, da integrare ad esempio in una tabella Observable:\n\nInputs.table(\n  pnrrdb.sql`SELECT Missione, SUM(\"Finanziamento PNRR\") AS total_PNRR\n  FROM progetti\n  GROUP BY ALL`, {locale: \"it-IT\"}\n)\n\n\n\n\n\n\n\n\n\n\n\n\nQuesta tabella\n\n\n\nIl codice Observable di esempio di sopra è eseguito al caricamento di questa pagina. E la tabella è proprio un esempio di output live (grazie al fatto che questo sito è basato su Quarto). Questo per mostrarti quanto sia facile e diretto l’uso di DuckDB in Observable. Nel codice per generare la tabella, locale: \"it-IT\" è per avere i numeri all’“italiana”, con il . come separatore delle migliaia e la , come separatore dei decimali."
  },
  {
    "objectID": "posts/duckdb-intro-csv/index.html#dati-opencoesione",
    "href": "posts/duckdb-intro-csv/index.html#dati-opencoesione",
    "title": "Gestire file CSV grandi, brutti e cattivi",
    "section": "Dati OpenCoesione",
    "text": "Dati OpenCoesione\n\nOpenCoesione è l’iniziativa nazionale di governo aperto (open government) sulle politiche di coesione1, coordinata dal Dipartimento per le Politiche di Coesione della Presidenza del Consiglio dei Ministri. Nasce, nel 2012, per favorire un migliore uso delle risorse pubbliche attraverso la diffusione e il riutilizzo di dati e informazioni sugli interventi finanziati con risorse nazionali ed europee, che vengono pubblicati sul portale.\n\nIl portale contiene i dati “a partire dal ciclo 2000-2006 limitatamente ai programmi FSC (Fondo per lo Sviluppo e la Coesione) a titolarità regionale e dal ciclo 2007-2013 per quanto riguarda tutti i programmi nazionali ed europei, aggiornati con cadenza bimestrale”.\n👏 È un progetto che ha fatto e fa scuola. E a proposito di “scuola”, non si può non citare il progetto “A Scuola di OpenCoesione”, che ha coinvolto migliaia di scuole di tutta Italia, portando nelle classi, tra studenti e studentesse, insegnanti e famiglie, il tema della trasparenza e della partecipazione.\nNel portale è presente la sezione dei dati aperti, in cui il dataset più importante è quello dei Progetti con tracciato esteso. Lo useremo come dataset di esempio, per mostrare come sia possibile lavorare comodamente con un file CSV di dimensioni “importanti” e di una certa complessità e ricchezza.\n\nEsplorare il dataset\nIl dataset Progetti con tracciato esteso (quello aggiornato al 30 aprile 2023) è un file CSV compresso in formato ZIP che pesa circa 200 MB; decompresso circa 4 GB e mezzo.\nLanciando unzip -l progetti_esteso_20230430.zip ottengo:\nArchive:  progetti_esteso_20230430.zip\n  Length      Date    Time    Name\n---------  ---------- -----   ----\n4541980767  2023-07-05 23:34   progetti_esteso_20230430.csv\n---------                     -------\n4541980767                     1 file\nÈ molto utile leggere qualche riga del CSV. Per farlo posso lanciare il comando di sotto, che invia il contenuto del file compresso all’utility head: mostra le prime 10 righe del file e si ferma. Lo fa in 0.003 secondi - istantaneamente - perché il file non viene decompresso per intero, ma solo la parte che serve per mostrare le prime 10 righe.\nunzip -p progetti_esteso_20230430.zip | head\nFacendolo riesco a leggere che:\n\nil separatore di campo è il ;;\nla prima riga è una riga di intestazione;\nche ci sono decine di campi (questo lo si può leggere anche dal file dei metadati).\n\nCon qualche sforzo in più di scorrimento a schermo, riesco a vedere che il separatore dei decimali è la , e che i campi con le date sono in un formato che sembra un numero intero, ma in realtà è YYYYMMDD (23 agosto 2021 è espresso come 20210823).\nModificando un po’ il comando di sopra, con unzip -p progetti_esteso_20230430.zip | wc -l, leggo che il file è composto da 1.936.840 righe.\nFatta questa prima esplorazione grezza, di solito voglio “guardare” in modo più “comodo” la tabella dei dati. Potrei salvare le prime 10 righe in un file, aggiungendo al comando di sopra &gt; prime_dieci_righe.csv, e infine aprirlo con un foglio elettronico; io preferisco usare lo straordinario VisiData per farlo. Con unzip -p progetti_esteso_20230430.zip | head | vd -f csv --csv-delimiter=\";\" ad esempio visualizzo le 10 righe di output e tutte le colonne (constatando che sono 199), ma soprattutto mi faccio una prima idea dei contenuti.\n\n\n\nFigura 6: Esempio visualizzazione con VisiData\n\n\nCi sono tantissimi altri strumenti e modalità con cui potrei fare queste ed altre operazioni di esplorazione e analisi, ma questo è un post dedicato in modo particolare a DuckDB e alla sua applicazione a riga di comando. Quindi da qui in poi, utilizzerò soprattutto DuckDB e la sua cli. Per replicare con DuckDB quanto visto sopra in Figura 6, il comando è (metto qualche “a capo” per renderlo più leggibile):\nunzip -p progetti_esteso_20230430.zip | head | \\\n1duckdb -cmd '.mode box' -c '\n2select * from read_csv_auto(\"/dev/stdin\");\n3' | less -S\n\n1\n\n-cmd '.mode box' abilita la vista “box”\n\n2\n\nL’input è lo stdin\n\n3\n\nL’output a less in modo che sia leggibile\n\n\nA schermo ho un’anteprima leggibile e navigabile, come questa di Figura 7.\n\n\n\nFigura 7: Output in modalità box di DuckDB\n\n\nCome detto sopra DuckDB può leggere nativamente file compressi in formato GZIP (e ZSTD). Allora per comodità, ho creato la versione GZIP del file Progetti con tracciato esteso di OpenCoesione.\nLa posso interrogare in modo diretto, applicando la funzione read_csv_auto, che prova a fare l’inferenza della presenza (o no) della linea di intestazione, del separatore dei campi, del separatore dei decimali, del tipo di campo, ecc.:\nduckdb -c '\nSELECT * from read_csv_auto(\"progetti_esteso_20230430.csv.gz\") LIMIT 10\n'\nIn output ho un’anteprima ben leggibile a schermo, in cui vedo 10 righe, alcuni dei campi e il loro tipo e il numero di colonne.\n\n\n\nFigura 8: DuckDb standard output\n\n\nPer vedere tutti i campi posso forzare l’output in CSV, aggiungendo --csv e infine leggerlo con VisiData:\nduckdb --csv -c '\nSELECT * from read_csv_auto(\"progetti_esteso_20230430.csv.gz\") LIMIT 10\n' | vd -f csv\nUn comando comodissimo di DuckDB è SUMMARIZE, che restituisce una tabella esplorativa di sintesi che è una piccola gemma. Per ogni campo restituisce:\n\ntipo di campo;\nvalore minimo;\nvalore massimo;\nnumero di valori univoci approssimato;\nvalore medio;\ndeviazione standard;\nvalore al 25°, 50° e 75° percentile;\nconteggio dei valori.\n\nQui sotto in Tabella 4 un esempio di output per alcune delle colonne del dataset di OpenCoesione. È una tabella di gran valore, perché consente di fare delle prime scelte di analisi e di trasformazione dei dati e avere restituito degli elementi di qualità dei dati.\n\n\n\nTabella 4: Output di esempio di SUMMARIZE\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncolumn_name\ncolumn_type\nmin\nmax\napprox_unique\navg\nstd\nq25\nq50\nq75\ncount\nnull_percentage\n\n\n\n\nOC_TOT_PAGAMENTI_RENDICONTAB_UE\nFLOAT\n-450519.72\n1433693200.0\n449379\n48410.75611172828\n1908955.4857718376\n455.01105803437605\n1815.3021594409709\n9166.19804364237\n1936839\n8.22%\n\n\nCUP_DESCR_SETTORE\nVARCHAR\n\nSERVIZI PER LA P.A. E PER LA COLLETTIVITA’\n12\n\n\n\n\n\n1936839\n0.0%\n\n\nDESCR_INDICATORE_2\nVARCHAR\n\ntematiche oggetto di approfondimento a favore dei beneficiari potenziale ed effettivi\n600\n\n\n\n\n\n1936839\n0.0%\n\n\nFINANZ_DA_REPERIRE\nFLOAT\n0.0\n929000000.0\n241\n19124433.744656816\n79954584.61051819\n42872.88574218749\n408335.86328125\n4609128.875\n1936839\n99.99%\n\n\nOC_FINANZ_UE_FESR_NETTO\nFLOAT\n-209719.28\n1024941760.0\n155171\n48779.41458696303\n2049515.9863344613\n0.0\n0.0\n180.9459324096726\n1936839\n42.73%\n\n\n\n\n\nIl comando si lancia anteponendolo alla lista di record di cui si vuole ottenere una preziosa sintesi:\nduckdb --csv -c '\nSUMMARIZE SELECT * from read_csv_auto(\"progetti_esteso_20230430.csv.gz\")\n'\nMa se lo faccio, ottengo un errore:\nError: Invalid Input Error: Could not convert string 'a.001'\nto INT64 in column \"OC_COD_ARTICOLAZ_PROGRAMMA\", at line 25439.\nQuesto avviene per un problema legato all’inferenza automatica del campo OC_COD_ARTICOLAZ_PROGRAMMA che nelle prime righe contiene valori numerici, ma che in realtà contiene anche caratteri non numerici, con una struttura molto variabile. Ecco alcuni valori di esempio di questo campo: 01 ; 08 ; III ; 04 ; 02 ; 1 ; 03 ; a ; 2 ; I ; 05 ; POCMOLISE ; VIII. Nel file dei metadati questo campo è dichiarato coerentemente come char.\nL’inferenza automatica di un CSV in di DuckDB lavora di default su 20.480 righe. Quando viene fatto su un file compresso, vengono lette le prime 20.480 righe a partire dall’inizio del file. È possibile aumentare il numero di righe da leggere, con il parametro sample_size:\nduckdb --csv -c '\nSUMMARIZE SELECT * from read_csv_auto(\"progetti_esteso_20230430.csv.gz\",sample_size=50000)\n'\nAnche se aumento l’inferenza a 50.000 righe, ho errori. Stavolta però per il campo COD_TIPO_PROCED_ATTIVAZIONE, che di base sembra contenere numeri, ma alle volte contiene il carattere .. Nel file dei metadati questo campo è dichiarato come num. È un errore, dovrebbe essere char.\nPortando il numero di righe a 100.000, non ho più errori. Ed è molto comodo - per un file “grande”, non ottimizzato per le performance e compresso - ottenere l’output di SUMMARIZE in circa 10 secondi. Ma la velocità è niente senza qualità: tanti dei campi sono associati a un tipo di campo non corretto. Queste le ragioni principali:\n\nil separatore dei decimali è la , e non il . (che è un po’ uno standard). Quindi, campi che contengono valori come 5234,14 vengono interpretati come stringhe. Sono le decine di campi con suffisso OC_FINANZ_. Si mappano correttamente se si aggiunge il parametro decimal_separator=\",\" alla funzione read_csv_auto, e se si definiscono tutti i campi numerici come tali.\nC’è però una difformità nell’uso dei separatori dei decimali. Per la grandissima parte dei campi numerici è la ,, ma in campi come PROGRAMMATO_INDICATORE_1 è il .. Quindi se si aggiunge decimal_separator=\",\" (è una dichiarazione globale), si dovrà per campi come PROGRAMMATO_INDICATORE_1 definirli prima come campi di testo e fare poi il casting a FLOAT.\nI campi con le date vengono mappati come numeri interi. Questo in realtà non è un problema ed è coerente con i metadati ufficiali. Il 23 agosto 2021 è espresso come 20210823. Per potere però usare funzioni correlate alle date, è meglio definire i campi come date e non come int. Si fa definendo il formato dei valori delle date, aggiungendo il parametro dateformat='%Y%m%d' alla funzione read_csv_auto e dichiarando i relativi campi come DATE.\n\nOpenCoesione pubblica un file dei metadati: oltre a contenere piccoli errori, non è disponibile anche in un formato che possa essere letto automaticamente da applicazioni e librerie software, non è in formato machine readable (vedi sezione sulla descrizione di un CSV). Questa mancanza rende l’uso di questo CSV non immediato e non scevro da errori.\nPer avere un output del comando SUMMARIZE corretto, in presenza di un CSV non standard (in questo senso) e con alcuni problemi, è necessario scrivere un comando molto “verboso”, come quello sottostante.\n\n\n\n\n\n\nProcedura corretta per SUMMARIZE\n\n\n\n\n\nduckdb --csv -c \"\nSUMMARIZE SELECT *\nREPLACE(\nCAST(PROGRAMMATO_INDICATORE_1 AS FLOAT) AS PROGRAMMATO_INDICATORE_1,\nCAST(PROGRAMMATO_INDICATORE_2 AS FLOAT) AS PROGRAMMATO_INDICATORE_2,\nCAST(PROGRAMMATO_INDICATORE_3 AS FLOAT) AS PROGRAMMATO_INDICATORE_3,\nCAST(PROGRAMMATO_INDICATORE_4 AS FLOAT) AS PROGRAMMATO_INDICATORE_4,\nCAST(REALIZZATO_INDICATORE_1 AS FLOAT) AS REALIZZATO_INDICATORE_1,\nCAST(REALIZZATO_INDICATORE_2 AS FLOAT) AS REALIZZATO_INDICATORE_2,\nCAST(REALIZZATO_INDICATORE_3 AS FLOAT) AS REALIZZATO_INDICATORE_3,\nCAST(REALIZZATO_INDICATORE_4 AS FLOAT) AS REALIZZATO_INDICATORE_4,\n)\nfrom read_csv_auto(\n'progetti_esteso_20230430.csv.gz',SEP=';',dateformat='%Y%m%d',decimal_separator=',',sample_size=100000,\ntypes={'OC_COD_ARTICOLAZ_PROGRAMMA':'VARCHAR','FINANZ_UE':'FLOAT','FINANZ_UE_FESR':'FLOAT','FINANZ_UE_FSE':'FLOAT','FINANZ_UE_FEASR':'FLOAT','FINANZ_UE_FEAMP':'FLOAT','FINANZ_UE_IOG':'FLOAT','FINANZ_STATO_FONDO_DI_ROTAZIONE':'FLOAT','FINANZ_STATO_FSC':'FLOAT','FINANZ_STATO_PAC':'FLOAT','FINANZ_STATO_COMPLETAMENTI':'FLOAT','FINANZ_STATO_ALTRI_PROVVEDIMENTI':'FLOAT','FINANZ_REGIONE':'FLOAT','FINANZ_PROVINCIA':'FLOAT','FINANZ_COMUNE':'FLOAT','FINANZ_RISORSE_LIBERATE':'FLOAT','FINANZ_ALTRO_PUBBLICO':'FLOAT','FINANZ_STATO_ESTERO':'FLOAT','FINANZ_PRIVATO':'FLOAT','FINANZ_DA_REPERIRE':'FLOAT','FINANZ_TOTALE_PUBBLICO':'FLOAT','ECONOMIE_TOTALI':'FLOAT','ECONOMIE_TOTALI_PUBBLICHE':'FLOAT','OC_FINANZ_UE_NETTO':'FLOAT','OC_FINANZ_UE_FESR_NETTO':'FLOAT','OC_FINANZ_UE_FSE_NETTO':'FLOAT','OC_FINANZ_UE_FEASR_NETTO':'FLOAT','OC_FINANZ_UE_FEAMP_NETTO':'FLOAT','OC_FINANZ_UE_IOG_NETTO':'FLOAT','OC_FINANZ_STATO_FONDO_ROT_NETTO':'FLOAT','OC_FINANZ_STATO_FSC_NETTO':'FLOAT','OC_FINANZ_STATO_PAC_NETTO':'FLOAT','OC_FINANZ_STATO_COMPL_NETTO':'FLOAT','OC_FINANZ_STATO_ALTRI_PROV_NETTO':'FLOAT','OC_FINANZ_REGIONE_NETTO':'FLOAT','OC_FINANZ_PROVINCIA_NETTO':'FLOAT','OC_FINANZ_COMUNE_NETTO':'FLOAT','OC_FINANZ_RISORSE_LIBERATE_NETTO':'FLOAT','OC_FINANZ_ALTRO_PUBBLICO_NETTO':'FLOAT','OC_FINANZ_STATO_ESTERO_NETTO':'FLOAT','OC_FINANZ_PRIVATO_NETTO':'FLOAT','OC_FINANZ_TOT_PUB_NETTO':'FLOAT','OC_COSTO_COESIONE':'FLOAT','IMPEGNI':'FLOAT','OC_IMPEGNI_GIURID_VINCOLANTI':'FLOAT','OC_IMPEGNI_TRASFERIMENTI':'FLOAT','OC_IMPEGNI_COESIONE':'FLOAT','TOT_PAGAMENTI':'FLOAT','OC_TOT_PAGAMENTI_BENEFICIARI':'FLOAT','OC_TOT_PAGAMENTI_TRASFERIMENTI':'FLOAT','COSTO_REALIZZATO':'FLOAT','COSTO_RENDICONTABILE_UE':'FLOAT','OC_TOT_PAGAMENTI_RENDICONTAB_UE':'FLOAT','OC_TOT_PAGAMENTI_FSC':'FLOAT','OC_TOT_PAGAMENTI_PAC':'FLOAT','OC_PAGAMENTI_COESIONE':'FLOAT','OC_DATA_INIZIO_PROGETTO':'DATE','OC_DATA_FINE_PROGETTO_PREVISTA':'DATE','OC_DATA_FINE_PROGETTO_EFFETTIVA':'DATE','DATA_INIZIO_PREV_STUDIO_FATT':'DATE','DATA_INIZIO_EFF_STUDIO_FATT':'DATE','DATA_FINE_PREV_STUDIO_FATT':'DATE','DATA_FINE_EFF_STUDIO_FATT':'DATE','DATA_INIZIO_PREV_PROG_PREL':'DATE','DATA_INIZIO_EFF_PROG_PREL':'DATE','DATA_FINE_PREV_PROG_PREL':'DATE','DATA_FINE_EFF_PROG_PREL':'DATE','DATA_INIZIO_PREV_PROG_DEF':'DATE','DATA_INIZIO_EFF_PROG_DEF':'DATE','DATA_FINE_PREV_PROG_DEF':'DATE','DATA_FINE_EFF_PROG_DEF':'DATE','DATA_INIZIO_PREV_PROG_ESEC':'DATE','DATA_INIZIO_EFF_PROG_ESEC':'DATE','DATA_FINE_PREV_PROG_ESEC':'DATE','DATA_FINE_EFF_PROG_ESEC':'DATE','DATA_INIZIO_PREV_AGG_BANDO':'DATE','DATA_INIZIO_EFF_AGG_BANDO':'DATE','DATA_FINE_PREV_AGG_BANDO':'DATE','DATA_FINE_EFF_AGG_BANDO':'DATE','DATA_INIZIO_PREV_STIP_ATTRIB':'DATE','DATA_INIZIO_EFF_STIP_ATTRIB':'DATE','DATA_FINE_PREV_STIP_ATTRIB':'DATE','DATA_FINE_EFF_STIP_ATTRIB':'DATE','DATA_INIZIO_PREV_ESECUZIONE':'DATE','DATA_INIZIO_EFF_ESECUZIONE':'DATE','DATA_FINE_PREV_ESECUZIONE':'DATE','DATA_FINE_EFF_ESECUZIONE':'DATE','DATA_INIZIO_PREV_COLLAUDO':'DATE','DATA_INIZIO_EFF_COLLAUDO':'DATE','DATA_FINE_PREV_COLLAUDO':'DATE','DATA_FINE_EFF_COLLAUDO':'DATE','COD_TIPO_PROCED_ATTIVAZIONE':'VARCHAR','OC_FLAG_REGIONE_UNICA':'INT','DATA_AGGIORNAMENTO':'DATE','OC_FLAG_CUP':'INT','PROGRAMMATO_INDICATORE_1':'VARCHAR','REALIZZATO_INDICATORE_1':'VARCHAR','PROGRAMMATO_INDICATORE_2':'VARCHAR','REALIZZATO_INDICATORE_2':'VARCHAR','PROGRAMMATO_INDICATORE_3':'VARCHAR','REALIZZATO_INDICATORE_3':'VARCHAR','PROGRAMMATO_INDICATORE_4':'VARCHAR','REALIZZATO_INDICATORE_4':'VARCHAR'}\n)\n\" &gt;summarize.csv\n\n\n\nCon questi dati di sintesi emerge ad esempio che circa 36 campi hanno almeno il 90% dei valori nulli. O che in alcuni campi con le date ci sono valori (di minimo o di massimo) strani e probabilmente errati, come quelli in Tabella 5.\n\n\n\nTabella 5: Campi con valori di date “strani”\n\n\ncolumn_name\nmin\nmax\n\n\n\n\nOC_DATA_INIZIO_PROGETTO\n1899-12-30\n2024-02-01\n\n\nDATA_INIZIO_EFF_STIP_ATTRIB\n1899-12-30\n2025-12-31\n\n\nDATA_INIZIO_EFF_AGG_BANDO\n1900-01-01\n2030-03-03\n\n\nDATA_FINE_PREV_STUDIO_FATT\n1987-07-28\n2103-06-30\n\n\nDATA_FINE_EFF_STUDIO_FATT\n1987-07-28\n2103-06-30\n\n\nOC_DATA_FINE_PROGETTO_PREVISTA\n1899-12-30\n2103-12-31\n\n\nDATA_FINE_PREV_ESECUZIONE\n1899-12-30\n2103-12-31\n\n\nDATA_INIZIO_EFF_ESECUZIONE\n1899-12-30\n2211-01-20\n\n\nOC_DATA_FINE_PROGETTO_EFFETTIVA\n1899-12-30\n2211-08-31\n\n\nDATA_FINE_EFF_ESECUZIONE\n1899-12-30\n2211-08-31\n\n\nDATA_FINE_EFF_STIP_ATTRIB\n1899-12-30\n4014-03-07\n\n\nDATA_FINE_PREV_AGG_BANDO\n1991-12-05\n8012-10-08\n\n\n\n\n\nMa soprattutto è possibile constatare quali siano i campi “categorici”, quelli con un numero limitato di valori univoci, che possono essere usati per raggruppare e aggregare i dati e fare emergere elementi interessanti.\nTra i campi categorici OC_MACROAREA, che definisce le macroaree geografiche italiane. E per sapere ad esempio qual è il finanziamento pubblico totale per ciascuna, si può usare la query SQL:\n\nLista 4: Query SQL: totale dei finanziamenti pubblici per macroaree\nSELECT OC_MACROAREA,\nCAST(SUM(FINANZ_TOTALE_PUBBLICO) AS BIGINT) FINANZ_TOTALE_PUBBLICO\nGROUP BY OC_MACROAREA\nORDER BY FINANZ_TOTALE_PUBBLICO DESC\n\nRestituirà qualcosa come in Tabella 6.\n\n\n\n\n\n\n\nTabella 6: Finanziamento pubblico per macro aree\n\n\nMacro area\nFinanziamento totale pubblico (€)\n\n\n\n\nMezzogiorno\n182.919.710.550\n\n\nCentro-Nord\n61.964.843.697\n\n\nAmbito Nazionale\n8.808.650.456\n\n\nAltro\n3.684.521.639\n\n\nEstero\n287.153.594\n\n\n\n\n\n\n\n\ndata = FileAttachment(\"./risorse/sum-by-region.csv\").csv({ typed: true })\nInputs.table(data, {locale: \"it-IT\"})\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n➡️ Purtroppo però non è possibile usare in modo comodo una query come quella di Lista 4. Perché, come scritto sopra, il CSV ha alcuni problemi e non è “standard”. Per eseguirla allora sarebbe necessario inserire la lunghissima lista di parametri per la funzione read_csv_auto e definire i tipi di campo, così come nel caso di SUMMARIZE. Ma sarebbe molto scomodo e poco pratico.\nDue modi per risolvere il problema:\n\ncreare un CSV “pulito” e standard, con i tipi di campo corretti, e lavorare su questo;\ntrasformare il file CSV di input in un file PARQUET e lavorare su questo.\n\n\n\nNote sul dataset progetti\n\n\n\n\n\n\nSu OpenCoesione\n\n\n\nPrima di scrivere alcune note sul dataset, voglio sottolineare alcuni punti su OpenCoesione:\n\nse non ci fossero questi dati, se non fossero dei dati aperti, se non fossero di grande interesse, se non fossero “grandi”, non avrei probabilmente scritto questo post;\nSono poche le banche dati aperte - come questa - che hanno portato a tanti casi di riuso, e da tanto tempo;\nSono pochi i gruppi di lavoro in questo contesto che rispondono a richieste di chiarimento e supporto, a suggerimenti implementativi e a progetti di collaborazione. Il gruppo di OpenCoesione lo fa;\nSono pochi i dataset pubblicati in Italia, corredati da metadati;\nApri il sito, OpenCoesione è molto di più di un singolo dataset.\n\n\n\nA seguire alcune note e proposte/suggerimenti sul dataset dei progetti con tracciato esteso di OpenCoesione (nella versione di aprile 2023):\n\nnel file dei metadati, non si fa riferimento al campo OC_MACROAREA, che è però presente nel dataset. È da aggiungere;\nil COD_TIPO_PROCED_ATTIVAZIONE è dichiarato come num, ma contiene anche stringhe. È da correggere la dichiarazione del tipo di campo;\nc’è una difformità nell’uso dei separatori dei decimali. Per la grandissima parte dei campi numerici è la ,, ma in campi come PROGRAMMATO_INDICATORE_1 è il .;\nci sono alcune celle con problemi di encoding dei caratteri. OpenCoesione ci ha già scritto che su questo punto stanno lavorando e che con la prossima release del dataset, il problema sarà risolto e/o ridotto;\nnei campi con le date ci sono a volte valori “strani”, troppo indietro nel passato o troppo avanti nel futuro (vedi Tabella 5);\ntutti i campi numerici sono dichiarati come num, indipendentemente dal fatto che siano interi o decimali. Sarebbe preferibile avere una distinzione tra int e float.\n\nMetto a parte, infine, delle proposte/suggerimenti a cui tengo particolarmente:\n\nusare come formato di compressione non lo ZIP, ma GZIP. Rende il dataset più pronto all’uso;\nassociare ai metadati in formato XLS anche una descrizione machine readable. O in uno dei formati standard descritti sopra, o anche semplicemente come schema SQL di creazione della tabella. Rende l’operativa sui dati molto più immediata e potenzialmente efficiente;\naffiancare al formato CSV il formato PARQUET. È di per sé ben compresso, rende possibile eseguire analisi complesse in modo quasi immediato da “normali” PC e laptop ed è un formato intrinsecamente ben descritto;\nprodurre dei CSV che siano il più “standard” possibile."
  },
  {
    "objectID": "posts/duckdb-intro-csv/index.html#in-conclusione",
    "href": "posts/duckdb-intro-csv/index.html#in-conclusione",
    "title": "Gestire file CSV grandi, brutti e cattivi",
    "section": "In conclusione",
    "text": "In conclusione\nCon questo lungo post non voglio contribuire a rendere il mondo brutto e fare in modo che il formato CSV sia utilizzato ancora di più. A parte gli scherzi, è certamente un formato con alcune problematicità, ma è ancora molto utilizzato come formato di scambio di dati in forma di testo strutturato.\nCon le dovute precauzioni, con gli strumenti giusti, con un corredo informativo adeguato e per dimensioni non eccessive, può essere un formato comodo e pratico. Descrivendolo, “standardizzandolo” e comprimendolo, diventa molto più usabile. E questo è un invito a chiunque rende disponibili online dati in formato CSV. Finché dura, usiamolo bene.\nUn altro suggerimento (a me per primo) è quello di usare SQL per costruire processi di analisi e di trasformazione dei dati. È un vecchio e diffuso linguaggio, utilizzabile da quasi qualsiasi applicazione e/o linguaggio di programmazione, correlati ai dati. È super documentato, ci sono migliaia di tutorial, messaggi sui forum, libri, ecc.\nE ci sono strumenti come DuckDB. Ok, non è vero che i “I Big Data” sono morti, ma con strumenti come questo, è possibile lavorare in modo comodo e rapido con dati di dimensioni importanti, senza dover ricorrere a infrastrutture complesse e costose.\nHo voluto suggerire l’opportunità di utilizzare il formato Parquet come uno dei formati principali di scambio e di lavoro con i dati.\nE infine voglio sottolineare ancora una volta, che senza dataset come quelli di OpenCoesione, non avrei pensato di scrivere questo post e non sarei riuscito a dargli questo sviluppo."
  },
  {
    "objectID": "posts/duckdb-intro-csv/index.html#buone-letture-e-fonti-di-ispirazione",
    "href": "posts/duckdb-intro-csv/index.html#buone-letture-e-fonti-di-ispirazione",
    "title": "Gestire file CSV grandi, brutti e cattivi",
    "section": "Buone letture e fonti di ispirazione",
    "text": "Buone letture e fonti di ispirazione\n\nBig data is dead\nWhy parquet files are my preferred API for bulk open data\nA Parquet File Is All You Need\nPerformance Explorations of GeoParquet (and DuckDB)\nParquet file format – everything you need to know!\nCSV Loading\nCSV on the Web: A Primer\nParquet devrait remplacer le format CSV\nGeospatial DuckDB"
  },
  {
    "objectID": "til.html",
    "href": "til.html",
    "title": "TIL (Today I Learned)",
    "section": "",
    "text": "Che vuol dire TIL?\n\n\n\nMi piace quando le persone usano il loro sito web, per prendere appunti su alcune delle cose che imparano/scoprono. Come fa il mitico Simon Willison. E Simon lo fa in modalità TIL, ovvero Today I Learned.\nI miei post qui saranno spesso “piccoli”, dei TIL in forma di appunti, con qualche possibile lacuna e/o bruttura.\n\n\n\n\n\n\n\n\n   \n     \n     \n       Ordinare per\n       Predefinito\n         \n          Data - Meno recente\n        \n         \n          Data - Più recente\n        \n         \n          Titolo\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\nData\n\n\nTitolo\n\n\n\n\n\n\n29 giu 2023\n\n\nInstallazione QSV\n\n\n\n\n23 apr 2023\n\n\nDuckDB: l’estensione spaziale\n\n\n\n\n4 mar 2023\n\n\nDuckDB: creare un file parquet a partire da file di testo di grandi dimensioni\n\n\n\n\n26 feb 2023\n\n\nQuarto: applicare stile CSS\n\n\n\n\n28 gen 2023\n\n\nEstrarre la lista dei file creati più di 30 giorni fa\n\n\n\n\n7 gen 2023\n\n\nPagina con codice R, Python e utility Bash\n\n\n\n\n3 dic 2022\n\n\nNushell: installarlo con il supporto ai dataframe\n\n\n\n\n28 nov 2022\n\n\nDuckDB: creare un file parquet a partire da un CSV\n\n\n\n\n26 nov 2022\n\n\nQuarto: renderizzare una tabella a partire da un CSV\n\n\n\n\n22 nov 2022\n\n\nFare convivere una cella Observable e un grafico Altair in Quarto\n\n\n\n\n21 nov 2022\n\n\nQuarto: leggere un CSV via Obeservable e visualizzare i dati\n\n\n\n\n20 nov 2022\n\n\nCome filtrare un file di testo a partire da una lista di stringhe\n\n\n\n\n19 nov 2022\n\n\nIl mio primo blog post\n\n\n\n\n\n\nNessun risultato\n\n Torna in cima"
  },
  {
    "objectID": "til/compilare-qsv/index.html",
    "href": "til/compilare-qsv/index.html",
    "title": "Installazione QSV",
    "section": "",
    "text": "Di base i comandi sono questi di sotto:\ngit clone https://github.com/jqnatividad/qsv.git\ncd qsv\ncargo build --release --locked --bin qsv -F all_features\nPer il mio ambiente è comodo, prima della compilazione, settare la variabile d’ambiente CARGO_BUILD_RUSTFLAGS, per avere in output un binario ottimizzato per la mia CPU:\nexport CARGO_BUILD_RUSTFLAGS='-C target-cpu=native'\nÈ consigliato avere un ambiente “pulito” prima della compilazione. Quindi la procedura potrebbe diventare questa:\ngit clone https://github.com/jqnatividad/qsv.git\nrustup up\ncargo clean\ncd qsv\ncargo build --release --locked --bin qsv -F all_features\nSe si ha poca RAM (meno di 16GB), è meglio rinunciare ad alcun feature (come to, che è oneroso da compilare):\ngit clone https://github.com/jqnatividad/qsv.git\nrustup up\ncargo clean\ncd qsv\ncargo build --release --locked --bin qsv -F feature_capable,polars\n\n\n\n Torna in cima"
  },
  {
    "objectID": "til/duckdb-creare-parquet-csv/index.html",
    "href": "til/duckdb-creare-parquet-csv/index.html",
    "title": "DuckDB: creare un file parquet a partire da un CSV",
    "section": "",
    "text": "DuckDB ha una cli molto comoda e potente.\nSe si vuole ad esempio creare il file parquet del file CSV degli Indicatori di rischio idrogeologico pubblicati da ISPRA, questo è il comando da lanciare:\nduckdb -c \"CREATE TABLE comuni_pir AS SELECT * FROM comuni_pir.csv;EXPORT DATABASE '.' (FORMAT PARQUET);\"\n\nviene creata una tabella comuni_pir in un db temporaneo, a partire dal file CSV;\nviene esportato il db in formato parquet (che conterrà una sola tabella), nella directory corrente;\n-c per eseguire i due comandi, separati da ; e poi uscire.\n\n\n\n\n\n\n\nAttenzione all’inferencing dei tipi di campo\n\n\n\nI campi di un file CSV non sono associati a una definizione di tipo di campo. DuckDB in import farà il cosiddetto inferencing, ovvero proverà a dedurlo.Non è detto che lo faccia correttamente ed è bene sempre fare un check (celle con valori come 08, 09, ecc. sono ad esempio spesso mappate come numeri e non come stringhe).\n\n\n\n\n\n Torna in cima"
  },
  {
    "objectID": "til/duckdb-ottimizzazione-performance-import-csv-jsonl-creazione-parquet/index.html",
    "href": "til/duckdb-ottimizzazione-performance-import-csv-jsonl-creazione-parquet/index.html",
    "title": "DuckDB: creare un file parquet a partire da file di testo di grandi dimensioni",
    "section": "",
    "text": "In queste settimane ho guardato un po’ i dati della “Banca dati Servizio Contratti Pubblici - SCP” che contiene gli avvisi, i bandi e gli esiti di gara in formato aperto, raccolti dalla “Banca dati SCP - Servizio Contratti Pubblici”, gestita dalla Direzione Generale per la regolazione e i contratti pubblici del Ministero delle Infrastrutture e Trasporti.\nNel dataset sono presenti file CSV di medie dimensioni, come quello denominato v_od_atti.csv, composto da 685.000 righe per 45 colonne, per un totale di circa 570 MB.\nNon sono big data e ci sono tanti modi per interrogarlo e trasformarlo con poco sforzo e rapidità. Uno molto comodo è quello di usare DuckDB: prima per la conversione di formato e poi per tutte le query che si vorranno fare.\nMolto comodo convertire il CSV in formato parquet. Si passa da circa 570 a 45 MB, e si ha a disposizione un formato che è rapidissimo da interrogare.\nPer farlo si può usare DuckDB a riga di comando:\necho \"COPY (SELECT *\nFROM read_csv_auto('input.csv'))\nTO 'output.parquet' (FORMAT 'PARQUET',\nCODEC  'Snappy',PER_THREAD_OUTPUT TRUE);\" \\\n| duckdb\nE in 10 secondi (sulla mia macchina con 16 GB di RAM e un pentium 7) il file è pronto.\nLa preziosa fonte/ispirazione è il bravissimo Mark Litwintschik.\n\n\n\n Torna in cima"
  },
  {
    "objectID": "til/filtrare-file-elenco-file-esterno/index.html",
    "href": "til/filtrare-file-elenco-file-esterno/index.html",
    "title": "Come filtrare un file di testo a partire da una lista di stringhe",
    "section": "",
    "text": "Avevo bisogno di filtrare un file CSV di grandi dimensioni, compresso in zip, a partire da una lista di stringhe contenute in un file. Dato il CSV, volevo estrarne soltanto le righe che contenevano una delle stringhe presenti nel file esterno.\nVia CLI, usando lo straordinario grep il comando è (list.txt, è il file che contiene per ogni riga la stringa da cercare):\nunzip -qq -c \"input.zip\"  | grep -F -f list.txt\nPer me questa modalità ha risolto tutto. Ma ne metto un paio di altre.\nUna è basata su ripgrep, un’altra straordinaria CLI per la ricerca di testo, più rapida di grep:\nunzip -qq -c \"input.zip\"  | rg -F -f list.txt\n\n\n\n\n\n\nNon si tiene conto del formato\n\n\n\nQueste due modalità non tengono però conto del formato CSV, non riescono ad esempio a cercare per colonna, ma solo per riga. Sotto una soluzione che riesce a farlo.\n\n\nCon qsv, è possibile ricercare per colonna:\nunzip -qq -c \"input.zip\"  | qsv searchset -d \"|\" -i -s nomeColonna list.txt\n\n\n\n\n\n\nNota\n\n\n\n\n-d \"|\" per impostare il separatore di colonna del CSV;\n-i per ignorare maiuscole e minuscole;\n-s nomeColonna per specificare la colonna in cui cercare.\n\n\n\n\n\n\n Torna in cima"
  },
  {
    "objectID": "til/installare-duckdb-spatial/index.html",
    "href": "til/installare-duckdb-spatial/index.html",
    "title": "DuckDB: l’estensione spaziale",
    "section": "",
    "text": "È stata rilasciata questa estensione spaziale per DuckDB.\nUno dei modi per istallarla è scaricare i binari precompilati, accessibili dai workflow di compilazione.\n\n\n\nI workflow, per i vari sistemi operativi\n\n\nPer installarla:\n\ndecomprimere il file scaricato;\nlanciare duckdb con l’opzione unsigned, ovvero duckdb -unsigned;\ninstallare l’estensione usando il percorso assoluto del file (sotto un esempio)\n\ninstall '/home/user/spatial.duckdb_extension';\n\ncaricare l’estensione, con LOAD spatial;.\n\nE una volta caricata, potrai vedere tutti i nuovi formati file supportati da duckdb con\nselect * from ST_LIST_DRIVERS() order by 1;\n\n\n\nLa lista dei formati disponibili\n\n\n\n\n\n Torna in cima"
  },
  {
    "objectID": "til/mescolare-r-python-bash/index.html",
    "href": "til/mescolare-r-python-bash/index.html",
    "title": "Pagina con codice R, Python e utility Bash",
    "section": "",
    "text": "Ad esempio voglio usare Miller per calcolare la somma di un campo di un file CSV.\nUso system in r, per lanciare un comando di sistema (in questo caso sono in ambiente Linux), e associo l’output a una variabile.\n\n```{r}\nsum &lt;- system('mlr --c2n stats1 -a sum -f a input.csv', intern = TRUE)\n```\n\nCosì facendo posso usare un’opzione comodissima dell’engine knitr, che mi consente di inserire il riferimento a una variabile r (o un comando r) all’interno di un testo markdown.\nSe scrivo ad esempio\n\nLa somma è `r sum`.\n\nAvrò restituito\nLa somma è 9.\nE tramite il package di r reticulate (qui un tutorial a tema), posso passare la variabile r a un blocco di codice python:\n\n```{python}\nsum_py = r.sum\nprint(sum_py)\n```\n\n9\n\n\n\n\n\n Torna in cima"
  },
  {
    "objectID": "til/nushell-installare-supporto-dataframe/index.html",
    "href": "til/nushell-installare-supporto-dataframe/index.html",
    "title": "Nushell: installarlo con il supporto ai dataframe",
    "section": "",
    "text": "Dalla release 0.72 di nushell il supporto ai dataframe non è abilitato di default.\nQuesta una modalità di compilarlo, con il supporto abilitato.\n# clona il repository\ngit clone https://github.com/nushell/nushell.git\n\ncd nushell\n\ncargo install --path=. --all-features\nVerrà installato in /home/username/.cargo/bin/nu.\n\n\n\n Torna in cima"
  },
  {
    "objectID": "til/nushell-lista-file-piu-vecchi-di/index.html",
    "href": "til/nushell-lista-file-piu-vecchi-di/index.html",
    "title": "Estrarre la lista dei file creati più di 30 giorni fa",
    "section": "",
    "text": "La data di creazione non è un parametro disponibile e/o interrogabile su tutti i tipi di file sytem.\nIl meraviglioso nushell riesce a farlo un po’ ovunque.\nQuesto un esempio:\nls **\\*  -l | where created &lt;= (date now) - 30day\nAlcune note:\n\nlegge la cartella corrente e tutte le sue sottocartelle con **\\* (qui c’è il forward slash e quindi è per sistemi Windows; su Linux è **/*);\nfiltra tutti i file creati più di 30 giorni fa con where created &lt;= (date now) - 30day.\n\nSe si vogliono cancellare gli elementi presenti nella lista del comando precedente:\nls **\\*  -l | where created &lt;= (date now) - 30day | each { rm $in.name }\n$in è una variabile creata automaticamente in corrispondenza di una lista.\n\n\n\n Torna in cima"
  },
  {
    "objectID": "til/quarto-applicare-stili-span/index.html",
    "href": "til/quarto-applicare-stili-span/index.html",
    "title": "Quarto: applicare stile CSS",
    "section": "",
    "text": "Se voglio applicare ad esempio a una sola parola uno stile definito inline, basterà fare come sotto:\nL'arancia è [arancione]{style=\"color:#ffa500\"}.\nL’arancia è arancione.\nSe invece voglio associare a una frase uno stile di bootstrap (come quelli sui pulsanti), potrò fare in questo modo:\n[☝️ Partecipa]{.btn .btn-success}\n☝️ Partecipa\nSe infine voglio applicare una determinata classe, seguita da un attributo personalizzato, in modo da poter associare uno stile personalizzato tramite uno specifico CSS Selector, potrò fare così:\n[Lorem ipsum]{.class key=\"val\"}\nIl codice HTML generato sarà:\n&lt;p&gt;\n  &lt;span class=\"class\" data-key=\"val\"&gt;Lorem ipsum&lt;/span&gt;\n&lt;/p&gt;\n\n\n\n Torna in cima"
  },
  {
    "objectID": "til/quarto-mescolare-observable-altair/index.html",
    "href": "til/quarto-mescolare-observable-altair/index.html",
    "title": "Fare convivere una cella Observable e un grafico Altair in Quarto",
    "section": "",
    "text": "Non è possibile in Quarto fare convivere una cella di codice di tipo Observable, con una cella Python con un grafico Altair.\n\n\nVedi issue 3424\nC’è però un workaround:\n\nda Altair generare la descrizione del grafico in formato JSON (è in formato vega-lite), con chart.to_json();\nfare leggere a una cella Observable il JSON, e visualizzare il grafico.\n\nQui ad esempio creo una cella Observable usata soltanto come esempio.\n\n```{ojs}\n//| echo: fenced\ndata = FileAttachment(\"ojs.csv\").csv({ typed: true })\nInputs.table(data)\n```\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPoi genero la descrizione di un grafico vega-lite, con Altair, salvando il file chart.json.\n\n```{python}\nimport pandas as pd\nimport altair as alt\nimport warnings\n\nwarnings.simplefilter(action='ignore', category=FutureWarning)\n\ndf = pd.read_csv(\"altair.csv\",keep_default_na=False)\n\ndf['year'] = pd.to_datetime(df['year'], format='%Y')\n\nchart=alt.Chart(df).mark_area().encode(\n    alt.X('year:T', timeUnit = 'year',title='year',axis=alt.Axis(tickCount='year')),\n    alt.Y('v:Q',axis=alt.Axis(format='%'),title='percentage'),\n    color='i:N'\n)\nchart.save('chart.json')\n```\n\nE infine faccio leggere a Observable la descrizione del grafico, che è stata generata da Altair e lo faccio visualizzare.\n\n```{ojs}\n//| echo: fenced\nfile = FileAttachment(\"chart.json\").json()\nembed = require(\"vega-embed@6\")\nembed(file)\n```\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Torna in cima"
  },
  {
    "objectID": "til/quarto-renderizzare-tabelle-r/index.html",
    "href": "til/quarto-renderizzare-tabelle-r/index.html",
    "title": "Quarto: renderizzare una tabella a partire da un CSV",
    "section": "",
    "text": "Per prima cosa carico delle librerie per leggere il CSV e per renderizzare la tabella, e carico la tabella:\n```{r}\nlibrary(tidyverse)\nlibrary(lubridate)\nlibrary(knitr)\n\nt = read_csv(\"input.csv\")\n```\nE poi la renderizzo in vari modi.\n\n```{r}\nkable(t)\n```\n\n\n\n\nyear\ni\nv\n\n\n\n\n2016\nF\n0.9599717\n\n\n2016\nG\n0.0382419\n\n\n2016\nNA\n0.0012658\n\n\n2016\nW\n0.0000122\n\n\n2016\nS\n0.0000454\n\n\n2016\nO\n0.0004631\n\n\n2017\nF\n0.9598036\n\n\n2017\nG\n0.0384042\n\n\n2017\nC\n0.0012674\n\n\n2017\nW\n0.0000153\n\n\n2017\nS\n0.0000486\n\n\n2017\nO\n0.0004608\n\n\n2018\nF\n0.9598013\n\n\n\n\n\n\n```{r}\n#| tbl-cap: \"Soltanto le prime righe\"\nkable(head(t))\n```\n\n\nSoltanto le prime righe \n\n\nyear\ni\nv\n\n\n\n\n2016\nF\n0.9599717\n\n\n2016\nG\n0.0382419\n\n\n2016\nNA\n0.0012658\n\n\n2016\nW\n0.0000122\n\n\n2016\nS\n0.0000454\n\n\n2016\nO\n0.0004631\n\n\n\n\n\n\n```{r}\n#| tbl-cap: \"Selezionare righe e colonne\"\nkable(t[1:4, 1:2])\n```\n\n\nSelezionare righe e colonne \n\n\nyear\ni\n\n\n\n\n2016\nF\n\n\n2016\nG\n\n\n2016\nNA\n\n\n2016\nW\n\n\n\n\n\n\n```{r}\n#| tbl-cap: \"Paginazione\"\n\nrmarkdown::paged_table(t)\n```\n\n\n Paginazione\n  \n\n\n\n\n\n\n Torna in cima"
  },
  {
    "objectID": "til/renderizzare-csv-quarto-observable/index.html",
    "href": "til/renderizzare-csv-quarto-observable/index.html",
    "title": "Quarto: leggere un CSV via Obeservable e visualizzare i dati",
    "section": "",
    "text": "Si può usare semplicemente il metodo FileAttachment di Observable, per data.csv\n```{ojs}\ndata = FileAttachment(\"data.csv\").csv({ typed: true })\nInputs.table(data)\n```\nper ottenere\n\ndata = FileAttachment(\"data.csv\").csv({ typed: true })\nInputs.table(data)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nO in alternativa con arquero (ma vale la pena usarlo anche per trasformare i dati)\n\n```{ojs}\n//| echo: fenced\nimport { aq, op } from '@uwdata/arquero'\ndati_aquero = aq.loadCSV(\"data.csv\")\n\ndati_aquero.view()\n```\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIl Quarto del titolo è lui https://quarto.org/\n\n\n\n\n\n Torna in cima"
  },
  {
    "objectID": "til/un-sito-in-quarto/index.html",
    "href": "til/un-sito-in-quarto/index.html",
    "title": "Il mio primo blog post",
    "section": "",
    "text": "Uso quarto da diverse settimane per creare slide in HTML scritte in markdown e basate su reveal.js.\nQuarto è un sistema di pubblicazione scientifica e tecnica, open source, basato su Pandoc:\n\nCrea contenuti dinamici con Python, R, Julia e Observable;\nI documenti sono o file markdown in plain text o Jupyter notebook;\nConsente di pubblicare articoli, report, presentazioni, siti Web, blog e libri di alta qualità in HTML, PDF, MS Word, ePub e altri formati;\nConsente di creare contenuti utilizzando scientific markdown, incluse equazioni, citazioni, riferimenti incrociati, pannelli di immagini, didascalie, layout avanzato e altro ancora.\n\nQuello che ho fatto per creare la prima versione di questo sito è stato:\n\nInstallare quarto;\ncreare un nuovo progetto, dandogli per nome il mio profilo utente GitHub;\n\nquarto create-project aborruso.github.io --type website\n\nimpostare a docs la cartella di output di pubblicazione del sito, aggiungendo l’istruzione nel file _quarto.yml:\n\n\n\n_quarto.yml\n\nproject:\n  type: website\n  output-dir: docs\n\n\ncreare dei primi contenuti da pubblicare come questo post;\ncreare il repo aborruso.github.io su GitHub;\ngenerare il sito con il comando render\n\nquarto render ./\n\nimpostare come sorgente delle GitHub Pages del repo creato, la cartella docs citata sopra;\npubblicare tutto su GitHub.\n\nPer farlo, mi hanno aiutato queste letture:\n\nCreating your personal website using Quarto https://ucsb-meds.github.io/creating-quarto-websites/\nCreating a Website (dal sito ufficiale) https://quarto.org/docs/websites/\nCreating a Blog (dal sito ufficiale) https://quarto.org/docs/websites/website-blog.html\n\n\n\n\n\n\n\nImportante\n\n\n\nHo seguito questi step per la primissima pubblicazione, per vedere subito un primo risultato. Poi ho cambiato molte cose, quindi le impostazioni attuali sono diverse da quelle descritte sopra.\n\n\n\n\n\n Torna in cima"
  }
]