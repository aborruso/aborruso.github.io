[
  {
    "objectID": "til/un-sito-in-quarto/index.html",
    "href": "til/un-sito-in-quarto/index.html",
    "title": "Il mio primo blog post",
    "section": "",
    "text": "Uso quarto da diverse settimane per creare slide in HTML scritte in markdown e basate su reveal.js.\nQuarto √® un sistema di pubblicazione scientifica e tecnica, open source, basato su Pandoc:\n\nCrea contenuti dinamici con Python, R, Julia e Observable;\nI documenti sono o file markdown in plain text o Jupyter notebook;\nConsente di pubblicare articoli, report, presentazioni, siti Web, blog e libri di alta qualit√† in HTML, PDF, MS Word, ePub e altri formati;\nConsente di creare contenuti utilizzando scientific markdown, incluse equazioni, citazioni, riferimenti incrociati, pannelli di immagini, didascalie, layout avanzato e altro ancora.\n\nQuello che ho fatto per creare la prima versione di questo sito √® stato:\n\nInstallare quarto;\ncreare un nuovo progetto, dandogli per nome il mio profilo utente GitHub;\n\nquarto create-project aborruso.github.io --type website\n\nimpostare a docs la cartella di output di pubblicazione del sito, aggiungendo l‚Äôistruzione nel file _quarto.yml:\n\n\n\n_quarto.yml\n\nproject:\n  type: website\n  output-dir: docs\n\n\ncreare dei primi contenuti da pubblicare come questo post;\ncreare il repo aborruso.github.io su GitHub;\ngenerare il sito con il comando render\n\nquarto render ./\n\nimpostare come sorgente delle GitHub Pages del repo creato, la cartella docs citata sopra;\npubblicare tutto su GitHub.\n\nPer farlo, mi hanno aiutato queste letture:\n\nCreating your personal website using Quarto https://ucsb-meds.github.io/creating-quarto-websites/\nCreating a Website (dal sito ufficiale) https://quarto.org/docs/websites/\nCreating a Blog (dal sito ufficiale) https://quarto.org/docs/websites/website-blog.html\n\n\n\n\n\n\n\nImportante\n\n\n\nHo seguito questi step per la primissima pubblicazione, per vedere subito un primo risultato. Poi ho cambiato molte cose, quindi le impostazioni attuali sono diverse da quelle descritte sopra.\n\n\n\n\n\n Torna in cima"
  },
  {
    "objectID": "til/quarto-renderizzare-tabelle-r/index.html",
    "href": "til/quarto-renderizzare-tabelle-r/index.html",
    "title": "Quarto: renderizzare una tabella a partire da un CSV",
    "section": "",
    "text": "Per prima cosa carico delle librerie per leggere il CSV e per renderizzare la tabella, e carico la tabella:\n```{r}\nlibrary(tidyverse)\nlibrary(lubridate)\nlibrary(knitr)\n\nt = read_csv(\"input.csv\")\n```\nE poi la renderizzo in vari modi.\n\n```{r}\nkable(t)\n```\n\n\n\n\nyear\ni\nv\n\n\n\n\n2016\nF\n0.9599717\n\n\n2016\nG\n0.0382419\n\n\n2016\nNA\n0.0012658\n\n\n2016\nW\n0.0000122\n\n\n2016\nS\n0.0000454\n\n\n2016\nO\n0.0004631\n\n\n2017\nF\n0.9598036\n\n\n2017\nG\n0.0384042\n\n\n2017\nC\n0.0012674\n\n\n2017\nW\n0.0000153\n\n\n2017\nS\n0.0000486\n\n\n2017\nO\n0.0004608\n\n\n2018\nF\n0.9598013\n\n\n\n\n\n\n```{r}\n#| tbl-cap: \"Soltanto le prime righe\"\nkable(head(t))\n```\n\n\nSoltanto le prime righe\n\n\nyear\ni\nv\n\n\n\n\n2016\nF\n0.9599717\n\n\n2016\nG\n0.0382419\n\n\n2016\nNA\n0.0012658\n\n\n2016\nW\n0.0000122\n\n\n2016\nS\n0.0000454\n\n\n2016\nO\n0.0004631\n\n\n\n\n\n\n```{r}\n#| tbl-cap: \"Selezionare righe e colonne\"\nkable(t[1:4, 1:2])\n```\n\n\nSelezionare righe e colonne\n\n\nyear\ni\n\n\n\n\n2016\nF\n\n\n2016\nG\n\n\n2016\nNA\n\n\n2016\nW\n\n\n\n\n\n\n```{r}\n#| tbl-cap: \"Paginazione\"\n\nrmarkdown::paged_table(t)\n```\n\n Paginazione\n  \n\n\n\n\n\n\n Torna in cima"
  },
  {
    "objectID": "til/quarto-applicare-stili-span/index.html",
    "href": "til/quarto-applicare-stili-span/index.html",
    "title": "Quarto: applicare stile CSS",
    "section": "",
    "text": "Se voglio applicare ad esempio a una sola parola uno stile definito inline, baster√† fare come sotto:\nL'arancia √® [arancione]{style=\"color:#ffa500\"}.\nL‚Äôarancia √® arancione.\nSe invece voglio associare a una frase uno stile di bootstrap (come quelli sui pulsanti), potr√≤ fare in questo modo:\n[‚òùÔ∏è Partecipa]{.btn .btn-success}\n‚òùÔ∏è Partecipa\nSe infine voglio applicare una determinata classe, seguita da un attributo personalizzato, in modo da poter associare uno stile personalizzato tramite uno specifico CSS Selector, potr√≤ fare cos√¨:\n[Lorem ipsum]{.class key=\"val\"}\nIl codice HTML generato sar√†:\n&lt;p&gt;\n  &lt;span class=\"class\" data-key=\"val\"&gt;Lorem ipsum&lt;/span&gt;\n&lt;/p&gt;\n\n\n\n Torna in cima"
  },
  {
    "objectID": "til/nushell-lista-file-piu-vecchi-di/index.html",
    "href": "til/nushell-lista-file-piu-vecchi-di/index.html",
    "title": "Estrarre la lista dei file creati pi√π di 30 giorni fa",
    "section": "",
    "text": "La data di creazione non √® un parametro disponibile e/o interrogabile su tutti i tipi di file sytem.\nIl meraviglioso nushell riesce a farlo un po‚Äô ovunque.\nQuesto un esempio:\nls **\\*  -l | where created &lt;= (date now) - 30day\nAlcune note:\n\nlegge la cartella corrente e tutte le sue sottocartelle con **\\* (qui c‚Äô√® il forward slash e quindi √® per sistemi Windows; su Linux √® **/*);\nfiltra tutti i file creati pi√π di 30 giorni fa con where created &lt;= (date now) - 30day.\n\nSe si vogliono cancellare gli elementi presenti nella lista del comando precedente:\nls **\\*  -l | where created &lt;= (date now) - 30day | each { rm $in.name }\n$in √® una variabile creata automaticamente in corrispondenza di una lista.\n\n\n\n Torna in cima"
  },
  {
    "objectID": "til/mescolare-r-python-bash/index.html",
    "href": "til/mescolare-r-python-bash/index.html",
    "title": "Pagina con codice R, Python e utility Bash",
    "section": "",
    "text": "Ad esempio voglio usare Miller per calcolare la somma di un campo di un file CSV.\nUso system in r, per lanciare un comando di sistema (in questo caso sono in ambiente Linux), e associo l‚Äôoutput a una variabile.\n\n```{r}\nsum &lt;- system('mlr --c2n stats1 -a sum -f a input.csv', intern = TRUE)\n```\n\nCos√¨ facendo posso usare un‚Äôopzione comodissima dell‚Äôengine knitr, che mi consente di inserire il riferimento a una variabile r (o un comando r) all‚Äôinterno di un testo markdown.\nSe scrivo ad esempio\n\nLa somma √® `r sum`.\n\nAvr√≤ restituito\nLa somma √® 9.\nE tramite il package di r reticulate (qui un tutorial a tema), posso passare la variabile r a un blocco di codice python:\n\n```{python}\nsum_py = r.sum\nprint(sum_py)\n```\n\n9\n\n\n\n\n\n Torna in cima"
  },
  {
    "objectID": "til/filtrare-file-elenco-file-esterno/index.html",
    "href": "til/filtrare-file-elenco-file-esterno/index.html",
    "title": "Come filtrare un file di testo a partire da una lista di stringhe",
    "section": "",
    "text": "Avevo bisogno di filtrare un file CSV di grandi dimensioni, compresso in zip, a partire da una lista di stringhe contenute in un file. Dato il CSV, volevo estrarne soltanto le righe che contenevano una delle stringhe presenti nel file esterno.\nVia CLI, usando lo straordinario grep il comando √® (list.txt, √® il file che contiene per ogni riga la stringa da cercare):\nunzip -qq -c \"input.zip\"  | grep -F -f list.txt\nPer me questa modalit√† ha risolto tutto. Ma ne metto un paio di altre.\nUna √® basata su ripgrep, un‚Äôaltra straordinaria CLI per la ricerca di testo, pi√π rapida di grep:\nunzip -qq -c \"input.zip\"  | rg -F -f list.txt\n\n\n\n\n\n\nNon si tiene conto del formato\n\n\n\nQueste due modalit√† non tengono per√≤ conto del formato CSV, non riescono ad esempio a cercare per colonna, ma solo per riga. Sotto una soluzione che riesce a farlo.\n\n\nCon qsv, √® possibile ricercare per colonna:\nunzip -qq -c \"input.zip\"  | qsv searchset -d \"|\" -i -s nomeColonna list.txt\n\n\n\n\n\n\nNota\n\n\n\n\n-d \"|\" per impostare il separatore di colonna del CSV;\n-i per ignorare maiuscole e minuscole;\n-s nomeColonna per specificare la colonna in cui cercare.\n\n\n\n\n\n\n Torna in cima"
  },
  {
    "objectID": "til/duckdb-ottimizzazione-performance-import-csv-jsonl-creazione-parquet/index.html",
    "href": "til/duckdb-ottimizzazione-performance-import-csv-jsonl-creazione-parquet/index.html",
    "title": "DuckDB: creare un file parquet a partire da file di testo di grandi dimensioni",
    "section": "",
    "text": "In queste settimane ho guardato un po‚Äô i dati della ‚ÄúBanca dati Servizio Contratti Pubblici - SCP‚Äù che contiene gli avvisi, i bandi e gli esiti di gara in formato aperto, raccolti dalla ‚ÄúBanca dati SCP - Servizio Contratti Pubblici‚Äù, gestita dalla Direzione Generale per la regolazione e i contratti pubblici del Ministero delle Infrastrutture e Trasporti.\nNel dataset sono presenti file CSV di medie dimensioni, come quello denominato v_od_atti.csv, composto da 685.000 righe per 45 colonne, per un totale di circa 570 MB.\nNon sono big data e ci sono tanti modi per interrogarlo e trasformarlo con poco sforzo e rapidit√†. Uno molto comodo √® quello di usare DuckDB: prima per la conversione di formato e poi per tutte le query che si vorranno fare.\nMolto comodo convertire il CSV in formato parquet. Si passa da circa 570 a 45 MB, e si ha a disposizione un formato che √® rapidissimo da interrogare.\nPer farlo si pu√≤ usare DuckDB a riga di comando:\necho \"COPY (SELECT *\nFROM read_csv_auto('input.csv'))\nTO 'output.parquet' (FORMAT 'PARQUET',\nCODEC  'Snappy',PER_THREAD_OUTPUT TRUE);\" \\\n| duckdb\nE in 10 secondi (sulla mia macchina con 16 GB di RAM e un pentium 7) il file √® pronto.\nLa preziosa fonte/ispirazione √® il bravissimo Mark Litwintschik.\n\n\n\n Torna in cima"
  },
  {
    "objectID": "til/compilare-qsv/index.html",
    "href": "til/compilare-qsv/index.html",
    "title": "Installazione QSV",
    "section": "",
    "text": "Di base i comandi sono questi di sotto:\ngit clone https://github.com/jqnatividad/qsv.git\ncd qsv\ncargo build --release --locked --bin qsv -F all_features\nPer il mio ambiente √® comodo, prima della compilazione, settare la variabile d‚Äôambiente CARGO_BUILD_RUSTFLAGS, per avere in output un binario ottimizzato per la mia CPU:\nexport CARGO_BUILD_RUSTFLAGS='-C target-cpu=native'\n√à consigliato avere un ambiente ‚Äúpulito‚Äù prima della compilazione. Quindi la procedura potrebbe diventare questa:\ngit clone https://github.com/jqnatividad/qsv.git\ncd qsv\nrustup up\ncargo clean\ncargo build --release --locked --bin qsv -F all_features\nSe si ha poca RAM (meno di 16GB), √® meglio rinunciare ad alcun feature (come to, che √® oneroso da compilare):\ngit clone https://github.com/jqnatividad/qsv.git\ncd qsv\nrustup up\ncargo clean\ncargo build --release --locked --bin qsv -F feature_capable,apply,python,self_update,polars\nL‚Äôeseguibile compilato di qsv sar√† generato in target/release/qsv. Se lo si vuole rendere disponibile a tutti gli utenti del sistema, si pu√≤ copiare in /usr/local/bin o in qualsiasi altra cartella del PATH.\n\n\n\n Torna in cima"
  },
  {
    "objectID": "posts/accedere-google-ai-riga-di-comando/index.html",
    "href": "posts/accedere-google-ai-riga-di-comando/index.html",
    "title": "Usare la nuova intelligenza artificiale di Google",
    "section": "",
    "text": "üòâ Questo post √® per Cesare Gerbino."
  },
  {
    "objectID": "posts/accedere-google-ai-riga-di-comando/index.html#introduzione",
    "href": "posts/accedere-google-ai-riga-di-comando/index.html#introduzione",
    "title": "Usare la nuova intelligenza artificiale di Google",
    "section": "Introduzione",
    "text": "Introduzione\nGoogle ha lanciato a inizio dicembre del 2023 Gemini, il suo modello di intelligenza artificiale migliore. Pu√≤ comprendere e combinare diversi tipi di informazioni, come testo, codice, audio, immagini e video.\nDa poco sono disponibili le API e ho voluto fare qualche test di base, usando la riga di comando.\n\n\n\n\n\n\nNota\n\n\n\nAl momento le API sono accessibili soltanto dagli Stati Uniti, quindi bisogna usare un VPN. Io ho usato quella gratuita di Proton VPN (grazie a Francesco Passantino per il suggerimento di anni fa).\n\n\nA seguire una mini guida per testarle"
  },
  {
    "objectID": "posts/accedere-google-ai-riga-di-comando/index.html#connessione-alla-vpn",
    "href": "posts/accedere-google-ai-riga-di-comando/index.html#connessione-alla-vpn",
    "title": "Usare la nuova intelligenza artificiale di Google",
    "section": "Connessione alla VPN",
    "text": "Connessione alla VPN\nPer prima cosa bisogna connettersi alla VPN e scegliere come paese di connessione gli Stati Uniti.\n\n\n\nEsempio connessione usando Proton VPN"
  },
  {
    "objectID": "posts/accedere-google-ai-riga-di-comando/index.html#generare-una-chiave-api",
    "href": "posts/accedere-google-ai-riga-di-comando/index.html#generare-una-chiave-api",
    "title": "Usare la nuova intelligenza artificiale di Google",
    "section": "Generare una chiave API",
    "text": "Generare una chiave API\nUna volta connessi dagli Stati Uniti √® necessario generare una chiave API, per autenticarsi. Si pu√≤ fare da questa pagina: https://makersuite.google.com/app/apikey.\nUna volta generata - √® una lunga stringa - √® da archiviare da qualche parte."
  },
  {
    "objectID": "posts/accedere-google-ai-riga-di-comando/index.html#accesso-alle-api-in-rest-via-curl",
    "href": "posts/accedere-google-ai-riga-di-comando/index.html#accesso-alle-api-in-rest-via-curl",
    "title": "Usare la nuova intelligenza artificiale di Google",
    "section": "Accesso alle API in REST, via cURL",
    "text": "Accesso alle API in REST, via cURL\n√à il modo pi√π immediato e diretto. Si apre la shell e si manda una richiesta come questa, in cui si definisce prima una variabile con la chiave API e poi si lancia la chiamata.\n# Una variabile dove inserire la chiave API\nAPI_KEY=\"AIxxSyCnBOUyPuDLtjWY11HOwxxxxxxx\"\n\n# Lanciare la chiamata\ncurl -s \"https://generativelanguage.googleapis.com/v1beta/models/gemini-pro:generateContent?key=$API_KEY\" \\\n-H 'Content-Type: application/json' \\\n-X POST -d '{\"contents\": [{\"parts\":[{\"text\": \"Creami tre nomi buffi per un gatto siamese con le orecchie molto grandi\"}]}]}'\n\n\n\n\n\n\nQui un esempio di output JSON\n\n\n\n\n\n{\n  \"candidates\": [\n    {\n      \"content\": {\n        \"parts\": [\n          {\n            \"text\": \"1. Dumbo\\n2. Flitzer\\n3. Elicottero\"\n          }\n        ],\n        \"role\": \"model\"\n      },\n      \"finishReason\": \"STOP\",\n      \"index\": 0,\n      \"safetyRatings\": [\n        {\n          \"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\",\n          \"probability\": \"NEGLIGIBLE\"\n        },\n        {\n          \"category\": \"HARM_CATEGORY_HATE_SPEECH\",\n          \"probability\": \"NEGLIGIBLE\"\n        },\n        {\n          \"category\": \"HARM_CATEGORY_HARASSMENT\",\n          \"probability\": \"NEGLIGIBLE\"\n        },\n        {\n          \"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\",\n          \"probability\": \"NEGLIGIBLE\"\n        }\n      ]\n    }\n  ],\n  \"promptFeedback\": {\n    \"safetyRatings\": [\n      {\n        \"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\",\n        \"probability\": \"NEGLIGIBLE\"\n      },\n      {\n        \"category\": \"HARM_CATEGORY_HATE_SPEECH\",\n        \"probability\": \"NEGLIGIBLE\"\n      },\n      {\n        \"category\": \"HARM_CATEGORY_HARASSMENT\",\n        \"probability\": \"NEGLIGIBLE\"\n      },\n      {\n        \"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\",\n        \"probability\": \"NEGLIGIBLE\"\n      }\n    ]\n  }\n}\n\n\n\nSe si espande l‚Äôesempio di JSON qui sopra, la parte con la risposta alla chiamata √® quella contenuta in .candidates[0].content.parts[0].text. Si pu√≤ modifcare il comando di sopra e usare jq per estrarla:\ncurl -s \"https://generativelanguage.googleapis.com/v1beta/models/gemini-pro:generateContent?key=$API_KEY\" \\\n-H 'Content-Type: application/json' \\\n-X POST -d '{\"contents\": [{\"parts\":[{\"text\": \"Creami tre nomi buffi per un gatto siamese con le orecchie molto grandi\"}]}]}' | \\\njq '.candidates[0].content.parts[0].text' -r\nIn output si avr√† qualcosa come:\n1. Dumbo\n2. Flitzer\n3. Elicottero\nNon vi resta che testare e divertirvi, con esempi migliori del mio. La cosa interessante √® che √® un‚ÄôAPI REST, quindi si pu√≤ usare da qualsiasi linguaggio di programmazione."
  },
  {
    "objectID": "posts/accedere-google-ai-riga-di-comando/index.html#utilizzare-leccezionale-llm-via-cli",
    "href": "posts/accedere-google-ai-riga-di-comando/index.html#utilizzare-leccezionale-llm-via-cli",
    "title": "Usare la nuova intelligenza artificiale di Google",
    "section": "Utilizzare l‚Äôeccezionale LLM via cli",
    "text": "Utilizzare l‚Äôeccezionale LLM via cli\nLLM √® un‚Äôutility a riga di comando e una libreria Python per interagire con Large Language Models (LLM), ovvero modelli di linguaggio avanzati. Permette di utilizzare sia API remote per accedere a modelli ospitati su server esterni, sia modelli installati e eseguiti localmente sul proprio computer.\nEd √® possibile quindi usarlo per connettersi con il Large Language Models di Google Gemini.\nüôè L‚Äôautore della cli LLM √® quel genio di Simon Willison.\n\nInstallazione\nPer installarlo √® sufficiente usare pip:\npip3 install llm\nPer usare Gemini, √® necessario instalare il plug-in dedicato, llm-gemini:\nllm install llm-gemini\nO anche\npip3 install llm-gemini\n\n\nUtilizzo\nLa prima cosa da fare √® impostare la propria chiave API (quella richiesta sopra). Si apre la shell:\nllm keys set gemini\nSi incolla la chiave API e si preme Invio.\nUna volta fatto, si pu√≤ testare il funzionamento con un esempio:\nllm -m gemini-pro \"Creami tre nomi buffi per un gatto siamese con le orecchie molto grandi\"\nIn output si avr√† qualcosa come:\n- Dumbo\n- Elio\n- Pipistrello\nLa cosa bella √® che llm, come tutte le buone cli, pu√≤ ricevere input dallo stdin e quindi pu√≤ utilizzare l‚Äôoutput di altri comandi.\nAd esempio l‚Äôoutput di echo:\necho 'Dieci nomi per un blog che parla della riga di comando' | llm -m gemini-pro\nE avr√≤ in output qualcosa come:\n1. Il Comando Centrale\n2. Padronanza del Terminale\n3. Il Mago della Riga di Comando\n4. Il Domatore di Terminale\n5. Il Maestro del Prompt\n6. La Guida alla Riga di Comando\n7. Il Tutorial del Terminale\n8. Il Manuale del Terminale\n9. Le Avventure di un SysAdmin\n10. La Linea di Comando per Tutti\nUn esempio pi√π carino, che mostra le info sul sistema operativo che sto usando adesso per testare Gemini. Il comando da cui parto √® uname -a, che mi restituisce:\nLinux MSI 5.15.133.1-microsoft-standard-WSL2 #1 SMP Thu Oct 5 21:02:42 UTC 2023 x86_64 GNU/Linux\nVoglio creare un‚Äôinformazione pi√π leggile, e la voglio in formato markdown, per scriverla qui (il markdown √® il formato in cui √® scritto questo sito), e lo chiedo a Gemini, via llm:\nllm -m gemini-pro \"Questo √® il mio sistema operativo: $(uname -a). Dimmi qualcosa di pi√π, e dimmelo in markdown.\"\nIn output avr√≤ qualcosa come:\n\nOutput del comandoOutput in HTML\n\n\n* **Kernel**: Linux 5.15.133.1-microsoft-standard-WSL2\n    * Questo √® il kernel Linux utilizzato per eseguire WSL2. √à una versione modificata del kernel Linux 5.15 che √® stato ottimizzato per l'esecuzione in un ambiente Windows.\n* **Nome Host**: MSI\n    * Questo √® il nome del computer WSL2.\n* **Versione**: #1 SMP Thu Oct 5 21:02:42 UTC 2023\n    * Questa √® la versione della build del kernel Linux utilizzata per eseguire WSL2.\n* **Architettura**: x86_64\n    * Questa √® l'architettura del processore del computer WSL2.\n* **Sistema Operativo**: GNU/Linux\n    * Questo √® il sistema operativo utilizzato per eseguire WSL2. GNU/Linux √® una distribuzione Linux basata sul kernel Linux.\n\n\n\nKernel: Linux 5.15.133.1-microsoft-standard-WSL2\n\nQuesto √® il kernel Linux utilizzato per eseguire WSL2. √à una versione modificata del kernel Linux 5.15 che √® stato ottimizzato per l‚Äôesecuzione in un ambiente Windows.\n\nNome Host: MSI\n\nQuesto √® il nome del computer WSL2.\n\nVersione: #1 SMP Thu Oct 5 21:02:42 UTC 2023\n\nQuesta √® la versione della build del kernel Linux utilizzata per eseguire WSL2.\n\nArchitettura: x86_64\n\nQuesta √® l‚Äôarchitettura del processore del computer WSL2.\n\nSistema Operativo: GNU/Linux\n\nQuesto √® il sistema operativo utilizzato per eseguire WSL2. GNU/Linux √® una distribuzione Linux basata sul kernel Linux.\n\n\n\n\n\n\n\nChattare in modo interattivo\nPer attivare una modalit√† interattiva, domande e risposte, come una chat, il comando √®:\nllm chat -m gemini-pro\nUna volta attivato, si potr√† attivare il dialogo in modalit√† chat (vedi Figura¬†1).\n\n\n\n\n\n\nFigura¬†1: llm in modalit√† chat"
  },
  {
    "objectID": "posts/accedere-google-ai-riga-di-comando/index.html#il-costo-di-gemini-pro",
    "href": "posts/accedere-google-ai-riga-di-comando/index.html#il-costo-di-gemini-pro",
    "title": "Usare la nuova intelligenza artificiale di Google",
    "section": "Il costo di Gemini Pro",
    "text": "Il costo di Gemini Pro\n\nRight now, developers have free access to Gemini Pro and Gemini Pro Vision through Google AI Studio, with up to 60 requests per minute, making it suitable for most app development needs. Vertex AI developers can try the same models, with the same rate limits, at no cost until general availability early next year, after which there will be a charge per 1,000 characters or per image across Google AI Studio and Vertex AI.\n\nFonte: https://blog.google/technology/ai/gemini-api-developers-cloud"
  },
  {
    "objectID": "posts/accedere-google-ai-riga-di-comando/index.html#conclusioni",
    "href": "posts/accedere-google-ai-riga-di-comando/index.html#conclusioni",
    "title": "Usare la nuova intelligenza artificiale di Google",
    "section": "Conclusioni",
    "text": "Conclusioni\nIl bello di questo tipo di accesso, √® quello di poter creare in modo diretto e semplici, un utilizzo programmatico di questi strumenti. E la cosa √® applicabile alla gran parte dei ‚ÄúLarge Language Model‚Äù (LLM), ovvero questi tipi di AI che si concentrano sulla comprensione e generazione del linguaggio naturale umano.\nQuesto post ha lo scopo soltanto di farvi due passi - non di pi√π - nel nuovo motore di AI di Google, Gemini. L‚Äôutility llm √® un gioiellino e consente di fare molto, ma molto di pi√π.\nüòâ Su entrambi lascio a chi legge tutti i necessari e divertenti approfondimenti del caso."
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Blog",
    "section": "",
    "text": "Come leggere un file Parquet\n\n\n\n\n\n\nduckdb\n\n\nparquet\n\n\nsql\n\n\n\nTanti modi a partire dai dati di OpenCoesione\n\n\n\n\n\n22 feb 2024\n\n\nAndrea Borruso, Andrea Borruso, Davide Taibi\n\n\n\n\n\n\n\n\n\n\n\n\nLavorare con grandi file CSV compressi\n\n\n\n\n\n\nduckdb\n\n\ncsv\n\n\ncli\n\n\n\nL‚Äôesempio dei nuovi dati OpenCUP, da sfogliare a riga di comando\n\n\n\n\n\n15 feb 2024\n\n\nAndrea Borruso, Andrea Borruso, Matteo Fortini\n\n\n\n\n\n\n\n\n\n\n\n\nUsare la nuova intelligenza artificiale di Google\n\n\n\n\n\n\nai\n\n\ngoogle\n\n\ncli\n\n\n\nFarlo a riga di comando, gi√† oggi che non √® disponibile in Italia\n\n\n\n\n\n17 dic 2023\n\n\nAndrea Borruso\n\n\n\n\n\n\n\n\n\n\n\n\nGestire file CSV grandi, brutti e cattivi\n\n\n\n\n\n\nduckdb\n\n\ncsv\n\n\nparquet\n\n\n\nTips & tricks, ispirati da DuckDB, file Parquet e OpenCoesione\n\n\n\n\n\n21 ago 2023\n\n\nAndrea Borruso\n\n\n\n\n\n\nNessun risultato\n\n Torna in cima"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Andrea Borruso",
    "section": "",
    "text": "Ciao\nGrazie per essere qui. Questo √® uno spazio dove inserir√≤ alcuni appunti sulle cose che imparo, sui progetti che mi piacciono e/o che sto sviluppando, sugli strumenti con cui lavoro e gioco e sulle persone che incontro.\nSono socio dell‚Äôassociazione onData.\n\n\nQuando hai dato lo stesso consiglio 3 volte, scrivi un post (David Robinson).\n\n\n\n\n Torna in cima"
  },
  {
    "objectID": "posts/csv-ventitre-gigabyte-senza-affanno/index.html",
    "href": "posts/csv-ventitre-gigabyte-senza-affanno/index.html",
    "title": "Lavorare con grandi file CSV compressi",
    "section": "",
    "text": "Nota successiva alla pubblicazione\n\n\n\nQuesto post ci ha consentito di fare una prima lettura di questa importante banca dati e sono emerse alcune criticit√†. Una √® quelle citata nel post, sulle righe con numero di colonne errato. Quella che ci sembra pi√π importante √® relativa alla natura dei progetti, che era la novit√† pi√π importante: sul CSV ci sono soltanto 3 tipologie, quindi nulla √® cambiato. Le altre tipologie al momento sono visibili soltanto sul front-end del sito. √à stato annunciato, anche dopo una nostra segnalazione, che i dati saranno aggiornati in tal senso entro la fine di febbraio 2024.\nDa pochissimo, dal 14 febbraio 2024, √® andato online il nuovo portale OpenCUP.\nOpenCUP mette a disposizione di tutti - cittadini, istituzioni ed altri enti - i dati, in formato aperto, sulle decisioni di investimento pubblico finanziate con fondi pubblici nazionali, comunitarie o regionali o con risorse private registrate con il Codice Unico di Progetto.\nLa novit√† pi√π importante di questo aggiornamento, √® legata alla natura degli interventi in elenco: a ‚ÄúLavori Pubblici‚Äù e ‚ÄúIncentivi alle imprese e Contributi per calamit√† naturali‚Äù, si aggiungono ‚Äúacquisti di beni, servizi, corsi di formazione, strumenti finanziari, progetti di ricerca e contributi a entit√† diverse dalle unit√† produttive per una Pubblica Amministrazione pi√π trasparente e vicina al cittadino‚Äù (qui la notizia di lancio).\nIn numeri si passa da circa 6,5 a 9,5 milioni di progetti."
  },
  {
    "objectID": "posts/csv-ventitre-gigabyte-senza-affanno/index.html#gli-open-data-di-opencup",
    "href": "posts/csv-ventitre-gigabyte-senza-affanno/index.html#gli-open-data-di-opencup",
    "title": "Lavorare con grandi file CSV compressi",
    "section": "Gli Open Data di OpenCUP",
    "text": "Gli Open Data di OpenCUP\nSul sito di OpenCUP, √® disponibile da tempo la sezione OpenData. Con questo aggiornamento, sono stati pubblicati i nuovi dati, in formato CSV e XML. Bisogna scorrere la pagina e guardare la sezione ‚ÄúProgetti OpenData‚Äù.\nIn questo post, mostreremo rapidamente come esplorare il file ‚ÄúComplessivo‚Äù di tutti i progetti.\n√à un file ‚Äúgrande‚Äù. Quello in formato CSV √® compresso come zip: pesa circa 2.9 GB, e decompresso 23 GB. √à probabilmente il file CSV pi√π grande (o uno dei pi√π grandi), pubblicato in una sezione open data di un sito di una Pubblica Amministrazione italiana.\nNon √® gestibile con un comune foglio di calcolo, n√© con tanti altri strumenti. A riga di comando invece si pu√≤ osservare, filtrare, e analizzare, con grande leggerezza e rapidit√†.\n\n\n\n\n\n\nAvviso\n\n\n\nCi sono alcune righe con un numero di separatori di campo errato: devono essere 90. Per estrarre soltanto quelle con 90 separatori si pu√≤ usare awk:\nunzip -p \"OpenData Complessivo.zip\"  | awk '{if(gsub(/\\|/,\"&\") == 90) print}' &gt;open_cup_corrette.csv\nVedi anche la sezione Importazione dell‚Äôintero file."
  },
  {
    "objectID": "posts/csv-ventitre-gigabyte-senza-affanno/index.html#la-prima-lettura",
    "href": "posts/csv-ventitre-gigabyte-senza-affanno/index.html#la-prima-lettura",
    "title": "Lavorare con grandi file CSV compressi",
    "section": "La prima lettura",
    "text": "La prima lettura\nUn file cos√¨ grande richiede un po‚Äô di tempo per il download e poi per la decompressione. Per fortuna per√≤ √® possibile fare una prima lettura immediata, dopo il download, senza decomprimere per intero il file.\nSi pu√≤ usare l‚Äôutilty unzip e lanciare il comando unzip -p nomefile.zip. Ma per una prima esplorazione, l‚Äôideale √® leggere soltanto le prime righe. E per fortuna esiste il comando head, che fa proprio questo e di default legge le prime 10 righe.:\nunzip -p \"OpenData Complessivo.zip\"  | head\nE subito si vedr√† qualcosa come in Figura¬†2, che ci restituisce gi√† elementi interessanti:\n\ni nomi dei campi;\nil separatore √® il |\n\n\n\n\n\n\n\nFigura¬†2: Le prime righe del file con i progetti"
  },
  {
    "objectID": "posts/csv-ventitre-gigabyte-senza-affanno/index.html#estrazione-di-un-campione",
    "href": "posts/csv-ventitre-gigabyte-senza-affanno/index.html#estrazione-di-un-campione",
    "title": "Lavorare con grandi file CSV compressi",
    "section": "Estrazione di un campione",
    "text": "Estrazione di un campione\nViste le prime 10 righe e mappate le caratteristiche del CSV, il passo consigliato successivo √® quello dell‚Äôestrazione di un campione di righe pi√π ampio, per fare un‚Äôanalisi pi√π approfondita. Con una piccola modifica al comando soprastante, si possono ad esempio estrarre le prime 10.000 righe e salvarle in un nuovo file CSV (in 0.055 secondi):\nunzip -p \"OpenData Complessivo.zip\"  | head -n 10000 &gt; campione.csv\nE questo nuovo file sar√† esplorabile con foglio elettronico o con altri strumenti. A me ad esempio piace usare VisiData per queste cose, con cui di solito subito osservo un po‚Äô di dati sui campi (i valori nulli, i valori distinti, ecc.).\n\n\n\n\n\n\nFigura¬†3: Esplorazione dei dati campione con VisiData\n\n\n\n\n\n\n\n\n\nNota\n\n\n\nIl professore Taibi ci ha fatto notare che questo non √® un vero campione, ma soltanto le prime n righe del file. Un vero campione dovrebbe essere estratto in modo casuale, ma sarebbe necessario decomprimere tutto il file per farlo. E quindi qui, per un‚Äôesplorazione rapida, ci accontentiamo di questo."
  },
  {
    "objectID": "posts/csv-ventitre-gigabyte-senza-affanno/index.html#filtrare-lintero-file",
    "href": "posts/csv-ventitre-gigabyte-senza-affanno/index.html#filtrare-lintero-file",
    "title": "Lavorare con grandi file CSV compressi",
    "section": "Filtrare l‚Äôintero file",
    "text": "Filtrare l‚Äôintero file\nL‚Äôesplorazione del campione consente di farsi un‚Äôidea di quali sono quei campi/colonne che fanno un po‚Äô da elementi ‚Äúcategorici‚Äù, da usare come filtro per estrarre solo le righe che ci interessano. Tra questi ad esempio il campo SOGGETTO_TITOLARE, con il nome della Pubblica Amministrazione, titolare del progetto, che posso usare per estrarre ad esempio tutti i progetti in cui la PA titolare √® ‚ÄúREGIONE AUTONOMA DELLA SICILIA‚Äù.\nL‚Äôutility grep - una delle pi√π importanti di tutti i tempi - √® perfetta per questo scopo.\nunzip -p \"OpenData Complessivo.zip\"  | grep -P '\\|REGIONE AUTONOMA DELLA SICILIA\\|' &gt;regione_siciliana.csv\nSi applica un filtro al file CSV zippato di input, e per ogni riga in cui √® presente la stringa REGIONE AUTONOMA DELLA SICILIA tra due caratteri |, viene salvata nel file regione_siciliana.csv. Nel filtro √® stato inserito il carattere \\ prima del |, perch√© | √® un carattere speciale. E in un minuto e mezzo circa, sul mio buon (ma normale) PC ottengo 392.395 progetti associati alla Regione Siciliana, dopo aver letto 9,5 milioni di righe.\nMa cos√¨ c‚Äô√® un problema: si perde l‚Äôintestazione del file, si perdono i nomi delle colonne. C‚Äô√® allora da modificare il filtro ed estrarre anche la prima riga. E tutto questo √® facile, grazie all‚Äôesplorazione fatta, in cui ho visto che la prima riga contiene ad esempio la stringa ‚ÄúANNO_DECISIONE‚Äù. Il nuovo comando sar√†:\nunzip -p \"OpenData Complessivo.zip\"  | \\\ngrep -P '(ANNO_DECISIONE|\\|REGIONE AUTONOMA DELLA SICILIA\\|)' &gt;regione_siciliana.csv\nCos√¨ vengono cercate tutte le righe che contengono o ANNO_DECISIONE o |REGIONE AUTONOMA DELLA SICILIA| e quindi viene restituito un CSV con i nomi delle colonne e le righe filtrate."
  },
  {
    "objectID": "posts/csv-ventitre-gigabyte-senza-affanno/index.html#un-file-parquet-a-partire-dai-dati-filtrati",
    "href": "posts/csv-ventitre-gigabyte-senza-affanno/index.html#un-file-parquet-a-partire-dai-dati-filtrati",
    "title": "Lavorare con grandi file CSV compressi",
    "section": "Un file parquet a partire dai dati filtrati",
    "text": "Un file parquet a partire dai dati filtrati\nQuesto ultimo passo, perch√© √® utile avere a disposizione questi dati anche in un formato pi√π efficiente, come il parquet, che √® un formato di file binario, compresso e colonnare, che permette di ridurre notevolmente lo spazio occupato e di velocizzare le operazioni di lettura e analisi. Per chi non ne ha mai sentito parlare, ne ho scritto lungamente qui.\nL‚Äôutility stavolta √® DuckDB. E il comando √®:\nunzip -p \"OpenData Complessivo.zip\"  | \\\ngrep -P '(ANNO_DECISIONE|\\|REGIONE AUTONOMA DELLA SICILIA\\|)' | \\\nduckdb -c \"COPY(\n  SELECT * FROM read_csv_auto('/dev/stdin', delim = '|')\n) TO 'regione_siciliana.parquet' (\n  FORMAT 'parquet',\n  COMPRESSION 'ZSTD',\n  ROW_GROUP_SIZE 100000\n)\"\nAlcune note su questo comando (messo su pi√π righe, per migliorare la leggibilit√†):\n\nsi parte dal comando precedente, con il filtro per estrarre il campione;\nsi passa l‚Äôoutput a duckdb, che legge il CSV da stdin e nel SELECT al posto del nome del file √® necessario inserire '/dev/stdin', che √® il file virtuale che rappresenta lo standard input;\nsi usa il comando COPY per copiare questa selezione in un file parquet chiamato regione_siciliana.parquet, con compressione ZSTD e con un ROW_GROUP_SIZE di 100000.\n\nTutto circa sempre in un minuto e mezzo, con un file di output che pesa circa 33 MB, contro i 530 MB del file CSV."
  },
  {
    "objectID": "posts/csv-ventitre-gigabyte-senza-affanno/index.html#importazione-dellintero-file",
    "href": "posts/csv-ventitre-gigabyte-senza-affanno/index.html#importazione-dellintero-file",
    "title": "Lavorare con grandi file CSV compressi",
    "section": "Importazione dell‚Äôintero file",
    "text": "Importazione dell‚Äôintero file\nIl file CSV completo ha, alla data di oggi (15 Febbraio 2024) un problema a 2 righe (su 17.065.848), che hanno un numero errato di colonne (91 anzich√© 90). Per poterlo importare occorre filtrare solo le linee che hanno un numero di colonne corretto.\n√à possibile importare il file in un DB in formato DuckDB con i seguenti comandi (occorre avere il file CSV su disco per limitare l‚Äôoccupazione di RAM):\nunzip \"OpenData Complessivo.zip\"\nawk -F\\| '{if (NF-1 == 90) { print } }' &lt; TOTALE.csv &gt; TOTALEfix.csv\nduckdb OpenCUP.db -c \"CREATE TABLE OpenCUP AS SELECT * FROM read_csv_auto('TOTALEfix.csv',delim='|',header = true,dateformat='%d-%b-%y',parallel=FALSE,types={'DATA_ULTIMA_MODIFICA_SSC':'DATE','DATA_ULTIMA_MODIFICA_UTENTE':'DATE','DATA_CHIUSURA_REVOCA':'DATE','DATA_GENERAZIONE_CUP':'DATE'});\"\nNotare che √® stato esplicitato il formato delle date, e quali sono i campi di tipo DATE, in modo che DuckDB converta correttamente i campi relativi, che sono del tipo 01-JAN-15. Questo permette sia una migliore gestione dei dati, che una rappresentazione pi√π efficiente in termini di spazio su disco.\nIl file DuckDB risultante, a fronte di un CSV di 23.946.237.777 byte, √® 4.431.294.464 byte, con una riduzione a circa 1/6, merito del formato binario contro quello ASCII.\n\nPrestazioni del formato DuckDB\nUna query di esempio\ntime duckdb OpenCUP.db -c \"SELECT ANNO_DECISIONE, COUNT(*) FROM OpenCUP GROUP BY ANNO_DECISIONE\" &gt; /dev/null\n\nreal    0m0,039s\nuser    0m0,156s\nsys 0m0,063s\n\n\nConversione nel formato Parquet\nCon il comando\nduckdb OpenCUP.db  -c \"COPY (SELECT * FROM OpenCUP) TO 'OpenCUP.parquet' (FORMAT 'PARQUET', CODEC 'ZSTD');\nsi ottiene un file Parquet di 2.296.193.848 byte, con una riduzione di circa 1/2 rispetto al formato nativo DuckDB.\nSe con questo comando si hanno problemi di memoria si pu√≤ provare a impostare SET preserve_insertion_order=false:\nduckdb OpenCUP.db  -c \"SET preserve_insertion_order=false;COPY (SELECT * FROM OpenCUP) TO 'OpenCUP.parquet' (FORMAT 'PARQUET', CODEC 'ZSTD');\"\nLe prestazioni del formato Parquet sono notevoli\ntime duckdb -s 'SELECT ANNO_DECISIONE, COUNT (*) FROM read_parquet(\"OpenCUP.parquet\") GROUP BY ANNO_DECISIONE;' &gt; /dev/null\n\nreal    0m0,074s\nuser    0m0,372s\nsys 0m0,043s\n\n\nNote sui tempi di esecuzione\nI tempi di esecuzione sono stati misurati su un computer abbastanza ‚Äúnormale‚Äù, con 16 GB di RAM e questo processore:\nVendor ID:               GenuineIntel\n  Model name:            12th Gen Intel(R) Core(TM) i7-1280P\n    CPU family:          6\n    Model:               154\n    Thread(s) per core:  2\n    Core(s) per socket:  10\nQuesta la tabella riassuntiva dei tempi di esecuzione:\n\n\n\n\n\n\n\nOperazione\nTempo (minuti:secondi)\n\n\n\n\nDownload (con wget)\n12:13.81\n\n\nDecompressione (con unzip)\n1:33.05\n\n\nEstrazione delle sole righe corrette (con ugrep)1\n2:25.63\n\n\nImportazione in DuckDB\n4:15.97\n\n\nConversione in Parquet\n0:28.811\n\n\n\n\n\nVersione pi√π sintetica e rapida\nIn DuckDB, nell‚Äôimport dei file CSV, c‚Äô√® l‚Äôopzione ignore_errors=true, che permette di ignorare le righe che contengono errori (come quelle con un numero di colonne errato). E quindi si pu√≤ saltare la creazione di un secondo grande CSV per applicare il filtro.\nInoltre, se non abbiamo bisogno del database DuckDB, possiamo convertire direttamente il file CSV in parquet e saltare quindi un altro passaggio.\nIl comando sintetico √®:\nduckdb -c \"SET preserve_insertion_order=false;COPY(SELECT * FROM read_csv_auto('TOTALE.csv',delim='|',header = true,ignore_errors=true,dateformat='%d-%b-%y',parallel=FALSE,types={'DATA_ULTIMA_MODIFICA_SSC':'DATE','DATA_ULTIMA_MODIFICA_UTENTE':'DATE','DATA_CHIUSURA_REVOCA':'DATE','DATA_GENERAZIONE_CUP':'DATE'})) TO 'opencup.parquet' WITH (FORMAT PARQUET, COMPRESSION ZSTD,ROW_GROUP_SIZE 100000)\"\nCos√¨ facendo, c‚Äô√® un risparmio di tempo di circa il 30%."
  },
  {
    "objectID": "posts/csv-ventitre-gigabyte-senza-affanno/index.html#note-finali",
    "href": "posts/csv-ventitre-gigabyte-senza-affanno/index.html#note-finali",
    "title": "Lavorare con grandi file CSV compressi",
    "section": "Note finali",
    "text": "Note finali\nQuesta descritta √® soltanto una modalit√† per fare soprattutto una prima esplorazione e analisi di questo nuovo e importante aggiornamento dei dati di OpenCUP. √à utilissima per capire per cosa √® possibile usare questi dati, quali sono le informazioni che ci interessano di pi√π, che storia poter raccontare, che mappa creare, che dashboard realizzare, come usare l‚Äôintelligenza artificiale per arricchirli, come metterli in relazione con altri dati, ecc..\nMa sono dati grandi, e in produzione bisogner√† andare un po‚Äô oltre l‚Äôutilizzo delle eccezionali utility a riga di comando. In ogni caso DuckDB, se usato bene, fa gi√† tanta tanta roba (partizionando il dataset originario in pi√π file parquet, si ha disposizione tutto il dataset con una grandissima facilit√† di accesso e lettura).\n\n\n\nFigura¬†2: Le prime righe del file con i progetti\nFigura¬†3: Esplorazione dei dati campione con VisiData"
  },
  {
    "objectID": "posts/csv-ventitre-gigabyte-senza-affanno/index.html#footnotes",
    "href": "posts/csv-ventitre-gigabyte-senza-affanno/index.html#footnotes",
    "title": "Lavorare con grandi file CSV compressi",
    "section": "Note",
    "text": "Note\n\n\nugrep '^(?:[^|]*\\|){90}[^|]*$' TOTALE.csv &gt;TOTALEfix.csv‚Ü©Ô∏é"
  },
  {
    "objectID": "til.html",
    "href": "til.html",
    "title": "TIL (Today I Learned)",
    "section": "",
    "text": "Che vuol dire TIL?\n\n\n\nMi piace quando le persone usano il loro sito web, per prendere appunti su alcune delle cose che imparano/scoprono. Come fa il mitico Simon Willison. E Simon lo fa in modalit√† TIL, ovvero Today I Learned.\nI miei post qui saranno spesso ‚Äúpiccoli‚Äù, dei TIL in forma di appunti, con qualche possibile lacuna e/o bruttura.\n\n\n\n\n\n\n\n\n   \n     \n     \n       Ordinare per\n       Predefinito\n         \n          Data - Meno recente\n        \n         \n          Data - Pi√π recente\n        \n         \n          Titolo\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\nData\n\n\nTitolo\n\n\n\n\n\n\n29 giu 2023\n\n\nInstallazione QSV\n\n\n\n\n23 apr 2023\n\n\nDuckDB: l‚Äôestensione spaziale\n\n\n\n\n4 mar 2023\n\n\nDuckDB: creare un file parquet a partire da file di testo di grandi dimensioni\n\n\n\n\n26 feb 2023\n\n\nQuarto: applicare stile CSS\n\n\n\n\n28 gen 2023\n\n\nEstrarre la lista dei file creati pi√π di 30 giorni fa\n\n\n\n\n7 gen 2023\n\n\nPagina con codice R, Python e utility Bash\n\n\n\n\n3 dic 2022\n\n\nNushell: installarlo con il supporto ai dataframe\n\n\n\n\n28 nov 2022\n\n\nDuckDB: creare un file parquet a partire da un CSV\n\n\n\n\n26 nov 2022\n\n\nQuarto: renderizzare una tabella a partire da un CSV\n\n\n\n\n22 nov 2022\n\n\nFare convivere una cella Observable e un grafico Altair in Quarto\n\n\n\n\n21 nov 2022\n\n\nQuarto: leggere un CSV via Obeservable e visualizzare i dati\n\n\n\n\n20 nov 2022\n\n\nCome filtrare un file di testo a partire da una lista di stringhe\n\n\n\n\n19 nov 2022\n\n\nIl mio primo blog post\n\n\n\n\n\nNessun risultato\n\n Torna in cima"
  },
  {
    "objectID": "til/duckdb-creare-parquet-csv/index.html",
    "href": "til/duckdb-creare-parquet-csv/index.html",
    "title": "DuckDB: creare un file parquet a partire da un CSV",
    "section": "",
    "text": "DuckDB ha una cli molto comoda e potente.\nSe si vuole ad esempio creare il file parquet del file CSV degli Indicatori di rischio idrogeologico pubblicati da ISPRA, questo √® il comando da lanciare:\nduckdb -c \"CREATE TABLE comuni_pir AS SELECT * FROM comuni_pir.csv;EXPORT DATABASE '.' (FORMAT PARQUET);\"\n\nviene creata una tabella comuni_pir in un db temporaneo, a partire dal file CSV;\nviene esportato il db in formato parquet (che conterr√† una sola tabella), nella directory corrente;\n-c per eseguire i due comandi, separati da ; e poi uscire.\n\n\n\n\n\n\n\nAttenzione all‚Äôinferencing dei tipi di campo\n\n\n\nI campi di un file CSV non sono associati a una definizione di tipo di campo. DuckDB in import far√† il cosiddetto inferencing, ovvero prover√† a dedurlo.Non √® detto che lo faccia correttamente ed √® bene sempre fare un check (celle con valori come 08, 09, ecc. sono ad esempio spesso mappate come numeri e non come stringhe).\n\n\n\n\n\n Torna in cima"
  },
  {
    "objectID": "til/installare-duckdb-spatial/index.html",
    "href": "til/installare-duckdb-spatial/index.html",
    "title": "DuckDB: l‚Äôestensione spaziale",
    "section": "",
    "text": "√à stata rilasciata questa estensione spaziale per DuckDB.\nUno dei modi per istallarla √® scaricare i binari precompilati, accessibili dai workflow di compilazione.\n\n\n\nI workflow, per i vari sistemi operativi\n\n\nPer installarla:\n\ndecomprimere il file scaricato;\nlanciare duckdb con l‚Äôopzione unsigned, ovvero duckdb -unsigned;\ninstallare l‚Äôestensione usando il percorso assoluto del file (sotto un esempio)\n\ninstall '/home/user/spatial.duckdb_extension';\n\ncaricare l‚Äôestensione, con LOAD spatial;.\n\nE una volta caricata, potrai vedere tutti i nuovi formati file supportati da duckdb con\nselect * from ST_LIST_DRIVERS() order by 1;\n\n\n\nLa lista dei formati disponibili\n\n\n\n\n\n Torna in cima"
  },
  {
    "objectID": "til/nushell-installare-supporto-dataframe/index.html",
    "href": "til/nushell-installare-supporto-dataframe/index.html",
    "title": "Nushell: installarlo con il supporto ai dataframe",
    "section": "",
    "text": "Dalla release 0.72 di nushell il supporto ai dataframe non √® abilitato di default.\nQuesta una modalit√† di compilarlo, con il supporto abilitato.\n# clona il repository\ngit clone https://github.com/nushell/nushell.git\n\ncd nushell\n\ncargo install --path=. --all-features\nVerr√† installato in /home/username/.cargo/bin/nu.\n\n\n\n Torna in cima"
  },
  {
    "objectID": "til/quarto-mescolare-observable-altair/index.html",
    "href": "til/quarto-mescolare-observable-altair/index.html",
    "title": "Fare convivere una cella Observable e un grafico Altair in Quarto",
    "section": "",
    "text": "Non √® possibile in Quarto fare convivere una cella di codice di tipo Observable, con una cella Python con un grafico Altair.\n\n\nVedi issue 3424\nC‚Äô√® per√≤ un workaround:\n\nda Altair generare la descrizione del grafico in formato JSON (√® in formato vega-lite), con chart.to_json();\nfare leggere a una cella Observable il JSON, e visualizzare il grafico.\n\nQui ad esempio creo una cella Observable usata soltanto come esempio.\n\n```{ojs}\n//| echo: fenced\ndata = FileAttachment(\"ojs.csv\").csv({ typed: true })\nInputs.table(data)\n```\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPoi genero la descrizione di un grafico vega-lite, con Altair, salvando il file chart.json.\n\n```{python}\nimport pandas as pd\nimport altair as alt\nimport warnings\n\nwarnings.simplefilter(action='ignore', category=FutureWarning)\n\ndf = pd.read_csv(\"altair.csv\",keep_default_na=False)\n\ndf['year'] = pd.to_datetime(df['year'], format='%Y')\n\nchart=alt.Chart(df).mark_area().encode(\n    alt.X('year:T', timeUnit = 'year',title='year',axis=alt.Axis(tickCount='year')),\n    alt.Y('v:Q',axis=alt.Axis(format='%'),title='percentage'),\n    color='i:N'\n)\nchart.save('chart.json')\n```\n\nE infine faccio leggere a Observable la descrizione del grafico, che √® stata generata da Altair e lo faccio visualizzare.\n\n```{ojs}\n//| echo: fenced\nfile = FileAttachment(\"chart.json\").json()\nembed = require(\"vega-embed@6\")\nembed(file)\n```\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Torna in cima"
  },
  {
    "objectID": "til/renderizzare-csv-quarto-observable/index.html",
    "href": "til/renderizzare-csv-quarto-observable/index.html",
    "title": "Quarto: leggere un CSV via Obeservable e visualizzare i dati",
    "section": "",
    "text": "Si pu√≤ usare semplicemente il metodo FileAttachment di Observable, per data.csv\n```{ojs}\ndata = FileAttachment(\"data.csv\").csv({ typed: true })\nInputs.table(data)\n```\nper ottenere\n\ndata = FileAttachment(\"data.csv\").csv({ typed: true })\nInputs.table(data)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSe vuoi applicare la formattazione della localizzazione italiana, con la , come separatore decimale e il . come separatore delle migliaia, basta modifcarlo in\n```{ojs}\ndata = FileAttachment(\"data.csv\").csv({ typed: true })\nInputs.table(data,{ locale: \"it-IT\" })\n```\nper ottenere\n\nInputs.table(data,{ locale: \"it-IT\" })\n\n\n\n\n\n\nO in alternativa con arquero (ma vale la pena usarlo anche per trasformare i dati)\n\n```{ojs}\n//| echo: fenced\nimport { aq, op } from '@uwdata/arquero'\ndati_aquero = aq.loadCSV(\"data.csv\")\n\ndati_aquero.view()\n```\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNota\n\n\n\nIl Quarto del titolo √® lui https://quarto.org/\n\n\n\n\n\n Torna in cima"
  },
  {
    "objectID": "posts/leggere-interrogare-file-parquet/index.html",
    "href": "posts/leggere-interrogare-file-parquet/index.html",
    "title": "Come leggere un file Parquet",
    "section": "",
    "text": "In questa piccola guida, ti porteremo per mano alla scoperta del formato Parquet e ti spiegheremo cos‚Äô√®. Potresti preferirlo al buon vecchio formato CSV, visto che pu√≤ rendere la tua vita con i dati un po‚Äô pi√π facile e molto pi√π veloce.\n√à un formato di archiviazione ottimizzato per lavorare con dati complessi e voluminosi. A differenza del CSV - che memorizza i dati per riga - Parquet organizza i dati per colonne.\nImmagina di avere una tabella con le 4 colonne ID, Nome, Et√†, E-mail. Come differisce l‚Äôaccesso ai dati tra un file CSV e un file Parquet?\nIn un file CSV, se vuoi accedere soltanto alla colonna Et√† per tutte le righe, il sistema deve leggere l‚Äôintero file, riga per riga, per estrarre l‚Äôinformazione relativa. Questo processo pu√≤ essere piuttosto inefficiente, soprattutto con grandi volumi di dati, perch√© comporta la lettura di molti dati inutili (quelli delle colonne ID, Nome e E-mail).\n\n\ntabella.csv\n\nID,Nome,Et√†,E-mail\n1,Mario Rossi,30,mario.rossi@email.com\n2,Laura Bianchi,25,laura.bianchi@email.com\n...\n\nViceversa ad un file Parquet, essendo organizzato per colonne, si pu√≤ accedere direttamente, e anche esclusivamente, alla colonna Et√†, senza dover leggere anche le altre colonne. Qui sotto un esempio, in si vede per accedere alla colonna Et√†, si possono saltare tutte le altre e leggere soltanto quella.\n\n\ntabella.parquet\n\nID: 1,2,...\nNome: Mario Rossi,Laura Bianchi,...\nEt√†: 30,25,...\nE-mail: mario.rossi@email.com,laura.bianchi@email.com,...\n\nQuesto rende l‚Äôaccesso ai dati molto pi√π veloce ed efficiente."
  },
  {
    "objectID": "posts/leggere-interrogare-file-parquet/index.html#introduzione",
    "href": "posts/leggere-interrogare-file-parquet/index.html#introduzione",
    "title": "Come leggere un file Parquet",
    "section": "",
    "text": "In questa piccola guida, ti porteremo per mano alla scoperta del formato Parquet e ti spiegheremo cos‚Äô√®. Potresti preferirlo al buon vecchio formato CSV, visto che pu√≤ rendere la tua vita con i dati un po‚Äô pi√π facile e molto pi√π veloce.\n√à un formato di archiviazione ottimizzato per lavorare con dati complessi e voluminosi. A differenza del CSV - che memorizza i dati per riga - Parquet organizza i dati per colonne.\nImmagina di avere una tabella con le 4 colonne ID, Nome, Et√†, E-mail. Come differisce l‚Äôaccesso ai dati tra un file CSV e un file Parquet?\nIn un file CSV, se vuoi accedere soltanto alla colonna Et√† per tutte le righe, il sistema deve leggere l‚Äôintero file, riga per riga, per estrarre l‚Äôinformazione relativa. Questo processo pu√≤ essere piuttosto inefficiente, soprattutto con grandi volumi di dati, perch√© comporta la lettura di molti dati inutili (quelli delle colonne ID, Nome e E-mail).\n\n\ntabella.csv\n\nID,Nome,Et√†,E-mail\n1,Mario Rossi,30,mario.rossi@email.com\n2,Laura Bianchi,25,laura.bianchi@email.com\n...\n\nViceversa ad un file Parquet, essendo organizzato per colonne, si pu√≤ accedere direttamente, e anche esclusivamente, alla colonna Et√†, senza dover leggere anche le altre colonne. Qui sotto un esempio, in si vede per accedere alla colonna Et√†, si possono saltare tutte le altre e leggere soltanto quella.\n\n\ntabella.parquet\n\nID: 1,2,...\nNome: Mario Rossi,Laura Bianchi,...\nEt√†: 30,25,...\nE-mail: mario.rossi@email.com,laura.bianchi@email.com,...\n\nQuesto rende l‚Äôaccesso ai dati molto pi√π veloce ed efficiente."
  },
  {
    "objectID": "posts/leggere-interrogare-file-parquet/index.html#lesempio-di-opencoesione",
    "href": "posts/leggere-interrogare-file-parquet/index.html#lesempio-di-opencoesione",
    "title": "Come leggere un file Parquet",
    "section": "L‚Äôesempio di OpenCoesione",
    "text": "L‚Äôesempio di OpenCoesione\nOpenCoesione √® un progetto nazionale il cui obiettivo √® promuovere trasparenza, collaborazione e partecipazione riguardo alle politiche di coesione nel paese.\n√à da sempre uno dei progetti di riferimento per l‚Äôapertura dei dati in Italia, che ha fatto sempre scuola. E lo ha fatto ancora una volta: dal 21 febbraio 2024 ha iniziato a pubblicare la propria banca dati anche in formato Parquet.\nEd √® probabilmente il primo progetto italiano di una Pubblica Amministrazione a farlo, e sicuramente il primo con una banca dati di questa ricchezza e dimensione.\n‚û°Ô∏è Stiamo parlando del catalogo dei Progetti con tracciato esteso, disponibile qui, anche in formato Parquet: https://opencoesione.gov.it/it/opendata/#!progetti_section\nAbbiamo sottolineato come il formato Parquet sia molto pi√π efficiente per l‚Äôaccesso e l‚Äôanalisi dei dati, rispetto al CSV. Questa caratteristica √® molto evidente, soprattutto per tabelle molto grandi, come questa dei progetti di OpenCoesione, composta da circa 2.000.000 di righe x 200 colonne.\n√à possibile ad esempio interrogarla, per avere restituito il totale di finanziamento pubblico per ogni ciclo di finanziamento (vedi Tabella¬†1), e avere la risposta in 0,07 secondi.\n\n\n\n\nTabella¬†1: totale di finanziamento pubblico per ciclo di finanziamento\n\n\n\n\n\n\n\n\nCiclo\nTotale finanziamento pubblico (‚Ç¨)\n\n\n\n\nCiclo di programmazione 2000-2006\n17.667.666.307,18\n\n\nCiclo di programmazione 2007-2013\n101.629.942.347,96\n\n\nCiclo di programmazione 2014-2020\n154.061.098.680,89\n\n\nCiclo di programmazione 2021-2027\n7.465.189.173,71\n\n\n\n\n\n\n\n\nE questa rapidit√† si ottiene sul proprio computer di lavoro, senza che sia necessario mettere in campo risorse di calcolo particolarmente potenti e dispendiose sul cloud. O senza che sia necessario importare il file in un database relazionale, con tutte le operazioni di trasformazione e pulizia dei dati che questo comporta evitando anche l‚Äôinstallazione e la configurazione di un db relazionale."
  },
  {
    "objectID": "posts/leggere-interrogare-file-parquet/index.html#come-leggere-un-file-parquet",
    "href": "posts/leggere-interrogare-file-parquet/index.html#come-leggere-un-file-parquet",
    "title": "Come leggere un file Parquet",
    "section": "Come leggere un file Parquet",
    "text": "Come leggere un file Parquet\nSe non hai mai sentito parlare di questo formato, probabilmente penserai che per te sia impossibile usarlo per leggere, filtrare, analizzare, ecc. i dati di OpenCoesione. Penserai che √® un formato solo per ‚Äútecnici‚Äù. Niente di pi√π sbagliato! Leggere un file Parquet √® facile e veloce, e paradossalmente √® pi√π necessario l‚Äôaiuto di un tecnico per leggere un file CSV di 4,5 Gigabyte (come quello dei progetti di OpenCoesione). Per la gran parte degli utenti √® infatti impossibile leggere un file di queste dimensioni, anche con un buon Personal Computer. E non pensare di utilizzare programmi come Excel, hanno un limite di circa 1.000.000 di righe per foglio di lavoro (tante ma non sufficienti nel nostro caso in cui ne abbiamo circa 2.000.000).\n\n\n\n\n\n\nNota bene\n\n\n\nUn file di queste dimensioni, aldil√† del formato, deve essere gestito con attenzione e con un minimo di competenza.\n\n\n\nAl doppio click\nPer visualizzare un file Parquet con un semplice doppio click, puoi usare Tad, un visualizzatore di file Parquet (e anche CSV, SQLite e DuckDB) open source, gratuito e disponibile per Windows, Mac e Linux.\nUna volta installato, baster√† fare doppio click sul file per aprirlo e visualizzarne il contenuto (vedi Figura¬†1). Questo di OpenCoesione √® un file grande e sar√† necessario qualche secondo.\n\n\n\n\n\n\nFigura¬†1: esempio di visualizzazione e filtro dati con TAD\n\n\n\nTra le funzionalit√† di Tad c‚Äô√® anche la possibilit√† di filtrare i dati. Nell‚Äôimmagine di sopra:\n\nil filtro applicato;\nil modulo per costruire il filtro;\nil mini report sul numero di righe filtrate.\n\n\n\nCon un client visuale SQL\nSQL √® uno dei linguaggi pi√π diffusi e standard per l‚Äôinterrogazione e la manipolazione dei dati. √à nato 50 anni fa, quindi c‚Äô√® un gran numero di libri, tutorial, corsi, forum, cheatsheet, ecc. per imparare a usarlo, e ci sono centinaia di applicazioni, librerie, framework, dedicati.\nUn file Parquet √® interrogabile con SQL, quindi tutti possono usarlo per interrogare e analizzare i dati in questo formato.\nUn‚Äôapplicazione SQL ‚Äúvisuale‚Äù, open source e multi-piattaforma, che puoi usare per interrogare un file Parquet √® DBeaver. Questi i passi che dovrai seguire per interrogare un file Parquet con DBeaver:\n\nscaricarla e installarla (https://dbeaver.io/download/);\nlanciarla, aprire il menu Database e selezionare New Database Connection;\ncercare DuckDB, selezionarlo e fare click su Next;\n\n\n\n\n\n\n\nFigura¬†2: DBeaver - selezionare DuckDB come database\n\n\n\n\nimpostare :memory: come Path;\n\n\n\n\n\n\n\nFigura¬†3: DBeaver - impostazione path DuckDB\n\n\n\n\nfare click su Test Connection, che verificher√† la necessit√† di installare eventuali componenti mancanti. Se manca qualcosa, installarla facendo click su Download.\n\nA questo punto, nel riquadro di sinistra ‚ÄúDatabase Navigator‚Äù dovrebbe apparire una connessione al database DuckDB memory.\n\n\n\n\n\n\nFigura¬†4: DBeaver - riquadro Database\n\n\n\nPer lanciare una query non ti resta che fare click con il pulsante destro del mouse su memory, selezionare SQL Editor, poi New SQL script e scrivere la tua prima query, per leggere ad esempio le prime 5 righe del file Parquet:\nSELECT  *\n1FROM \"c:\\tmp\\progetti_esteso_20230831.parquet\"\nLIMIT 5\n\n1\n\nPer puntare al file, √® stato inserito il percorso del file Parquet.\n\n\n\n\n\n\n\n\nFigura¬†5: DBeaver - la prima query\n\n\n\nLa query di esempio di sopra, con i dati per ciclo di finanziamento di Tabella¬†1, √® invece il risultato di questa query, che puoi provare a lanciare in DBeaver:\nSELECT oc_descr_ciclo Ciclo, SUM(finanz_totale_pubblico) \"Totale finanziamento pubblico (‚Ç¨)\"\nFROM \"C\\tmp\\progetti_esteso_20230831.parquet\"\nGROUP BY oc_descr_ciclo\nORDER BY ciclo;\n\n\nA riga di comando con DuckDB\nSe preferisci lavorare da riga di comando, puoi usare lo straordinario DuckDB, un sistema di gestione di database relazionali (RDBMS), che supporta il formato Parquet.\nPer installarlo, puoi seguire le istruzioni disponibili qui: https://duckdb.org/docs/installation.\nPer usarlo non ti resta che lanciare il comando duckdb e scrivere la tua prima query.\nPotrebbe essere diversa dalle precedenti, come quella comodissima per avere un riepilogo rapido dei dati, basata sul comando SUMMARIZE di DuckDB: restituisce per ogni campo, il tipo di campo e una ricca serie di calcoli, il numero di valori distinti, la percentuali di valori nulli, il minimo, il massimo, la media, ecc.. Qui sotto la sintassi della query e un esempio di output in Figura¬†6.\nSUMMARIZE select * from 'progetti_esteso_20231231.parquet';\n\n\n\n\n\n\nFigura¬†6: Esempio di output del comando SUMMARIZE\n\n\n\n√à un comando di grande comodit√†, che si pu√≤ usare ad esempio per sapere quali sono le colonne che hanno meno del 10% di valori nulli:\nSELECT * FROM (summarize select * from 'progetti_esteso_20231231.parquet')\nWHERE\nnull_percentage &lt;10;\nSono 86 colonne su circa 200 e si potrebbe scegliere di concentrarsi su questo campione pi√π ristretto e velocizzare ulteriormente le operazioni.\nE sempre da SUMMARIZE ci si pu√≤ fare un‚Äôidea delle colonne pi√π ‚Äúdatose‚Äù, ovvero quelle con meno valori distinti, che sono spesso quelle pi√π interessanti per fare analisi e visualizzazioni, perch√© consentono di definire categorie (le regioni, il settore, la natura, ecc.).\nSELECT * FROM (summarize select * from 'progetti_esteso_20231231.parquet')\nWHERE\napprox_unique &lt;30;\n\n\n\n\n\n\nNota\n\n\n\nSUMMARIZE restituisce in realt√† un valore approssimativo, ma molto vicino al reale, dei valori distinti. Questo per ottimizzare i tempi di esecuzione.\n\n\n\n\nA riga di comando con VisiData\nVisiData √® ‚Äúil coltellino svizzero per i dati, che probabilmente non conosci‚Äù. E proprio con il formato Parquet se ne ha un‚Äôidea.\nBasta scrivere vd nome_file.parquet e premere INVIO, per aprire il file e iniziare a esplorarlo, filtrarlo, analizzarlo, ecc.. Con il file di OpenCoesione ci mette qualche secondo, perch√© √® un file grande, ma poi si pu√≤ iniziare a esplorarlo e ad esempio avere restituito il totale di finanziamento pubblico per regione.\n\n\n\n\n\n\nFigura¬†7: VisiData - esplorare un file Parquet\n\n\n\n\n\nAccesso tramite Python\nLa Tabella¬†1 √® generata proprio a partire da codice Python, che legge il file Parquet dei progetti di OpenCoesione e ne estrae una sintesi. Un modo comodissimo per farlo √® usare la libreria duckdb e il suo metodo query, che permette di eseguire una query SQL e trasformare il risultato in un DataFrame di pandas.\n# importa modulo duckdb\nimport duckdb\n\n# definisci la query\nquery= \"\"\"\n  SELECT oc_descr_ciclo Ciclo, SUM(finanz_totale_pubblico) \"Totale finanziamento pubblico (‚Ç¨)\"\n  FROM 'progetti_esteso_20231231.parquet'\n  GROUP BY oc_descr_ciclo\n  ORDER BY ciclo;\n  \"\"\"\n\n# esegui la query e trasforma il risultato in un DataFrame\nriepilogo_finanziamento=duckdb.query(query).df()\nTi consigliamo di approfondire nella documentazione ufficiale dedicata.\n\n\nAccesso Observable\nL‚Äôultima modalit√† di accesso che ti proponiamo √® tramite Observable, una delle pi√π importanti e belle piattaforme/framework per la visualizzazione e l‚Äôanalisi dei dati.\n√à un esempio a nostro avviso significativo, perch√© mostra come il Parquet sia un formato pronto all‚Äôuso per una grandissima variet√† di ambienti, linguaggi e strumenti.\nIl linguaggio di programmazione di Observable √® JavaScript. La prima cosa da fare √® caricare il file Parquet, che pu√≤ essere fatto con la libreria duckdb.\ndb = DuckDBClient.of({\n  progetti: FileAttachment(\"progetti_esteso_20231231.parquet\")\n})\n\n\n\n\n\n\nNota\n\n\n\nNella versione di Observable gratuita e online, il limite delle dimensioni di un file √® di 50 MB. Ma utilizzando Observable Framework o con Quarto, le dimensioni dei file non sono un problema. In ogni caso, √® bene evitare di fare caricare nel DOM della pagina web array di dati molto grandi.\n\n\nPoi ad esempio si pu√≤ interrogare il file, ancora una volta con una query SQL, per avere ad esempio il conteggio dei progetti, per stato del progetto:\nviewof tbl_stato_proggetti = {\n  const data = await db.query(`SELECT\n  OC_STATO_PROGETTO AS \"Stato Progetto\", count(*) Conteggio FROM progetti\n  GROUP BY OC_STATO_PROGETTO\n  ORDER BY Conteggio DESC`)\n  return Inputs.table(data, {height: 200, layout: \"auto\",locale: \"it-IT\"})\n}\nIn output:\n\ndata = FileAttachment(\"observable.csv\").csv({ typed: true })\nInputs.table(data,{locale: \"it-IT\"})\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nE visto che siamo in un ambiente specializzato per la visualizzazione e l‚Äôanalisi dei dati, si pu√≤ anche visualizzare il risultato con un grafico a barre.\nviewof graficoStatoProgetti = {\n  const data = await db.query(`SELECT\n  OC_STATO_PROGETTO AS \"Stato Progetto\", count(*) AS Conteggio FROM progetti\n  GROUP BY OC_STATO_PROGETTO\n  ORDER BY Conteggio DESC`);\n\n  // Definisci una funzione di formattazione per il locale \"it-IT\"\n  const formatter = new Intl.NumberFormat(\"it-IT\").format;\n\n  // Crea il grafico a barre utilizzando Plot con formattazione personalizzata\n  return Plot.plot({\n    marks: [\n      Plot.barY(data, {x: \"Stato Progetto\", y: \"Conteggio\", fill: \"Stato Progetto\"}),\n    ],\n    width: 600,\n    height: 400,\n    marginLeft: 50, // Aumenta il margine sinistro se necessario\n    color: {\n      legend: true\n    },\n    x: {\n      label: \"Stato Progetto\"\n    },\n    y: {\n      label: \"Conteggio\",\n      // Usa la funzione di formattazione per le etichette dell'asse Y\n      tickFormat: formatter\n    },\n    style: {\n      overflow: \"visible\"\n    }\n  });\n}\nIn output:\n\nviewof graficoStatoProgetti = {\n\n  // Definisci una funzione di formattazione per il locale \"it-IT\"\n  const formatter = new Intl.NumberFormat(\"it-IT\").format;\n\n  // Crea il grafico a barre utilizzando Plot con formattazione personalizzata\n  return Plot.plot({\n    marks: [\n      Plot.barY(data, {x: \"Stato Progetto\", y: \"Conteggio\", fill: \"Stato Progetto\"}),\n    ],\n    width: 600,\n    height: 400,\n    marginLeft: 50, // Aumenta il margine sinistro se necessario\n    color: {\n      legend: true\n    },\n    x: {\n      label: \"Stato Progetto\"\n    },\n    y: {\n      label: \"Conteggio\",\n      // Usa la funzione di formattazione per le etichette dell'asse Y\n      tickFormat: formatter\n    },\n    style: {\n      overflow: \"visible\"\n    }\n  });\n}"
  },
  {
    "objectID": "posts/leggere-interrogare-file-parquet/index.html#note-finali",
    "href": "posts/leggere-interrogare-file-parquet/index.html#note-finali",
    "title": "Come leggere un file Parquet",
    "section": "Note finali",
    "text": "Note finali\nAbbiamo scritto questo post, perch√© crediamo che il formato Parquet sia un‚Äôopportunit√† per molte persone di migliorare la propria esperienza con i dati. E non solo per chi √® ‚Äúesperto‚Äù, ma anche per chi ha delle competenze di base con SQL. Perch√© come √® evidente le modalit√† di accesso sono molteplici e adatte a diversi livelli di competenza.\nIl fatto che OpenCoesione sia stato il primo progetto a scegliere di pubblicare i propri dati in formato Parquet √® un segnale molto importante: siamo confidenti che possa essere seguito presto da altre amministrazioni e organizzazioni.\nNon avremmo mai immaginato che la scrittura di questo altro post avrebbe contribuito a generare questa bella conseguenza. E per questo gli autori del post e tutta l‚Äôassociazione onData ringraziano OpenCoesione e il suo staff per essere stati prima in ascolto e poi protagonisti di questa scelta.\n\n\n\nFigura¬†1: esempio di visualizzazione e filtro dati con TAD\nFigura¬†4: DBeaver - riquadro Database\nFigura¬†5: DBeaver - la prima query\nFigura¬†6: Esempio di output del comando SUMMARIZE"
  }
]