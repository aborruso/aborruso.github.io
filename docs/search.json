[
  {
    "objectID": "posts/leggere-interrogare-file-parquet/index.html",
    "href": "posts/leggere-interrogare-file-parquet/index.html",
    "title": "Come leggere un file Parquet",
    "section": "",
    "text": "In questa piccola guida, ti porteremo per mano alla scoperta del formato Parquet e ti spiegheremo cos’è. Potresti preferirlo al buon vecchio formato CSV, visto che può rendere la tua vita con i dati un po’ più facile e molto più veloce.\nÈ un formato di archiviazione ottimizzato per lavorare con dati complessi e voluminosi. A differenza del CSV - che memorizza i dati per riga - Parquet organizza i dati per colonne.\nImmagina di avere una tabella con le 4 colonne ID, Nome, Età, E-mail. Come differisce l’accesso ai dati tra un file CSV e un file Parquet?\nIn un file CSV, se vuoi accedere soltanto alla colonna Età per tutte le righe, il sistema deve leggere l’intero file, riga per riga, per estrarre l’informazione relativa. Questo processo può essere piuttosto inefficiente, soprattutto con grandi volumi di dati, perché comporta la lettura di molti dati inutili (quelli delle colonne ID, Nome e E-mail).\n\n\ntabella.csv\n\nID,Nome,Età,E-mail\n1,Mario Rossi,30,mario.rossi@email.com\n2,Laura Bianchi,25,laura.bianchi@email.com\n...\n\nViceversa ad un file Parquet, essendo organizzato per colonne, si può accedere direttamente, e anche esclusivamente, alla colonna Età, senza dover leggere anche le altre colonne. Qui sotto un esempio, in si vede per accedere alla colonna Età, si possono saltare tutte le altre e leggere soltanto quella.\n\n\ntabella.parquet\n\nID: 1,2,...\nNome: Mario Rossi,Laura Bianchi,...\nEtà: 30,25,...\nE-mail: mario.rossi@email.com,laura.bianchi@email.com,...\n\nQuesto rende l’accesso ai dati molto più veloce ed efficiente."
  },
  {
    "objectID": "posts/leggere-interrogare-file-parquet/index.html#introduzione",
    "href": "posts/leggere-interrogare-file-parquet/index.html#introduzione",
    "title": "Come leggere un file Parquet",
    "section": "",
    "text": "In questa piccola guida, ti porteremo per mano alla scoperta del formato Parquet e ti spiegheremo cos’è. Potresti preferirlo al buon vecchio formato CSV, visto che può rendere la tua vita con i dati un po’ più facile e molto più veloce.\nÈ un formato di archiviazione ottimizzato per lavorare con dati complessi e voluminosi. A differenza del CSV - che memorizza i dati per riga - Parquet organizza i dati per colonne.\nImmagina di avere una tabella con le 4 colonne ID, Nome, Età, E-mail. Come differisce l’accesso ai dati tra un file CSV e un file Parquet?\nIn un file CSV, se vuoi accedere soltanto alla colonna Età per tutte le righe, il sistema deve leggere l’intero file, riga per riga, per estrarre l’informazione relativa. Questo processo può essere piuttosto inefficiente, soprattutto con grandi volumi di dati, perché comporta la lettura di molti dati inutili (quelli delle colonne ID, Nome e E-mail).\n\n\ntabella.csv\n\nID,Nome,Età,E-mail\n1,Mario Rossi,30,mario.rossi@email.com\n2,Laura Bianchi,25,laura.bianchi@email.com\n...\n\nViceversa ad un file Parquet, essendo organizzato per colonne, si può accedere direttamente, e anche esclusivamente, alla colonna Età, senza dover leggere anche le altre colonne. Qui sotto un esempio, in si vede per accedere alla colonna Età, si possono saltare tutte le altre e leggere soltanto quella.\n\n\ntabella.parquet\n\nID: 1,2,...\nNome: Mario Rossi,Laura Bianchi,...\nEtà: 30,25,...\nE-mail: mario.rossi@email.com,laura.bianchi@email.com,...\n\nQuesto rende l’accesso ai dati molto più veloce ed efficiente."
  },
  {
    "objectID": "posts/leggere-interrogare-file-parquet/index.html#lesempio-di-opencoesione",
    "href": "posts/leggere-interrogare-file-parquet/index.html#lesempio-di-opencoesione",
    "title": "Come leggere un file Parquet",
    "section": "L’esempio di OpenCoesione",
    "text": "L’esempio di OpenCoesione\nOpenCoesione è un progetto nazionale il cui obiettivo è promuovere trasparenza, collaborazione e partecipazione riguardo alle politiche di coesione nel paese.\nÈ da sempre uno dei progetti di riferimento per l’apertura dei dati in Italia, che ha fatto sempre scuola. E lo ha fatto ancora una volta: dal 21 febbraio 2024 ha iniziato a pubblicare la propria banca dati anche in formato Parquet.\nEd è probabilmente il primo progetto italiano di una Pubblica Amministrazione a farlo, e sicuramente il primo con una banca dati di questa ricchezza e dimensione.\n➡️ Stiamo parlando del catalogo dei Progetti con tracciato esteso, disponibile qui, anche in formato Parquet: https://opencoesione.gov.it/it/opendata/#!progetti_section\nAbbiamo sottolineato come il formato Parquet sia molto più efficiente per l’accesso e l’analisi dei dati, rispetto al CSV. Questa caratteristica è molto evidente, soprattutto per tabelle molto grandi, come questa dei progetti di OpenCoesione, composta da circa 2.000.000 di righe x 200 colonne.\nÈ possibile ad esempio interrogarla, per avere restituito il totale di finanziamento pubblico per ogni ciclo di finanziamento (vedi Tabella 1), e avere la risposta in 0,07 secondi.\n\n\n\n\nTabella 1: totale di finanziamento pubblico per ciclo di finanziamento\n\n\n\n\n\n\n\n\nCiclo\nTotale finanziamento pubblico (€)\n\n\n\n\nCiclo di programmazione 2000-2006\n17.667.666.307,18\n\n\nCiclo di programmazione 2007-2013\n101.629.942.347,96\n\n\nCiclo di programmazione 2014-2020\n154.061.098.680,89\n\n\nCiclo di programmazione 2021-2027\n7.465.189.173,71\n\n\n\n\n\n\n\n\nE questa rapidità si ottiene sul proprio computer di lavoro, senza che sia necessario mettere in campo risorse di calcolo particolarmente potenti e dispendiose sul cloud. O senza che sia necessario importare il file in un database relazionale, con tutte le operazioni di trasformazione e pulizia dei dati che questo comporta evitando anche l’installazione e la configurazione di un db relazionale."
  },
  {
    "objectID": "posts/leggere-interrogare-file-parquet/index.html#come-leggere-un-file-parquet",
    "href": "posts/leggere-interrogare-file-parquet/index.html#come-leggere-un-file-parquet",
    "title": "Come leggere un file Parquet",
    "section": "Come leggere un file Parquet",
    "text": "Come leggere un file Parquet\nSe non hai mai sentito parlare di questo formato, probabilmente penserai che per te sia impossibile usarlo per leggere, filtrare, analizzare, ecc. i dati di OpenCoesione. Penserai che è un formato solo per “tecnici”. Niente di più sbagliato! Leggere un file Parquet è facile e veloce, e paradossalmente è più necessario l’aiuto di un tecnico per leggere un file CSV di 4,5 Gigabyte (come quello dei progetti di OpenCoesione). Per la gran parte degli utenti è infatti impossibile leggere un file di queste dimensioni, anche con un buon Personal Computer. E non pensare di utilizzare programmi come Excel, hanno un limite di circa 1.000.000 di righe per foglio di lavoro (tante ma non sufficienti nel nostro caso in cui ne abbiamo circa 2.000.000).\n\n\n\n\n\n\nNota bene\n\n\n\nUn file di queste dimensioni, aldilà del formato, deve essere gestito con attenzione e con un minimo di competenza.\n\n\n\nAl doppio click\nPer visualizzare un file Parquet con un semplice doppio click, puoi usare Tad, un visualizzatore di file Parquet (e anche CSV, SQLite e DuckDB) open source, gratuito e disponibile per Windows, Mac e Linux.\nUna volta installato, basterà fare doppio click sul file per aprirlo e visualizzarne il contenuto (vedi Figura 1). Questo di OpenCoesione è un file grande e sarà necessario qualche secondo.\n\n\n\n\n\n\nFigura 1: esempio di visualizzazione e filtro dati con TAD\n\n\n\nTra le funzionalità di Tad c’è anche la possibilità di filtrare i dati. Nell’immagine di sopra:\n\nil filtro applicato;\nil modulo per costruire il filtro;\nil mini report sul numero di righe filtrate.\n\n\n\nCon un client visuale SQL\nSQL è uno dei linguaggi più diffusi e standard per l’interrogazione e la manipolazione dei dati. È nato 50 anni fa, quindi c’è un gran numero di libri, tutorial, corsi, forum, cheatsheet, ecc. per imparare a usarlo, e ci sono centinaia di applicazioni, librerie, framework, dedicati.\nUn file Parquet è interrogabile con SQL, quindi tutti possono usarlo per interrogare e analizzare i dati in questo formato.\nUn’applicazione SQL “visuale”, open source e multi-piattaforma, che puoi usare per interrogare un file Parquet è DBeaver. Questi i passi che dovrai seguire per interrogare un file Parquet con DBeaver:\n\nscaricarla e installarla (https://dbeaver.io/download/);\nlanciarla, aprire il menu Database e selezionare New Database Connection;\ncercare DuckDB, selezionarlo e fare click su Next;\n\n\n\n\n\n\n\nFigura 2: DBeaver - selezionare DuckDB come database\n\n\n\n\nimpostare :memory: come Path;\n\n\n\n\n\n\n\nFigura 3: DBeaver - impostazione path DuckDB\n\n\n\n\nfare click su Test Connection, che verificherà la necessità di installare eventuali componenti mancanti. Se manca qualcosa, installarla facendo click su Download.\n\nA questo punto, nel riquadro di sinistra “Database Navigator” dovrebbe apparire una connessione al database DuckDB memory.\n\n\n\n\n\n\nFigura 4: DBeaver - riquadro Database\n\n\n\nPer lanciare una query non ti resta che fare click con il pulsante destro del mouse su memory, selezionare SQL Editor, poi New SQL script e scrivere la tua prima query, per leggere ad esempio le prime 5 righe del file Parquet:\nSELECT  *\n1FROM \"c:\\tmp\\progetti_esteso_20230831.parquet\"\nLIMIT 5\n\n1\n\nPer puntare al file, è stato inserito il percorso del file Parquet.\n\n\n\n\n\n\n\n\nFigura 5: DBeaver - la prima query\n\n\n\nLa query di esempio di sopra, con i dati per ciclo di finanziamento di Tabella 1, è invece il risultato di questa query, che puoi provare a lanciare in DBeaver:\nSELECT oc_descr_ciclo Ciclo, SUM(finanz_totale_pubblico) \"Totale finanziamento pubblico (€)\"\nFROM \"C\\tmp\\progetti_esteso_20230831.parquet\"\nGROUP BY oc_descr_ciclo\nORDER BY ciclo;\n\n\nA riga di comando con DuckDB\nSe preferisci lavorare da riga di comando, puoi usare lo straordinario DuckDB, un sistema di gestione di database relazionali (RDBMS), che supporta il formato Parquet.\nPer installarlo, puoi seguire le istruzioni disponibili qui: https://duckdb.org/docs/installation.\nPer usarlo non ti resta che lanciare il comando duckdb e scrivere la tua prima query.\nPotrebbe essere diversa dalle precedenti, come quella comodissima per avere un riepilogo rapido dei dati, basata sul comando SUMMARIZE di DuckDB: restituisce per ogni campo, il tipo di campo e una ricca serie di calcoli, il numero di valori distinti, la percentuali di valori nulli, il minimo, il massimo, la media, ecc.. Qui sotto la sintassi della query e un esempio di output in Figura 6.\nSUMMARIZE select * from 'progetti_esteso_20231231.parquet';\n\n\n\n\n\n\nFigura 6: Esempio di output del comando SUMMARIZE\n\n\n\nÈ un comando di grande comodità, che si può usare ad esempio per sapere quali sono le colonne che hanno meno del 10% di valori nulli:\nSELECT * FROM (summarize select * from 'progetti_esteso_20231231.parquet')\nWHERE\nnull_percentage &lt;10;\nSono 86 colonne su circa 200 e si potrebbe scegliere di concentrarsi su questo campione più ristretto e velocizzare ulteriormente le operazioni.\nE sempre da SUMMARIZE ci si può fare un’idea delle colonne più “datose”, ovvero quelle con meno valori distinti, che sono spesso quelle più interessanti per fare analisi e visualizzazioni, perché consentono di definire categorie (le regioni, il settore, la natura, ecc.).\nSELECT * FROM (summarize select * from 'progetti_esteso_20231231.parquet')\nWHERE\napprox_unique &lt;30;\n\n\n\n\n\n\nNota\n\n\n\nSUMMARIZE restituisce in realtà un valore approssimativo, ma molto vicino al reale, dei valori distinti. Questo per ottimizzare i tempi di esecuzione.\n\n\n\n\nA riga di comando con VisiData\nVisiData è “il coltellino svizzero per i dati, che probabilmente non conosci”. E proprio con il formato Parquet se ne ha un’idea.\nBasta scrivere vd nome_file.parquet e premere INVIO, per aprire il file e iniziare a esplorarlo, filtrarlo, analizzarlo, ecc.. Con il file di OpenCoesione ci mette qualche secondo, perché è un file grande, ma poi si può iniziare a esplorarlo e ad esempio avere restituito il totale di finanziamento pubblico per regione.\n\n\n\n\n\n\nFigura 7: VisiData - esplorare un file Parquet\n\n\n\n\n\nAccesso tramite Python\nLa Tabella 1 è generata proprio a partire da codice Python, che legge il file Parquet dei progetti di OpenCoesione e ne estrae una sintesi. Un modo comodissimo per farlo è usare la libreria duckdb e il suo metodo query, che permette di eseguire una query SQL e trasformare il risultato in un DataFrame di pandas.\n# importa modulo duckdb\nimport duckdb\n\n# definisci la query\nquery= \"\"\"\n  SELECT oc_descr_ciclo Ciclo, SUM(finanz_totale_pubblico) \"Totale finanziamento pubblico (€)\"\n  FROM 'progetti_esteso_20231231.parquet'\n  GROUP BY oc_descr_ciclo\n  ORDER BY ciclo;\n  \"\"\"\n\n# esegui la query e trasforma il risultato in un DataFrame\nriepilogo_finanziamento=duckdb.query(query).df()\nTi consigliamo di approfondire nella documentazione ufficiale dedicata.\n\n\nAccesso Observable\nL’ultima modalità di accesso che ti proponiamo è tramite Observable, una delle più importanti e belle piattaforme/framework per la visualizzazione e l’analisi dei dati.\nÈ un esempio a nostro avviso significativo, perché mostra come il Parquet sia un formato pronto all’uso per una grandissima varietà di ambienti, linguaggi e strumenti.\nIl linguaggio di programmazione di Observable è JavaScript. La prima cosa da fare è caricare il file Parquet, che può essere fatto con la libreria duckdb.\ndb = DuckDBClient.of({\n  progetti: FileAttachment(\"progetti_esteso_20231231.parquet\")\n})\n\n\n\n\n\n\nNota\n\n\n\nNella versione di Observable gratuita e online, il limite delle dimensioni di un file è di 50 MB. Ma utilizzando Observable Framework o con Quarto, le dimensioni dei file non sono un problema. In ogni caso, è bene evitare di fare caricare nel DOM della pagina web array di dati molto grandi.\n\n\nPoi ad esempio si può interrogare il file, ancora una volta con una query SQL, per avere ad esempio il conteggio dei progetti, per stato del progetto:\nviewof tbl_stato_proggetti = {\n  const data = await db.query(`SELECT\n  OC_STATO_PROGETTO AS \"Stato Progetto\", count(*) Conteggio FROM progetti\n  GROUP BY OC_STATO_PROGETTO\n  ORDER BY Conteggio DESC`)\n  return Inputs.table(data, {height: 200, layout: \"auto\",locale: \"it-IT\"})\n}\nIn output:\n\ndata = FileAttachment(\"observable.csv\").csv({ typed: true })\nInputs.table(data,{locale: \"it-IT\"})\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nE visto che siamo in un ambiente specializzato per la visualizzazione e l’analisi dei dati, si può anche visualizzare il risultato con un grafico a barre.\nviewof graficoStatoProgetti = {\n  const data = await db.query(`SELECT\n  OC_STATO_PROGETTO AS \"Stato Progetto\", count(*) AS Conteggio FROM progetti\n  GROUP BY OC_STATO_PROGETTO\n  ORDER BY Conteggio DESC`);\n\n  // Definisci una funzione di formattazione per il locale \"it-IT\"\n  const formatter = new Intl.NumberFormat(\"it-IT\").format;\n\n  // Crea il grafico a barre utilizzando Plot con formattazione personalizzata\n  return Plot.plot({\n    marks: [\n      Plot.barY(data, {x: \"Stato Progetto\", y: \"Conteggio\", fill: \"Stato Progetto\"}),\n    ],\n    width: 600,\n    height: 400,\n    marginLeft: 50,\n    color: {\n      legend: true\n    },\n    x: {\n      label: \"Stato Progetto\"\n    },\n    y: {\n      label: \"Conteggio\",\n      // Usa la funzione di formattazione per le etichette dell'asse Y\n      tickFormat: formatter\n    },\n    style: {\n      overflow: \"visible\"\n    }\n  });\n}\nIn output:\n\nviewof graficoStatoProgetti = {\n\n  // Definisci una funzione di formattazione per il locale \"it-IT\"\n  const formatter = new Intl.NumberFormat(\"it-IT\").format;\n\n  // Crea il grafico a barre utilizzando Plot con formattazione personalizzata\n  return Plot.plot({\n    marks: [\n      Plot.barY(data, {x: \"Stato Progetto\", y: \"Conteggio\", fill: \"Stato Progetto\"}),\n    ],\n    width: 600,\n    height: 400,\n    marginLeft: 50,\n    color: {\n      legend: true\n    },\n    x: {\n      label: \"Stato Progetto\"\n    },\n    y: {\n      label: \"Conteggio\",\n      // Usa la funzione di formattazione per le etichette dell'asse Y\n      tickFormat: formatter\n    },\n    style: {\n      overflow: \"visible\"\n    }\n  });\n}"
  },
  {
    "objectID": "posts/leggere-interrogare-file-parquet/index.html#note-finali",
    "href": "posts/leggere-interrogare-file-parquet/index.html#note-finali",
    "title": "Come leggere un file Parquet",
    "section": "Note finali",
    "text": "Note finali\nAbbiamo scritto questo post, perché crediamo che il formato Parquet sia un’opportunità per molte persone di migliorare la propria esperienza con i dati. E non solo per chi è “esperto”, ma anche per chi ha delle competenze di base con SQL. Perché come è evidente le modalità di accesso sono molteplici e adatte a diversi livelli di competenza.\nIl fatto che OpenCoesione sia stato il primo progetto a scegliere di pubblicare i propri dati in formato Parquet è un segnale molto importante: siamo confidenti che possa essere seguito presto da altre amministrazioni e organizzazioni.\nNon avremmo mai immaginato che la scrittura di questo altro post avrebbe contribuito a generare questa bella conseguenza. E per questo gli autori del post e tutta l’associazione onData ringraziano OpenCoesione e il suo staff per essere stati prima in ascolto e poi protagonisti di questa scelta."
  },
  {
    "objectID": "til/quarto-mescolare-observable-altair/index.html",
    "href": "til/quarto-mescolare-observable-altair/index.html",
    "title": "Fare convivere una cella Observable e un grafico Altair in Quarto",
    "section": "",
    "text": "Non è possibile in Quarto fare convivere una cella di codice di tipo Observable, con una cella Python con un grafico Altair.\n\n\nVedi issue 3424\nC’è però un workaround:\n\nda Altair generare la descrizione del grafico in formato JSON (è in formato vega-lite), con chart.to_json();\nfare leggere a una cella Observable il JSON, e visualizzare il grafico.\n\nQui ad esempio creo una cella Observable usata soltanto come esempio.\n\n```{ojs}\n//| echo: fenced\ndata = FileAttachment(\"ojs.csv\").csv({ typed: true })\nInputs.table(data)\n```\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPoi genero la descrizione di un grafico vega-lite, con Altair, salvando il file chart.json.\n\n```{python}\nimport pandas as pd\nimport altair as alt\nimport warnings\n\nwarnings.simplefilter(action='ignore', category=FutureWarning)\n\ndf = pd.read_csv(\"altair.csv\",keep_default_na=False)\n\ndf['year'] = pd.to_datetime(df['year'], format='%Y')\n\nchart=alt.Chart(df).mark_area().encode(\n    alt.X('year:T', timeUnit = 'year',title='year',axis=alt.Axis(tickCount='year')),\n    alt.Y('v:Q',axis=alt.Axis(format='%'),title='percentage'),\n    color='i:N'\n)\nchart.save('chart.json')\n```\n\nE infine faccio leggere a Observable la descrizione del grafico, che è stata generata da Altair e lo faccio visualizzare.\n\n```{ojs}\n//| echo: fenced\nfile = FileAttachment(\"chart.json\").json()\nembed = require(\"vega-embed@6\")\nembed(file)\n```\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Torna in cima"
  },
  {
    "objectID": "posts/beni_immobili_pubblica_amministrazione/index.html",
    "href": "posts/beni_immobili_pubblica_amministrazione/index.html",
    "title": "I (quasi) tre milioni di beni immobili della Pubblica Amministrazione",
    "section": "",
    "text": "L’ENEA - Agenzia nazionale per le nuove tecnologie, l’energia e lo sviluppo economico sostenibile - ha pubblicato a fine luglio del 2024 il rapporto “La consistenza del parco immobiliare nazionale”. Per la parte relativa ai fabbricati di proprietà pubblica è stato realizzato con il contributo dei dati forniti dal Dipartimento dell’Economia (DE) – Direzione III Valorizzazione del patrimonio pubblico.\nQuesto dipartimento infatti gestisce la banca dati degli immobili pubblici, alla quale è associata la pubblicazione dei dati aperti in formato CSV, con licenza CC-BY 4.0: Elenco dei beni immobili dichiarati dalle Amministrazioni Pubbliche per l’anno 2019, ai sensi dell’art. 2, comma 222, legge 23 dicembre 2009, n. 191\n\nPer ciascuna Amministrazione sono riportate le seguenti informazioni sui beni immobili dichiarati: identificativo univoco del bene (unità immobiliare, u.i.), identificativi catastali (o codice bene nel caso di immobili non accatastati), titolo e quota di proprietà, localizzazione, tipologia immobiliare, dimensione (superficie/cubatura), epoca di costruzione, natura giuridica, vincolo culturale paesaggistico, appartenenza a un compendio, tipo di utilizzo e finalità, indicazione se l’unità immobiliare è data in uso a terzi, totalmente o in parte. Per beni non di proprietà, sono riportati: titolo di detenzione, superficie detenuta (se non si detiene l’intera u.i.), Amministrazione da cui si è ricevuto il bene (Amministrazione cedente). Le informazioni sulle detenzioni a favore di terzi, relative all’intera u.i. o ad una parte, sono contenute nel corrispondente file .csv Detenzioni a favore di terzi.\n\n“In considerazione dell’elevato numero di record” - circa 2,8 milioni - il Dipartimento ha suddiviso la pubblicazione di questi dati in circa 30 file CSV compressi (zip). Per fortuna tutti con un solo schema.\nIn questo articolo voglio mostrare come metterli di nuovo insieme, in modo da poter fare elaborazioni e sintesi come quelle dell’ENEA. Tenuti compressi, separati e in questo formato, infatti, non sono molto comodi da gestire. DuckDB si rivela nuovamente uno strumento prezioso per questo compito (vedi l’articolo “Gestire file CSV grandi, brutti e cattivi”).\nAncora una volta sarà l’occasione - lo farò in rappresentanza dell’associazione onData - di proporre al Dipartimento di pubblicare i dati anche in altre modalità. E magari andrà bene, così come è avvenuto con OpenCoesione.\nPrima di passare all’“azione”, una nota sulla sottostante Figura 1. Si tratta di una mappa che rappresenta la densità degli immobili pubblici in Italia, una cosiddetta heatmap: le aree più scure indicano una maggiore densità di immobili. Nella brillante vignetta di xkcd, viene mostrato come le heatmap siano spesso inutili perché, in sostanza, riflettono la distribuzione della popolazione: le “cose” si trovano dove ci sono le persone. Anche in questo caso è così, ma si notano colori insolitamente tenui anche in alcune aree densamente popolate del Sud Italia. Tuttavia, questa è un’analisi che spetterà a qualche altra persona.\n\n\n\n\n\n\nFigura 1: heatmap della posizione degli immobili"
  },
  {
    "objectID": "posts/beni_immobili_pubblica_amministrazione/index.html#introduzione",
    "href": "posts/beni_immobili_pubblica_amministrazione/index.html#introduzione",
    "title": "I (quasi) tre milioni di beni immobili della Pubblica Amministrazione",
    "section": "",
    "text": "L’ENEA - Agenzia nazionale per le nuove tecnologie, l’energia e lo sviluppo economico sostenibile - ha pubblicato a fine luglio del 2024 il rapporto “La consistenza del parco immobiliare nazionale”. Per la parte relativa ai fabbricati di proprietà pubblica è stato realizzato con il contributo dei dati forniti dal Dipartimento dell’Economia (DE) – Direzione III Valorizzazione del patrimonio pubblico.\nQuesto dipartimento infatti gestisce la banca dati degli immobili pubblici, alla quale è associata la pubblicazione dei dati aperti in formato CSV, con licenza CC-BY 4.0: Elenco dei beni immobili dichiarati dalle Amministrazioni Pubbliche per l’anno 2019, ai sensi dell’art. 2, comma 222, legge 23 dicembre 2009, n. 191\n\nPer ciascuna Amministrazione sono riportate le seguenti informazioni sui beni immobili dichiarati: identificativo univoco del bene (unità immobiliare, u.i.), identificativi catastali (o codice bene nel caso di immobili non accatastati), titolo e quota di proprietà, localizzazione, tipologia immobiliare, dimensione (superficie/cubatura), epoca di costruzione, natura giuridica, vincolo culturale paesaggistico, appartenenza a un compendio, tipo di utilizzo e finalità, indicazione se l’unità immobiliare è data in uso a terzi, totalmente o in parte. Per beni non di proprietà, sono riportati: titolo di detenzione, superficie detenuta (se non si detiene l’intera u.i.), Amministrazione da cui si è ricevuto il bene (Amministrazione cedente). Le informazioni sulle detenzioni a favore di terzi, relative all’intera u.i. o ad una parte, sono contenute nel corrispondente file .csv Detenzioni a favore di terzi.\n\n“In considerazione dell’elevato numero di record” - circa 2,8 milioni - il Dipartimento ha suddiviso la pubblicazione di questi dati in circa 30 file CSV compressi (zip). Per fortuna tutti con un solo schema.\nIn questo articolo voglio mostrare come metterli di nuovo insieme, in modo da poter fare elaborazioni e sintesi come quelle dell’ENEA. Tenuti compressi, separati e in questo formato, infatti, non sono molto comodi da gestire. DuckDB si rivela nuovamente uno strumento prezioso per questo compito (vedi l’articolo “Gestire file CSV grandi, brutti e cattivi”).\nAncora una volta sarà l’occasione - lo farò in rappresentanza dell’associazione onData - di proporre al Dipartimento di pubblicare i dati anche in altre modalità. E magari andrà bene, così come è avvenuto con OpenCoesione.\nPrima di passare all’“azione”, una nota sulla sottostante Figura 1. Si tratta di una mappa che rappresenta la densità degli immobili pubblici in Italia, una cosiddetta heatmap: le aree più scure indicano una maggiore densità di immobili. Nella brillante vignetta di xkcd, viene mostrato come le heatmap siano spesso inutili perché, in sostanza, riflettono la distribuzione della popolazione: le “cose” si trovano dove ci sono le persone. Anche in questo caso è così, ma si notano colori insolitamente tenui anche in alcune aree densamente popolate del Sud Italia. Tuttavia, questa è un’analisi che spetterà a qualche altra persona.\n\n\n\n\n\n\nFigura 1: heatmap della posizione degli immobili"
  },
  {
    "objectID": "posts/beni_immobili_pubblica_amministrazione/index.html#scaricare-i-file",
    "href": "posts/beni_immobili_pubblica_amministrazione/index.html#scaricare-i-file",
    "title": "I (quasi) tre milioni di beni immobili della Pubblica Amministrazione",
    "section": "Scaricare i file",
    "text": "Scaricare i file\nL’ho fatto in blocco. Ho prima esplorato un po’ la pagina e ho notato che tutti i file hanno un URL che inizia con la stringa /modules e termina per .zip. Allora ho iniziato dall’estrarre la lista di questi file, usando l’espressione regolare /modules.+.zip:\nSupponiamo di lavorare in una cartella immobili_pubblici:\n\nmkdir immobili_pubblici\ncd immobili_pubblici\n\n# usiamo la variabile $folder per tener traccia della cartella di lavoro\n\nfolder=$(pwd)\nURL=\"https://www.de.mef.gov.it/it/attivita_istituzionali/patrimonio_pubblico/censimento_immobili_pubblici/open_data_immobili/dati_immobili_2019.html\"\n\n# Otteniamo il file con i cookies del sito per poter accedere successivamente\nmkdir tmp\ncurl -c \"${folder}/tmp/cookies.txt\" \"$URL\"\n\ncurl -s -b \"${folder}/tmp/cookies.txt\" -H 'User-Agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/128.0.0.0 Safari/537.36' \"$URL\" | grep -P -o '/modules.+.zip' | tee \"${folder}/tmp/filelist.txt\"\n\n\n\n\n\n\nNota\n\n\n\n\nè necessario generare prima il file cookies.txt con il comando curl -c \"$folder\"/tmp/cookies.txt \"$URL\" e poi leggerlo;\ngrep -P -o è per usare le espressioni regolari Perl ed estrarre solo la parte che interessa;\n$folder è la mia cartella di lavoro dello script;\n\n\n\nIn output si ottiene (ne metto soltanto una piccola parte):\n/modules/documenti_it/attivo_patrimonio/2019/open_data_imm/Imm_Camere_di_commercio_ed_Unioni_di_Camere_di_Commercio_2019.zip\n/modules/documenti_it/attivo_patrimonio/2019/open_data_imm/Imm_Province_e_Citta_Metropolitane_2019.zip\n/modules/documenti_it/attivo_patrimonio/2019/open_data_imm/Imm_Enti_Locali_del_Servizio_Sanitario_Nazionale_2019.zip\n/modules/documenti_it/attivo_patrimonio/2019/open_data_imm/Imm_Amministrazioni_Regionali_2019.zip\n/modules/documenti_it/attivo_patrimonio/2019/open_data_imm/Imm_Universita_2019.zip\n/modules/documenti_it/attivo_patrimonio/2019/open_data_imm/Imm_Amministrazioni_Comunali_ABRUZZO_2019.zip\n/modules/documenti_it/attivo_patrimonio/2019/open_data_imm/Imm_Amministrazioni_Comunali_BASILICATA_2019.zip\nCon la lista degli URL dei file disponibile, basta generare un while loop per scaricarli tutti (sempre con curl o wget).\nmkdir -p \"${folder}/data/raw\" && cd \"${folder}/data/raw/\"\n\n# Scarico tutti i file in filelist.txt\nxargs -I '{}'  wget -c 'https://www.de.mef.gov.it{}' &lt; \"${folder}/tmp/filelist.txt\"\n\ncd \"${folder}\""
  },
  {
    "objectID": "posts/beni_immobili_pubblica_amministrazione/index.html#elaborazione-dei-file-scaricati",
    "href": "posts/beni_immobili_pubblica_amministrazione/index.html#elaborazione-dei-file-scaricati",
    "title": "I (quasi) tre milioni di beni immobili della Pubblica Amministrazione",
    "section": "Elaborazione dei file scaricati",
    "text": "Elaborazione dei file scaricati\nA partire dalla lista dei file zip scaricati, ancora una volta si può generare un while loop per estrarli tutti in una cartella temporanea.\n\n\n\n\n\n\nAvviso\n\n\n\nFacendolo è emerso un primo piccolo problema: uno dei file zip ha una struttura diversa: contiene il CSV in una sotto cartella. Quindi era necessario creare un processo di unzip automatico che non tenesse conto della eventuale struttura a cartelle presente.\n\n\nfind \"${folder}/data/raw\" -type f -name \"*.zip\" -exec unzip -j '{}' -d \"${folder}/data/raw\" \\;\nUna volta estratti tutti i file e in presenza di uno schema dati comune, è molto semplice creare un unico file di output, usando duckdb. Il comando di base è:\n\n\n\nLista 1: esempio di unione di più file CSV a partire da una cartella che li contiene.\n\n\nduckdb --csv -c \"SELECT * FROM read_csv_auto('${folder}/data/raw/*.csv')\" &gt;output.csv\n\n\n\nAl primo tentativo però duckdb è andato in errore, per ragioni legate all’encoding dei caratteri dei file CSV, perché DuckDB supporta solo file CSV con encoding UTF-8.\nMa qual è l’encoding dei file CSV dell’Elenco dei beni immobili dichiarati dalle Amministrazioni Pubbliche? Purtroppo sul sito del Dipartimento non c’è traccia di questa informazione. Nelle linee guida open data dell’Agenzia per l’Italia Digitale (AgID) è riportato:\n\nè sempre necessario utilizzare una codifica standardizzata dei caratteri. In genere, UTF-8 è la codifica utilizzata nel web. È utile, in ogni caso, indicare qual è la codifica dei caratteri utilizzata.\n\nIn assenza dell’informazione si può provare a estrarre l’encoding facendo inferenza dai dati. Io in questi casi uso l’ottimo chardet, che per questi file ha dato come risultato la codifica Windows-1252.\nMappata la codifica di input li ho convertiti in UTF-8 con iconv:\nmkdir -p \"${folder}/data/raw/utf8\"\ncd \"${folder}/data/raw\"\nfind -type f -name \"*.csv\" -exec iconv -f Windows-1252 -t UTF-8 '{}' &gt;'utf8/{}' \\;\n\nr=2830855\nr_ita=r.toLocaleString('it-IT');\nc=51\nc_ita=c.toLocaleString('it-IT');\ncelle=(r*c).toLocaleString('it-IT');\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCon la conversione di encoding fatta, si può lanciare il comando visibile in Lista 1 e ottenere il file output.csv con tutti i dati “uniti”. Ma sono  righe per  colonne, ovvero  di celle ed è meglio comprimere l’output, così come consigliato qui.\nSi può modificare il comando di Lista 1 in questo modo:\nduckdb -c \"COPY (SELECT * FROM read_csv_auto('${folder}/data/raw/utf8/*.csv'))\nTO 'beni_immobili_pubblici.csv.gz'\"\nIn 20 secondi si ottiene un file CSV compresso di circa 68 MB. Non è un formato comodo per rapide elaborazioni, ma con gli strumenti giusti può sempre sorprendere. Il summarize di duckdb viene eseguito ad esempio in 2 secondi, con una comodissima sintesi su  di celle. Il file csv.gz di output è disponibile qui.\n\n\n\n\n\n\nFigura 2: esempio di output del comando summarize duckdb\n\n\n\nPer altri tipi di query, i tempi sarebbero molto più lunghi.\nSe si vuole conciliare un’ottima compressione, con una grande velocità di interrogazione e analisi dei dati, si può scegliere come formato di output il Parquet.\nIl comando per creare un file in questo formato è molto simile a quello per il CSV:\nduckdb -c \"COPY (\n  SELECT\n    *\n  FROM\n    read_csv_auto('${folder}/data/raw/utf8/*.csv')\n) TO 'beni_immobili_pubblici.parquet'\n(\n  FORMAT PARQUET,\n  COMPRESSION 'zstd',\n  ROW_GROUP_SIZE 100_000\n)\"\nIn circa 25 secondi si ha in output un file Parquet di circa 55 MB, che si può interrogare molto rapidamente con tanti strumenti ed essenzialmente da tutti i linguaggi di programmazione; su questo Davide Taibi e io abbiamo scritto un articolo di approfondimento.\nCreato il file Parquet, ho iniziato a fare le prime query e subito mi hanno dato fastidio tre cose:\n\ni nomi delle colonne sono troppo human readable, con spazi, caratteri speciali e mescola di maiuscole e minuscole (Sup. aree pertinenziali (mq), Vinc. culturale/paesaggistico, Immobile Geo-Ref., ecc.);\nalcuni campi numerici, come superficie e cubatura dei beni, risultano in output come stringhe;\nsi perde la traccia dei file di origine (il record xxx, da che file CSV di input ha origine?).\n\nPer il punto 1, c’è l’eccezionale parametro normalize_names, che se impostato a TRUE converte i nome delle colonne in snake_case, con tutte le lettere minuscole, gli spazi sostituiti da _ e con rimossi i caratteri speciali.\nPer il punto 2, si possono forzare i tipi di campo (parametro types) e il separatore dei decimali (parametro decimal_separator che per questi file di input è la ,).\n\n\n\n\n\n\nNota bene\n\n\n\nNella prossima versione di duckdb, basterà dichiarare che il separatore decimale sia la virgola e duckdb sarà in grado di inferire correttamente i tipi campo FLOAT; non sarà più necessario dichiararlo esplicitamente per ognuno (vedi paragrafo in fondo).\n\n\nPer il punto 3 si può usare il parametro filename, che se impostato a TRUE aggiunge una colonna con il nome del file di origine.\nMettendo tutto insieme, il comando per creare un file Parquet diventa:\n\n\n\nLista 2: comando finale per creare al meglio il file Parquet.\n\n\nduckdb -c \"COPY (\n  SELECT\n    * REPLACE (regexp_replace(filename, '^.+/', '') AS filename)\n  FROM\n    read_csv_auto(\n      '${folder}/data/raw/utf8/*.csv',\n      filename = TRUE,\n      types = { 'id_bene': 'VARCHAR',\n      'id_compendio': 'VARCHAR',\n      'latitudine': 'FLOAT',\n      'longitudine': 'FLOAT',\n      'superficie_mq': 'FLOAT',\n      'cubatura_mc': 'FLOAT',\n      'sup_aree_pertinenziali_mq': 'FLOAT',\n      'superficie_di_riferimento_mq': 'FLOAT' },\n      normalize_names = TRUE,\n      decimal_separator = ','\n    )\n) TO 'beni_immobili_pubblici.parquet' (\n  FORMAT PARQUET,\n  COMPRESSION 'zstd',\n  ROW_GROUP_SIZE 100_000\n)\"\n\n\n\nNella Lista 2 è stato aggiunto il comando REPLACE (regexp_replace(filename, '^.+/', '') AS filename). Questo comando rimuove il percorso del file e lascia solo il suo nome.\nIl file parquet di output è disponibile qui."
  },
  {
    "objectID": "posts/beni_immobili_pubblica_amministrazione/index.html#analisi-geografica-dei-dati",
    "href": "posts/beni_immobili_pubblica_amministrazione/index.html#analisi-geografica-dei-dati",
    "title": "I (quasi) tre milioni di beni immobili della Pubblica Amministrazione",
    "section": "Analisi geografica dei dati",
    "text": "Analisi geografica dei dati\nIl 99% dei dati è associato a una coppia di coordinate geografiche, con le colonne latitudine e longitudine. Non entro qui nel merito sulla qualità (tutta da verificare) di questi dati, ma voglio mostrare come può esser comodo anche in questo caso duckdb. Questa applicazione ha infatti anche l’estensione spaziale ed è quindi possibile fare delle interrogazioni geografiche.\nQuanti sono ad esempio i beni immobili che ricadono in provincia di Bergamo?\nCon questa banca dati c’è un modo “tradizionale” per farlo, perché è presente la colonna provincia_del_bene e basta fare una query come quella di sotto, per sapere in 0,1 secondi che sono 39.154:\nduckdb -c \"SELECT count(*) conteggio\nFROM read_parquet('beni_immobili_pubblici.parquet')\nWHERE provincia_del_bene = 'BERGAMO'\"\nMa immaginiamo di non avere la colonna provincia_del_bene e avere soltanto le coordinate dei beni e il poligono che rappresenta la provincia di Bergamo. Questo poligono si può ottenere in formato geojson dal comodo confini-amministrativi.it e ha questo URL: https://confini-amministrativi.it/api/v2/it/20240101/unita-territoriali-sovracomunali/16.geo.json\nVoglio arricchire il risultato e conteggiare i beni per epoca di costruzione e suddividerli per utilizzo del bene. La query spaziale che in pochi secondi restituisce il risultato è la seguente:\nduckdb -c \"PIVOT (\n  SELECT\n    id_bene,\n    epoca_costruzione,\n    utilizzo_del_bene\n  FROM\n    'beni_immobili_pubblici.parquet' AS immobili\n    JOIN ST_READ(\n      'https://confini-amministrativi.it/api/v2/it/20240101/unita-territoriali-sovracomunali/16.geo.json'\n    ) AS bergamo ON ST_Within(ST_POINT(longitudine, latitudine), bergamo.geom)\n) ON utilizzo_del_bene USING COUNT(id_bene)\nGROUP BY epoca_costruzione\nORDER BY epoca_costruzione\"\nA partire da un sottoinsieme di colonne - id_bene, epoca_costruzione e utilizzo_del_bene - viene creata una tabella pivot (vedi Tabella 1). Questa organizza i dati in modo che ogni tipologia di utilizzo dei beni (utilizzo_del_bene) diventi una colonna separata. Le celle della tabella mostrano il numero di beni per ciascuna combinazione di epoca di costruzione (epoca_costruzione) e utilizzo, utilizzando la funzione di aggregazione COUNT(id_bene). Infine, i risultati vengono raggruppati per epoca di costruzione e ordinati in base a questa colonna.\n\n\n\n\n\n\nLa ricerca stavolta è geografica\n\n\n\nNella query precedente si filtravano le righe per ricerca testuale, ovvero provincia_del_bene = 'BERGAMO'. In questa invece si filtrano le righe in base alla posizione geografica, ovvero ST_Within(ST_POINT(longitudine, latitudine), bergamo.geom):\n\nprima si trasformano le colonne longitudine e latitudine in punti con ST_POINT(longitudine, latitudine);\npoi si verifica quali sono i punti che ricadono dentro il poligono della provincia di Bergamo con la funzione ST_Within().\n\n\n\n\n\n\nTabella 1: numero di beni immobili pubblici per epoca di costruzione e utilizzo, all’interno della provincia di Bergamo\n\n\n\n\n\n\n\n\n\n\n\n\n\nepoca_costruzione\nIn ristrutturazione/manutenzione\nInutilizzabile\nNon utilizzato\nUtilizzato direttamente\n\n\n\n\nDal 1919 al 1945\n20\n100\n145\n637\n\n\nDal 1946 al 1960\n8\n53\n83\n441\n\n\nDal 1961 al 1970\n2\n26\n214\n650\n\n\nDal 1971 al 1980\n8\n17\n392\n685\n\n\nDal 1981 al 1990\n9\n12\n190\n1585\n\n\nDal 1991 al 2000\n4\n5\n176\n1018\n\n\nDal 2001 al 2010\n2\n7\n104\n868\n\n\nDopo il 2010\n1\n0\n43\n195\n\n\nND\n0\n6\n62\n469\n\n\nPrima del 1919\n20\n78\n226\n993\n\n\n\n15\n256\n9294\n11510\n\n\n\n\n\n\n\nÈ un esempio di base, in cui è evidente come è possibile combinare dati geografici e non, in modo molto semplice e veloce, anche da fonti remote da raggiungere in HTTP.\nNel contesto “spaziale” può essere utile creare un file in formato geografico, da usare su un sistema GIS. Ad esempio per creare un file in formato FlatGeobuf, progettato per memorizzare dati geografici in modo efficiente e compatto, si può usare una sintassi simile a quella vista sopra:\nduckdb -c \"COPY (\n  SELECT\n    *,\n    ST_POINT(longitudine, latitudine) geom\n  FROM\n    'beni_immobili_pubblici.parquet'\n  WHERE\n    longitudine IS NOT NULL\n    OR latitudine IS NOT NULL\n) TO 'output.fgb' WITH (FORMAT GDAL, DRIVER 'FlatGeobuf', SRS 'EPSG:4326')\"\nIl formato parquet ha anche la sua versione geografica, il GeoParquet, molto compresso e molto veloce da interrogare. Per crearlo non si può ancora usare duckdb, ma si può usare il comando ogr2ogr di GDAL:\nogr2ogr -f parquet -lco COMPRESSION=ZSTD beni_immobili_pubblici_geo.parquet output.fgb\nIl file GeoParquet è disponibile qui, pesa soltanto 80 MB, ed è visualizzabile con QGIS e altre applicazioni GIS.\n\n\n\n\n\n\nFigura 3: file GeoParquet visualizzato in QGIS"
  },
  {
    "objectID": "posts/beni_immobili_pubblica_amministrazione/index.html#alcune-considerazioni-sui-dati",
    "href": "posts/beni_immobili_pubblica_amministrazione/index.html#alcune-considerazioni-sui-dati",
    "title": "I (quasi) tre milioni di beni immobili della Pubblica Amministrazione",
    "section": "Alcune considerazioni sui dati",
    "text": "Alcune considerazioni sui dati\nÈ possibile combinare rapidamente e facilmente questi file per creare una base di dati efficiente e pronta per l’analisi e le interrogazioni. Questo avviene non solo grazie a strumenti come DuckDB, ma anche perché i file di input condividono uno schema unico, che permette di unire i file CSV senza difficoltà.\nE come scritto nelle conclusioni di un “famoso” articolo:\n\nCon le dovute precauzioni, con gli strumenti giusti, con un corredo informativo adeguato e per dimensioni non eccessive, (il CSV) può essere un formato comodo e pratico. Descrivendolo, “standardizzandolo” e comprimendolo, diventa molto più usabile.\n\nA proposito di “descrizione”, il Dipartimento dell’Economia pubblica un file dizionario, uno schema, ma soltanto in formato PDF. Quindi non è possibile creare una procedura automatica di importazione dei dati, in cui sono ad esempio impostati correttamente i tipi di campo (testuali, numerici, ecc.).\n\n\n\n\n\n\n📌 Importante\n\n\n\nLa pubblicazione non si dovrebbe limitare a rendere disponibile dei file, ma essere accompagnata da una descrizione del file stesso, leggibile dalle persone e dalle applicazioni.\n\n\nSui CSV sarebbe ideale:\n\ndocumentare qual è l’encoding dei caratteri;\navere come encoding l’UTF-8;\nevitare l’utilizzo di spazi, virgolette o altri caratteri speciali nei nomi dei campi;\ndichiarare quali sono i separatori di campo e dei decimali;\npubblicarli anche non separati in 30 file, ma in unico file compresso gzip.\n\nQuesti preziosi dati inoltre non sono ancora disponibili anche nel portale nazionale dei dati aperti. Ed è un peccato in termini di valorizzazione e diffusione degli stessi. Questo è anche un requisito previsto dalle linee guida nazionali sui dati aperti.\nInterrogando la banca dati si ha evidenza di alcuni beni con coordinate geografiche incoerenti con gli attributi territoriali dichiarati in altri campi. Ad esempio il bene con id_bene=2442321, ha coordinate YX 37.926914 e 15.28337, ovvero in provincia di Messina; ma nel campo comune_del_bene il valore è “Frascati”.\nAggiungo anche una nota sull’opportunità di un formato aggiuntivo di pubblicazione: il tante volte citato Parquet. È diventato uno dei formati standard per consentire a chiunque di fare analisi rapide ed efficienti, anche su grandi quantità di dati, quasi su qualsiasi PC. E sarebbe ideale pubblicare la banca dati degli immobili pubblici anche in formato Parquet. Un buon esempio da emulare è quello di OpenCoesione - uno dei portali di dati aperti più importanti in Italia - che da febbraio 2024 pubblica anche in questo formato.\nSono dati al 2019. Sarebbe interessante che fossero aggiornati con una frequenza maggiore, magari annuale. Sul portale del Tesoro si legge “Fissata al 27 settembre 2024 la chiusura della rilevazione dei dati dei beni immobili pubblici riferiti al 31/12/2023” e quindi c’è da aspettarsi un prossimo aggiornamento."
  },
  {
    "objectID": "posts/beni_immobili_pubblica_amministrazione/index.html#cosa-cambia-con-duckdb-v1.1.0.",
    "href": "posts/beni_immobili_pubblica_amministrazione/index.html#cosa-cambia-con-duckdb-v1.1.0.",
    "title": "I (quasi) tre milioni di beni immobili della Pubblica Amministrazione",
    "section": "Cosa cambia con DuckDB v1.1.0.",
    "text": "Cosa cambia con DuckDB v1.1.0.\nOggi, 9 settembre 2024, a una settimana dalla pubblicazione di questo articolo, è stata rilasciata la versione 1.1.0 di DuckDB.\nRisolve un bug che non consentiva di utilizzare pienamente il comodissimo parametro decimal_separator: si era costretti comunque a dichiarare esplicitamente il tipo di campo FLOAT come in Lista 2. Se si impostava decimal_separator = ',', DuckDB non era in grado di inferire correttamente i tipi campo FLOAT e li considerava sempre delle stringhe. Ora, con la versione 1.1.0, basta dichiarare il separatore decimale e DuckDB sarà in grado di inferire correttamente i tipi campo FLOAT.\nSotto le due versioni del comando per creare il file Parquet:\n\nVersione 1.1.0Versione 1.0.0\n\n\nduckdb -c \"COPY (\n  SELECT\n    * REPLACE (regexp_replace(filename, '^.+/', '') AS filename)\n  FROM\n    read_csv_auto(\n      '${folder}/data/raw/utf8/*.csv',\n      filename = TRUE,\n      normalize_names = TRUE,\n      decimal_separator = ','\n    )\n) TO 'beni_immobili_pubblici.parquet' (\n  FORMAT PARQUET,\n  COMPRESSION 'zstd',\n  ROW_GROUP_SIZE 100_000\n)\"\n\n\nduckdb -c \"COPY (\n  SELECT\n    * REPLACE (regexp_replace(filename, '^.+/', '') AS filename)\n  FROM\n    read_csv_auto(\n      '${folder}/data/raw/utf8/*.csv',\n      filename = TRUE,\n      types = { 'id_bene': 'VARCHAR',\n      'id_compendio': 'VARCHAR',\n      'latitudine': 'FLOAT',\n      'longitudine': 'FLOAT',\n      'superficie_mq': 'FLOAT',\n      'cubatura_mc': 'FLOAT',\n      'sup_aree_pertinenziali_mq': 'FLOAT',\n      'superficie_di_riferimento_mq': 'FLOAT' },\n      normalize_names = TRUE,\n      decimal_separator = ','\n    )\n) TO 'beni_immobili_pubblici.parquet' (\n  FORMAT PARQUET,\n  COMPRESSION 'zstd',\n  ROW_GROUP_SIZE 100_000\n)\""
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Andrea Borruso",
    "section": "",
    "text": "Ciao\nGrazie per essere qui. Questo è uno spazio dove inserirò alcuni appunti sulle cose che imparo, sui progetti che mi piacciono e/o che sto sviluppando, sugli strumenti con cui lavoro e gioco e sulle persone che incontro.\nSono socio dell’associazione onData.\n\n\nQuando hai dato lo stesso consiglio 3 volte, scrivi un post (David Robinson).\n\n\n\n\n Torna in cima"
  },
  {
    "objectID": "til/un-sito-in-quarto/index.html",
    "href": "til/un-sito-in-quarto/index.html",
    "title": "Il mio primo blog post",
    "section": "",
    "text": "Uso quarto da diverse settimane per creare slide in HTML scritte in markdown e basate su reveal.js.\nQuarto è un sistema di pubblicazione scientifica e tecnica, open source, basato su Pandoc:\n\nCrea contenuti dinamici con Python, R, Julia e Observable;\nI documenti sono o file markdown in plain text o Jupyter notebook;\nConsente di pubblicare articoli, report, presentazioni, siti Web, blog e libri di alta qualità in HTML, PDF, MS Word, ePub e altri formati;\nConsente di creare contenuti utilizzando scientific markdown, incluse equazioni, citazioni, riferimenti incrociati, pannelli di immagini, didascalie, layout avanzato e altro ancora.\n\nQuello che ho fatto per creare la prima versione di questo sito è stato:\n\nInstallare quarto;\ncreare un nuovo progetto, dandogli per nome il mio profilo utente GitHub;\n\nquarto create-project aborruso.github.io --type website\n\nimpostare a docs la cartella di output di pubblicazione del sito, aggiungendo l’istruzione nel file _quarto.yml:\n\n\n\n_quarto.yml\n\nproject:\n  type: website\n  output-dir: docs\n\n\ncreare dei primi contenuti da pubblicare come questo post;\ncreare il repo aborruso.github.io su GitHub;\ngenerare il sito con il comando render\n\nquarto render ./\n\nimpostare come sorgente delle GitHub Pages del repo creato, la cartella docs citata sopra;\npubblicare tutto su GitHub.\n\nPer farlo, mi hanno aiutato queste letture:\n\nCreating your personal website using Quarto https://ucsb-meds.github.io/creating-quarto-websites/\nCreating a Website (dal sito ufficiale) https://quarto.org/docs/websites/\nCreating a Blog (dal sito ufficiale) https://quarto.org/docs/websites/website-blog.html\n\n\n\n\n\n\n\nImportante\n\n\n\nHo seguito questi step per la primissima pubblicazione, per vedere subito un primo risultato. Poi ho cambiato molte cose, quindi le impostazioni attuali sono diverse da quelle descritte sopra.\n\n\n\n\n\n Torna in cima"
  },
  {
    "objectID": "til/nushell-lista-file-piu-vecchi-di/index.html",
    "href": "til/nushell-lista-file-piu-vecchi-di/index.html",
    "title": "Estrarre la lista dei file creati più di 30 giorni fa",
    "section": "",
    "text": "La data di creazione non è un parametro disponibile e/o interrogabile su tutti i tipi di file sytem.\nIl meraviglioso nushell riesce a farlo un po’ ovunque.\nQuesto un esempio:\nls **\\*  -l | where created &lt;= (date now) - 30day\nAlcune note:\n\nlegge la cartella corrente e tutte le sue sottocartelle con **\\* (qui c’è il forward slash e quindi è per sistemi Windows; su Linux è **/*);\nfiltra tutti i file creati più di 30 giorni fa con where created &lt;= (date now) - 30day.\n\nSe si vogliono cancellare gli elementi presenti nella lista del comando precedente:\nls **\\*  -l | where created &lt;= (date now) - 30day | each { rm $in.name }\n$in è una variabile creata automaticamente in corrispondenza di una lista.\n\n\n\n Torna in cima"
  },
  {
    "objectID": "til/installare-duckdb-spatial/index.html",
    "href": "til/installare-duckdb-spatial/index.html",
    "title": "DuckDB: l’estensione spaziale",
    "section": "",
    "text": "È stata rilasciata questa estensione spaziale per DuckDB.\nUno dei modi per istallarla è scaricare i binari precompilati, accessibili dai workflow di compilazione.\n\n\n\nI workflow, per i vari sistemi operativi\n\n\nPer installarla:\n\ndecomprimere il file scaricato;\nlanciare duckdb con l’opzione unsigned, ovvero duckdb -unsigned;\ninstallare l’estensione usando il percorso assoluto del file (sotto un esempio)\n\ninstall '/home/user/spatial.duckdb_extension';\n\ncaricare l’estensione, con LOAD spatial;.\n\nE una volta caricata, potrai vedere tutti i nuovi formati file supportati da duckdb con\nselect * from ST_LIST_DRIVERS() order by 1;\n\n\n\nLa lista dei formati disponibili\n\n\n\n\n\n Torna in cima"
  },
  {
    "objectID": "til/filtrare-file-elenco-file-esterno/index.html",
    "href": "til/filtrare-file-elenco-file-esterno/index.html",
    "title": "Come filtrare un file di testo a partire da una lista di stringhe",
    "section": "",
    "text": "Avevo bisogno di filtrare un file CSV di grandi dimensioni, compresso in zip, a partire da una lista di stringhe contenute in un file. Dato il CSV, volevo estrarne soltanto le righe che contenevano una delle stringhe presenti nel file esterno.\nVia CLI, usando lo straordinario grep il comando è (list.txt, è il file che contiene per ogni riga la stringa da cercare):\nunzip -qq -c \"input.zip\"  | grep -F -f list.txt\nPer me questa modalità ha risolto tutto. Ma ne metto un paio di altre.\nUna è basata su ripgrep, un’altra straordinaria CLI per la ricerca di testo, più rapida di grep:\nunzip -qq -c \"input.zip\"  | rg -F -f list.txt\n\n\n\n\n\n\nNon si tiene conto del formato\n\n\n\nQueste due modalità non tengono però conto del formato CSV, non riescono ad esempio a cercare per colonna, ma solo per riga. Sotto una soluzione che riesce a farlo.\n\n\nCon qsv, è possibile ricercare per colonna:\nunzip -qq -c \"input.zip\"  | qsv searchset -d \"|\" -i -s nomeColonna list.txt\n\n\n\n\n\n\nNota\n\n\n\n\n-d \"|\" per impostare il separatore di colonna del CSV;\n-i per ignorare maiuscole e minuscole;\n-s nomeColonna per specificare la colonna in cui cercare.\n\n\n\n\n\n\n Torna in cima"
  },
  {
    "objectID": "til/duckdb-ottimizzazione-performance-import-csv-jsonl-creazione-parquet/index.html",
    "href": "til/duckdb-ottimizzazione-performance-import-csv-jsonl-creazione-parquet/index.html",
    "title": "DuckDB: creare un file parquet a partire da file di testo di grandi dimensioni",
    "section": "",
    "text": "In queste settimane ho guardato un po’ i dati della “Banca dati Servizio Contratti Pubblici - SCP” che contiene gli avvisi, i bandi e gli esiti di gara in formato aperto, raccolti dalla “Banca dati SCP - Servizio Contratti Pubblici”, gestita dalla Direzione Generale per la regolazione e i contratti pubblici del Ministero delle Infrastrutture e Trasporti.\nNel dataset sono presenti file CSV di medie dimensioni, come quello denominato v_od_atti.csv, composto da 685.000 righe per 45 colonne, per un totale di circa 570 MB.\nNon sono big data e ci sono tanti modi per interrogarlo e trasformarlo con poco sforzo e rapidità. Uno molto comodo è quello di usare DuckDB: prima per la conversione di formato e poi per tutte le query che si vorranno fare.\nMolto comodo convertire il CSV in formato parquet. Si passa da circa 570 a 45 MB, e si ha a disposizione un formato che è rapidissimo da interrogare.\nPer farlo si può usare DuckDB a riga di comando:\necho \"COPY (SELECT *\nFROM read_csv_auto('input.csv'))\nTO 'output.parquet' (FORMAT 'PARQUET',\nCODEC  'Snappy',PER_THREAD_OUTPUT TRUE);\" \\\n| duckdb\nE in 10 secondi (sulla mia macchina con 16 GB di RAM e un pentium 7) il file è pronto.\nLa preziosa fonte/ispirazione è il bravissimo Mark Litwintschik.\n\n\n\n Torna in cima"
  },
  {
    "objectID": "til/compilare-qsv/index.html",
    "href": "til/compilare-qsv/index.html",
    "title": "Installazione QSV",
    "section": "",
    "text": "Di base i comandi sono questi di sotto:\ngit clone https://github.com/jqnatividad/qsv.git\ncd qsv\ncargo build --release --locked --bin qsv -F all_features\nPer il mio ambiente è comodo, prima della compilazione, settare la variabile d’ambiente CARGO_BUILD_RUSTFLAGS, per avere in output un binario ottimizzato per la mia CPU:\nexport CARGO_BUILD_RUSTFLAGS='-C target-cpu=native'\nÈ consigliato avere un ambiente “pulito” prima della compilazione. Quindi la procedura potrebbe diventare questa:\ngit clone https://github.com/jqnatividad/qsv.git\ncd qsv\nrustup up\ncargo clean\ncargo build --release --locked --bin qsv -F all_features\nSe si ha poca RAM (meno di 16GB), è meglio rinunciare ad alcun feature (come to, che è oneroso da compilare):\ngit clone https://github.com/jqnatividad/qsv.git\ncd qsv\nrustup up\ncargo clean\ncargo build --release --locked --bin qsv -F feature_capable,apply,python,self_update,polars\nL’eseguibile compilato di qsv sarà generato in target/release/qsv. Se lo si vuole rendere disponibile a tutti gli utenti del sistema, si può copiare in /usr/local/bin o in qualsiasi altra cartella del PATH.\n\n\n\n Torna in cima"
  },
  {
    "objectID": "posts/tutto-il-pnrr-in-500k/index.html",
    "href": "posts/tutto-il-pnrr-in-500k/index.html",
    "title": "Tutti i dati aperti del PNRR in 500 kB",
    "section": "",
    "text": "Strumenti e dati “giusti” consentono di ottenere risultati sorprendenti, e sopratutto allargano la platea di chi può fare analisi e ricerca su certe informazioni.\nIn questo post sarà illustrato come un piccolo file di 500 kB, possa essere il punto di ingresso sui dati aperti del Piano Nazionale di Ripresa e Resilienza (PNRR). Si tratta di un file, di un database DuckDB, che “contiene” l’elenco delle principali tabelle del Catalogo Open Data di Italiadomani, il portale del Governo che ospita i dati del PNRR.\nLa parola “contiene” è tra virgolette perché il file non contiene realmente i dati, ma semplicemente delle viste, dei puntatori ai dati veri e propri. Ma per le modalità in cui è stato costruito, è possibile fare query e analisi senza quasi accorgersi di questo.\nIn questo modo è possibile allargare il numero di persone che possono elaborare questi dati, ma c’è un prerequisito: conoscere il linguaggio SQL.Il SQL è un linguaggio di programmazione che consente di interrogare le tabelle di un database."
  },
  {
    "objectID": "posts/tutto-il-pnrr-in-500k/index.html#introduzione",
    "href": "posts/tutto-il-pnrr-in-500k/index.html#introduzione",
    "title": "Tutti i dati aperti del PNRR in 500 kB",
    "section": "",
    "text": "Strumenti e dati “giusti” consentono di ottenere risultati sorprendenti, e sopratutto allargano la platea di chi può fare analisi e ricerca su certe informazioni.\nIn questo post sarà illustrato come un piccolo file di 500 kB, possa essere il punto di ingresso sui dati aperti del Piano Nazionale di Ripresa e Resilienza (PNRR). Si tratta di un file, di un database DuckDB, che “contiene” l’elenco delle principali tabelle del Catalogo Open Data di Italiadomani, il portale del Governo che ospita i dati del PNRR.\nLa parola “contiene” è tra virgolette perché il file non contiene realmente i dati, ma semplicemente delle viste, dei puntatori ai dati veri e propri. Ma per le modalità in cui è stato costruito, è possibile fare query e analisi senza quasi accorgersi di questo.\nIn questo modo è possibile allargare il numero di persone che possono elaborare questi dati, ma c’è un prerequisito: conoscere il linguaggio SQL.Il SQL è un linguaggio di programmazione che consente di interrogare le tabelle di un database."
  },
  {
    "objectID": "posts/tutto-il-pnrr-in-500k/index.html#come-interrogare-il-database",
    "href": "posts/tutto-il-pnrr-in-500k/index.html#come-interrogare-il-database",
    "title": "Tutti i dati aperti del PNRR in 500 kB",
    "section": "Come interrogare il database",
    "text": "Come interrogare il database\nIl file in formato DuckDB si può scaricare da qui. È leggibile con decine di software, per tutti i sistemi operativi.\nUno degli strumenti che si possono utilizzare è DBeaver, un client SQL open source per tutte le principali piattaforme, che consente di interrogare database di diversi tipi, tra cui DuckDB.\nUna volta installato e lanciato, si potrà scegliere a che tipo di database connettersi: in questo caso DuckDB (vedi Figura 1).\n\n\n\n\n\n\nFigura 1: Scegliere il database a cui connettersi\n\n\n\nNei passaggi successivi della configurazione si dovrà indicare a quale file DuckDB connettersi. Qui sarà il file pnrr.db scaricato in precedenza. Una volta connessi, il database sarà visibile sulla sinistra, nel navigatore dei database. E facendo clic sui vari segni &gt;, sarà possibile vedere le 1️⃣ tabelle e le 2️⃣ viste, ecc. in cui è strutturato (vedi Figura 2).\n\n\n\n\n\n\nFigura 2: Database DuckDB in DBeaver\n\n\n\nLa tabella è soltanto una, si chiama info_viste e contiene l’elenco di tutte le viste presenti nel database. In questa tabella, per ogni vista:\n\nil nome della vista;\nil titolo del dataset;\nl’URL della pagina del dataset;\nil file che ha fatto da sorgente;\nla descrizione del dataset;\nla data di osservazione dei dati che hanno fatto da sorgente.\n\nQuesta tabella è quindi una tabella di metadati. Mentre per accedere ai dati bisogna utilizzare una o più delle viste presenti.\nE una prima query potrebbe essere quella per conteggiare il numero di progetti del PNRR:\nSELECT COUNT() conteggio from pnrr.main.PNRR_Progetti;\nSono, al momento, 269.300 progetti. La cosa interessante è che il risultato è molto rapido, nonostante il db non contenga i dati, ma solo dei puntatori.\nEd è molto comodo, che strumenti come DBeaver abbiano l’auto-completamento delle query, come è possibile vedere in Figura 3.\n\n\n\n\n\n\nFigura 3: Esempio di query con autocompletamento in DBeaver\n\n\n\nIl PNRR è suddiviso in Missioni e la query per conteggiare il numero di progetti per Missione - che viene eseguita in 0.5 secondi, è:\nSELECT\n  \"Descrizione Missione\",\n  COUNT() conteggio\nFROM\n  pnrr.main.PNRR_Progetti\nGROUP BY\n  ALL\nORDER BY\n  conteggio DESC;\n\n\n\nDescrizione Missione\nconteggio\n\n\n\n\nDigitalizzazione, innovazione, competitività e cultura\n83661\n\n\nRivoluzione verde e transizione ecologica\n81486\n\n\nIstruzione e ricerca\n75677\n\n\nInclusione e coesione\n18088\n\n\nSalute\n10084\n\n\nInfrastrutture per una mobilità sostenibile\n285\n\n\nREPowerEU\n19\n\n\n\nMa per dare un’idea più efficace della comodità, di poter utilizzare un piccolo file da 500 kB, si può costruire una query più complessa che mette in relazione due viste, ovvero due dataset: quello dei Progetti e quello delle Localizzazioni. L’obiettivo è quello di creare una tabella di sintesi, una tabella pivot, per restituisca il numero di progetti per Missione per ogni Regione.\nLa query è\nPIVOT (\n  SELECT\n    loc.\"Descrizione Regione\" AS Regione,\n    p.Missione\n  FROM\n    \"pnrr\".\"PNRR_Progetti\" p\n    LEFT JOIN (\n      SELECT\n        DISTINCT CUP,\n        \"CODICE Locale Progetto\",\n        \"Descrizione Regione\"\n      FROM\n        \"pnrr\".\"PNRR_Localizzazione\"\n    ) loc ON p.CUP = loc.CUP\n    AND p.\"Codice Locale Progetto\" = loc.\"CODICE Locale Progetto\"\n) ON Missione\nE in pochi secondi viene restituito il risultato (vedi Figura 4).\n\n\n\n\n\n\nFigura 4: Tabella pivot con numero di progetti per Missione per Regione"
  },
  {
    "objectID": "posts/tutto-il-pnrr-in-500k/index.html#come-è-stato-creato-il-database",
    "href": "posts/tutto-il-pnrr-in-500k/index.html#come-è-stato-creato-il-database",
    "title": "Tutti i dati aperti del PNRR in 500 kB",
    "section": "Come è stato creato il database",
    "text": "Come è stato creato il database\nL’ispirazione per la creazione di questa modalità di accesso ai dati del PNRR, viene dalla lettura di questo ottimo articolo, in cui si spiega come DuckDB possa diventare un catalogo portatile per la gestione dei dati, da fonti diverse (file parquet da fonti Amazon S3, database PostgreSQL, tabelle su Google Cloud, ecc.).\nPer questo file è stata replicata la logica descritta nell’articolo, ma sono state necessarie delle pre-lavorazioni dei dati.\nSul sito Italiadomani i file sono disponibili in formato CSV (JSON e XLSX) e una query SQL tramite DuckDB si può fare puntando direttamente all’URL del file. Per sapere ad esempio quanti sono i progetti del PNRR la query sarebbe:\nSELECT COUNT(*) FROM read_csv_auto('https://www.italiadomani.gov.it/content/dam/sogei-ng/opendata/PNRR_Progetti.csv')\nQuesto è possibile grazie all’estensione httpfs di DuckDB, che consente di leggere file da URL HTTP o da Amazon S3. Ci sono però dei problemi:\n\na volte il server di Italiadomani non risponde in modo adeguato, e la query fallisce;\nil formato CSV non è ottimizzato per query su file “grandi”, che non risiedano sul proprio PC, perché è necessario scaricare tutto il file per fare la query.\n\nEntrambi i punti sono abbastanza bloccanti e, per superarli, sono state fatte due cose:\n\npubblicare i file con i dati in uno spazio web che rispondesse meglio alle chiamate HTTP;\nconvertire i file CSV in un formato più adatto per fare query su file “grandi” remoti, come il formato parquet.\n\nLo spazio web creato, che ospita i file è GitHub, e la cartella è questa: https://github.com/ondata/italian-public-sector-pnrr-data-guide/tree/main/data/italia-domani/parquet\nL’URL di accesso diretto ai file ha questa struttura: https://raw.githubusercontent.com/ondata/italian-public-sector-pnrr-data-guide/refs/heads/main/data/italia-domani/parquet/Nome_File.parquet\nQuindi ad esempio l’URL del dataset dei progetti è: https://raw.githubusercontent.com/ondata/italian-public-sector-pnrr-data-guide/refs/heads/main/data/italia-domani/parquet/PNRR_Progetti.parquet\nE la query per conteggiare i progetti è quindi:\nSELECT COUNT(*) AS numero_progetti\nFROM\n'https://raw.githubusercontent.com/ondata/italian-public-sector-pnrr-data-guide/refs/heads/main/data/italia-domani/parquet/PNRR_Progetti.parquet';\nIl database DuckDB è stato implementato creando una vista per ogni file parquet presente nella suddetta cartella:\nCREATE\nOR replace view \"PNRR_Progetti\" AS\nSELECT\n  *\nFROM\n  read_parquet(\n    'https://raw.githubusercontent.com/ondata/italian-public-sector-pnrr-data-guide/refs/heads/main/data/italia-domani/parquet/PNRR_Progetti.parquet'\n  );"
  },
  {
    "objectID": "posts/tutto-il-pnrr-in-500k/index.html#due-esempi-di-accesso-live",
    "href": "posts/tutto-il-pnrr-in-500k/index.html#due-esempi-di-accesso-live",
    "title": "Tutti i dati aperti del PNRR in 500 kB",
    "section": "Due esempi di accesso “live”",
    "text": "Due esempi di accesso “live”\nUn database così strutturato, dà la possibilità di abilitare l’accesso ai dati in modo “live”, ovvero senza dover scaricare il file e interrogarlo in locale.\nDue applicazioni web, che consentono di accedere a risorse remote e interrogarle, lanciando una versione di DuckDB nel browser, senza dovere installare nulla sono queste due:\n\nla DuckDB Shell;\nil SQL Workbench.\n\nIn entrambi i casi basta prima dare il comando di ATTACH per collegare il database DuckDB remoto, e poi fare tutte le query che si vogliono.\nAd esempio, per la tabella pivot vista sopra, basterà per entrambi i casi:\nATTACH 'https://raw.githubusercontent.com/ondata/italian-public-sector-pnrr-data-guide/refs/heads/main/data/italia-domani/parquet/pnrr.db' as pnrr (READ_ONLY);\n\nPIVOT (\n  SELECT\n    loc.\"Descrizione Regione\" AS Regione,\n    p.Missione\n  FROM\n    \"pnrr\".\"PNRR_Progetti\" p\n    LEFT JOIN (\n      SELECT\n        DISTINCT CUP,\n        \"CODICE Locale Progetto\",\n        \"Descrizione Regione\"\n      FROM\n        \"pnrr\".\"PNRR_Localizzazione\"\n    ) loc ON p.CUP = loc.CUP\n    AND p.\"Codice Locale Progetto\" = loc.\"CODICE Locale Progetto\"\n) ON Missione ORDER BY REGIONE;\nEd è interessante il fatto che le query costruite in questo modo, possano essere condivise con altre persone con un link, come questi due (sotto le immagini con l’anteprima dei risultati):\n\nDuckDB shell;\nSQL Workbench.\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigura 5: DuckDB Shell\n\n\n\n\n\n\n\n\n\n\n\nFigura 6: SQL Workbench"
  },
  {
    "objectID": "posts/tutto-il-pnrr-in-500k/index.html#lo-strumento-consigliato",
    "href": "posts/tutto-il-pnrr-in-500k/index.html#lo-strumento-consigliato",
    "title": "Tutti i dati aperti del PNRR in 500 kB",
    "section": "Lo strumento consigliato",
    "text": "Lo strumento consigliato\nUna cosa comodissima di questa modalità di accesso ai dati PNRR, è che è possibile leggerli nel modo preferito: con un client SQL come DBeaver da installare sulla propria macchina, con un client web come DuckDB Shell o SQL Workbench, con un linguaggio di programmazione come Python o R, ecc. e anche da riga di comando.\nPer chi è abituato a lavorare da riga di comando, un ottimo strumento è il sorprendente e comodissimo Harlequin, gratuito e open source, installabile su qualsiasi sistema operativo, e in grado di accedere a decine di database, tra cui DuckDB.\n\n\n\n\n\n\nFigura 7: Esempio d’uso di Harlequin"
  },
  {
    "objectID": "posts/tutto-il-pnrr-in-500k/index.html#note-conclusive",
    "href": "posts/tutto-il-pnrr-in-500k/index.html#note-conclusive",
    "title": "Tutti i dati aperti del PNRR in 500 kB",
    "section": "Note conclusive",
    "text": "Note conclusive\nIn questo post si vuole sottolineare la comodità di accedere a una banca dati, a partire da un file piccolo e senza rinunciare alla possibilità di costruire analisi complesse e ricche. Non è la modalità facile e unica con cui capire tutto sui numeri del PNRR, ma si ha la possibilità di fare un primo passo, e di farlo in modo molto più semplice di quanto si possa pensare.\nCi sono alcuni requisiti propedeutici: approfondire come è strutturato il PNRR, leggere i metadati delle tabelle (in ogni pagina dei dati, c’è un link ai metadati, vedi Figura 8), e conoscere il linguaggio SQL.\n\n\n\n\n\n\nFigura 8: Metadati di una tabella\n\n\n\nQuesto è un primo rilascio del database, per mostrare un caso d’uso “tecnico”. Al momento non è una risorsa su cui è pianificato un aggiornamento automatico e non è stato fatto alcun controllo di qualità. In prospettiva è probabile che verrà creata una procedura automatica per aggiornare il database, e verrà migliorata la qualità dei dati con verifiche sulla coerenza dello schema dati.\nI file sorgente, pubblicati su Italiadomani hanno alcuni problemi, di facile risoluzione, a da conoscere:\n\nl’encoding dei file CSV. È nella gran parte dei casi UTF-8, ma in alcuni casi non è così e bisogna fare attenzione per l’accesso e l’eventuale conversione;\nl’importante file CSV sui “Soggetti dei progetti del PNRR” - che fra l’altro è uno di quelli di dimensioni maggiori - contiene diverse righe non corrette, con un numero di colonne non compatibile con lo schema.\n\nRiprendo infine una riflessione già fatta nell’apprezzato post “Gestire file CSV grandi, brutti e cattivi”: con uno sforzo piccolo e un po’ di cura, è possibile pubblicare dati in modalità che ampliano di molto il numero di persone che possono accedervi e utilizzarli in modo efficace. Ad esempio:\n\nquesti file potrebbero pure essere pubblicati su un bucket di Amazon S3 (o altre modalità simili), e DuckDB (o altri client) potrebbe leggerli da lì, con grande rapidità;\nsi potrebbero pubblicare anche in formato compresso, ad esempio in csv.gz, o ancora meglio in formati efficienti e specializzati per l’analisi, come il parquet;\nsi potrebbe pubblicare anche un unico file in formato DuckDB, con tutte le tabelle del PNRR, con la corretta definizione dei tipi di campo, con definite le chiavi primarie, le chiavi esterne, ecc.."
  },
  {
    "objectID": "posts/llm-cmd-suggerimenti-ai-a-riga-di-comando/index.html",
    "href": "posts/llm-cmd-suggerimenti-ai-a-riga-di-comando/index.html",
    "title": "I suggerimenti dell’intelligenza artificiale per la riga di comando",
    "section": "",
    "text": "Una delle vignette più famose e (da me) amate di xkcd è quella che mostra una persona che deve disattivare una bomba, ma per farlo deve ricordarsi la sintassi di tar. Ma l’impresa è ardua, perché è uno di quei comandi della cli, di cui spesso non ricordiamo la sintassi.\n\n\n\n“tar”, una meravigliosa vignetta di xkcd\n\n\nCon l’“uscita” quotidiana di “cose” legate all’intelligenza artificiale, stanno arrivando soluzioni anche per questo problema: una ce la dà il bellissimo progetto di quel genio di Simon Willison, che ha creato il plugin llm-cmd per la sua straordinaria applicazione LLM.\nUna volta installato, si apre la shell e si digita il comando llm cmd seguito da una richiesta, da un prompt, su ciò che si vuole fare.\nSotto ad esempio chiedo come decomprimo il file data.tar.gz dalla cartella corrente alla cartella /home/tmp?.\nE in risposta ho tar -xzvf data.tar.gz -C /home/tmp. E se lo valuto corretto, posso premere INVIO e il comando verrà eseguito.\n👏 A Simon!!\n\n\n\nEsempio di utilizzo di llm-cmd\n\n\n\n\n\n\n\n\nNota\n\n\n\nLLM di default utilizza il modello gpt-3.5-turbo di OpenAI, ma è possibile configurare il proprio modello.\n\n\n\n\n\n Torna in cima"
  },
  {
    "objectID": "posts/csv-ventitre-gigabyte-senza-affanno/index.html",
    "href": "posts/csv-ventitre-gigabyte-senza-affanno/index.html",
    "title": "Lavorare con grandi file CSV compressi",
    "section": "",
    "text": "Nota successiva alla pubblicazione\n\n\n\nQuesto post ci ha consentito di fare una prima lettura di questa importante banca dati e sono emerse alcune criticità. Una è quelle citata nel post, sulle righe con numero di colonne errato. Quella che ci sembra più importante è relativa alla natura dei progetti, che era la novità più importante: sul CSV ci sono soltanto 3 tipologie, quindi nulla è cambiato. Le altre tipologie al momento sono visibili soltanto sul front-end del sito. È stato annunciato, anche dopo una nostra segnalazione, che i dati saranno aggiornati in tal senso entro la fine di febbraio 2024.\nDa pochissimo, dal 14 febbraio 2024, è andato online il nuovo portale OpenCUP.\nOpenCUP mette a disposizione di tutti - cittadini, istituzioni ed altri enti - i dati, in formato aperto, sulle decisioni di investimento pubblico finanziate con fondi pubblici nazionali, comunitarie o regionali o con risorse private registrate con il Codice Unico di Progetto.\nLa novità più importante di questo aggiornamento, è legata alla natura degli interventi in elenco: a “Lavori Pubblici” e “Incentivi alle imprese e Contributi per calamità naturali”, si aggiungono “acquisti di beni, servizi, corsi di formazione, strumenti finanziari, progetti di ricerca e contributi a entità diverse dalle unità produttive per una Pubblica Amministrazione più trasparente e vicina al cittadino” (qui la notizia di lancio).\nIn numeri si passa da circa 6,5 a 9,5 milioni di progetti."
  },
  {
    "objectID": "posts/csv-ventitre-gigabyte-senza-affanno/index.html#gli-open-data-di-opencup",
    "href": "posts/csv-ventitre-gigabyte-senza-affanno/index.html#gli-open-data-di-opencup",
    "title": "Lavorare con grandi file CSV compressi",
    "section": "Gli Open Data di OpenCUP",
    "text": "Gli Open Data di OpenCUP\nSul sito di OpenCUP, è disponibile da tempo la sezione OpenData. Con questo aggiornamento, sono stati pubblicati i nuovi dati, in formato CSV e XML. Bisogna scorrere la pagina e guardare la sezione “Progetti OpenData”.\nIn questo post, mostreremo rapidamente come esplorare il file “Complessivo” di tutti i progetti.\nÈ un file “grande”. Quello in formato CSV è compresso come zip: pesa circa 2.9 GB, e decompresso 23 GB. È probabilmente il file CSV più grande (o uno dei più grandi), pubblicato in una sezione open data di un sito di una Pubblica Amministrazione italiana.\nNon è gestibile con un comune foglio di calcolo, né con tanti altri strumenti. A riga di comando invece si può osservare, filtrare, e analizzare, con grande leggerezza e rapidità.\n\n\n\n\n\n\nAvviso\n\n\n\nCi sono alcune righe con un numero di separatori di campo errato: devono essere 90. Per estrarre soltanto quelle con 90 separatori si può usare awk:\nunzip -p \"OpenData Complessivo.zip\"  | awk '{if(gsub(/\\|/,\"&\") == 90) print}' &gt;open_cup_corrette.csv\nVedi anche la sezione Importazione dell’intero file."
  },
  {
    "objectID": "posts/csv-ventitre-gigabyte-senza-affanno/index.html#la-prima-lettura",
    "href": "posts/csv-ventitre-gigabyte-senza-affanno/index.html#la-prima-lettura",
    "title": "Lavorare con grandi file CSV compressi",
    "section": "La prima lettura",
    "text": "La prima lettura\nUn file così grande richiede un po’ di tempo per il download e poi per la decompressione. Per fortuna però è possibile fare una prima lettura immediata, dopo il download, senza decomprimere per intero il file.\nSi può usare l’utilty unzip e lanciare il comando unzip -p nomefile.zip. Ma per una prima esplorazione, l’ideale è leggere soltanto le prime righe. E per fortuna esiste il comando head, che fa proprio questo e di default legge le prime 10 righe.:\nunzip -p \"OpenData Complessivo.zip\"  | head\nE subito si vedrà qualcosa come in Figura 2, che ci restituisce già elementi interessanti:\n\ni nomi dei campi;\nil separatore è il |\n\n\n\n\n\n\n\nFigura 2: Le prime righe del file con i progetti"
  },
  {
    "objectID": "posts/csv-ventitre-gigabyte-senza-affanno/index.html#estrazione-di-un-campione",
    "href": "posts/csv-ventitre-gigabyte-senza-affanno/index.html#estrazione-di-un-campione",
    "title": "Lavorare con grandi file CSV compressi",
    "section": "Estrazione di un campione",
    "text": "Estrazione di un campione\nViste le prime 10 righe e mappate le caratteristiche del CSV, il passo consigliato successivo è quello dell’estrazione di un campione di righe più ampio, per fare un’analisi più approfondita. Con una piccola modifica al comando soprastante, si possono ad esempio estrarre le prime 10.000 righe e salvarle in un nuovo file CSV (in 0.055 secondi):\nunzip -p \"OpenData Complessivo.zip\"  | head -n 10000 &gt; campione.csv\nE questo nuovo file sarà esplorabile con foglio elettronico o con altri strumenti. A me ad esempio piace usare VisiData per queste cose, con cui di solito subito osservo un po’ di dati sui campi (i valori nulli, i valori distinti, ecc.).\n\n\n\n\n\n\nFigura 3: Esplorazione dei dati campione con VisiData\n\n\n\n\n\n\n\n\n\nNota\n\n\n\nIl professore Taibi ci ha fatto notare che questo non è un vero campione, ma soltanto le prime n righe del file. Un vero campione dovrebbe essere estratto in modo casuale, ma sarebbe necessario decomprimere tutto il file per farlo. E quindi qui, per un’esplorazione rapida, ci accontentiamo di questo."
  },
  {
    "objectID": "posts/csv-ventitre-gigabyte-senza-affanno/index.html#filtrare-lintero-file",
    "href": "posts/csv-ventitre-gigabyte-senza-affanno/index.html#filtrare-lintero-file",
    "title": "Lavorare con grandi file CSV compressi",
    "section": "Filtrare l’intero file",
    "text": "Filtrare l’intero file\nL’esplorazione del campione consente di farsi un’idea di quali sono quei campi/colonne che fanno un po’ da elementi “categorici”, da usare come filtro per estrarre solo le righe che ci interessano. Tra questi ad esempio il campo SOGGETTO_TITOLARE, con il nome della Pubblica Amministrazione, titolare del progetto, che posso usare per estrarre ad esempio tutti i progetti in cui la PA titolare è “REGIONE AUTONOMA DELLA SICILIA”.\nL’utility grep - una delle più importanti di tutti i tempi - è perfetta per questo scopo.\nunzip -p \"OpenData Complessivo.zip\"  | grep -P '\\|REGIONE AUTONOMA DELLA SICILIA\\|' &gt;regione_siciliana.csv\nSi applica un filtro al file CSV zippato di input, e per ogni riga in cui è presente la stringa REGIONE AUTONOMA DELLA SICILIA tra due caratteri |, viene salvata nel file regione_siciliana.csv. Nel filtro è stato inserito il carattere \\ prima del |, perché | è un carattere speciale. E in un minuto e mezzo circa, sul mio buon (ma normale) PC ottengo 392.395 progetti associati alla Regione Siciliana, dopo aver letto 9,5 milioni di righe.\nMa così c’è un problema: si perde l’intestazione del file, si perdono i nomi delle colonne. C’è allora da modificare il filtro ed estrarre anche la prima riga. E tutto questo è facile, grazie all’esplorazione fatta, in cui ho visto che la prima riga contiene ad esempio la stringa “ANNO_DECISIONE”. Il nuovo comando sarà:\nunzip -p \"OpenData Complessivo.zip\"  | \\\ngrep -P '(ANNO_DECISIONE|\\|REGIONE AUTONOMA DELLA SICILIA\\|)' &gt;regione_siciliana.csv\nCosì vengono cercate tutte le righe che contengono o ANNO_DECISIONE o |REGIONE AUTONOMA DELLA SICILIA| e quindi viene restituito un CSV con i nomi delle colonne e le righe filtrate."
  },
  {
    "objectID": "posts/csv-ventitre-gigabyte-senza-affanno/index.html#un-file-parquet-a-partire-dai-dati-filtrati",
    "href": "posts/csv-ventitre-gigabyte-senza-affanno/index.html#un-file-parquet-a-partire-dai-dati-filtrati",
    "title": "Lavorare con grandi file CSV compressi",
    "section": "Un file parquet a partire dai dati filtrati",
    "text": "Un file parquet a partire dai dati filtrati\nQuesto ultimo passo, perché è utile avere a disposizione questi dati anche in un formato più efficiente, come il parquet, che è un formato di file binario, compresso e colonnare, che permette di ridurre notevolmente lo spazio occupato e di velocizzare le operazioni di lettura e analisi. Per chi non ne ha mai sentito parlare, ne ho scritto lungamente qui.\nL’utility stavolta è DuckDB. E il comando è:\nunzip -p \"OpenData Complessivo.zip\"  | \\\ngrep -P '(ANNO_DECISIONE|\\|REGIONE AUTONOMA DELLA SICILIA\\|)' | \\\nduckdb -c \"COPY(\n  SELECT * FROM read_csv_auto('/dev/stdin', delim = '|')\n) TO 'regione_siciliana.parquet' (\n  FORMAT 'parquet',\n  COMPRESSION 'ZSTD',\n  ROW_GROUP_SIZE 100000\n)\"\nAlcune note su questo comando (messo su più righe, per migliorare la leggibilità):\n\nsi parte dal comando precedente, con il filtro per estrarre il campione;\nsi passa l’output a duckdb, che legge il CSV da stdin e nel SELECT al posto del nome del file è necessario inserire '/dev/stdin', che è il file virtuale che rappresenta lo standard input;\nsi usa il comando COPY per copiare questa selezione in un file parquet chiamato regione_siciliana.parquet, con compressione ZSTD e con un ROW_GROUP_SIZE di 100000.\n\nTutto circa sempre in un minuto e mezzo, con un file di output che pesa circa 33 MB, contro i 530 MB del file CSV."
  },
  {
    "objectID": "posts/csv-ventitre-gigabyte-senza-affanno/index.html#importazione-dellintero-file",
    "href": "posts/csv-ventitre-gigabyte-senza-affanno/index.html#importazione-dellintero-file",
    "title": "Lavorare con grandi file CSV compressi",
    "section": "Importazione dell’intero file",
    "text": "Importazione dell’intero file\nIl file CSV completo ha, alla data di oggi (15 Febbraio 2024) un problema a 2 righe (su 17.065.848), che hanno un numero errato di colonne (91 anziché 90). Per poterlo importare occorre filtrare solo le linee che hanno un numero di colonne corretto.\nÈ possibile importare il file in un DB in formato DuckDB con i seguenti comandi (occorre avere il file CSV su disco per limitare l’occupazione di RAM):\nunzip \"OpenData Complessivo.zip\"\nawk -F\\| '{if (NF-1 == 90) { print } }' &lt; TOTALE.csv &gt; TOTALEfix.csv\nduckdb OpenCUP.db -c \"CREATE TABLE OpenCUP AS SELECT * FROM read_csv_auto('TOTALEfix.csv',delim='|',header = true,dateformat='%d-%b-%y',parallel=FALSE,types={'DATA_ULTIMA_MODIFICA_SSC':'DATE','DATA_ULTIMA_MODIFICA_UTENTE':'DATE','DATA_CHIUSURA_REVOCA':'DATE','DATA_GENERAZIONE_CUP':'DATE'});\"\nNotare che è stato esplicitato il formato delle date, e quali sono i campi di tipo DATE, in modo che DuckDB converta correttamente i campi relativi, che sono del tipo 01-JAN-15. Questo permette sia una migliore gestione dei dati, che una rappresentazione più efficiente in termini di spazio su disco.\nIl file DuckDB risultante, a fronte di un CSV di 23.946.237.777 byte, è 4.431.294.464 byte, con una riduzione a circa 1/6, merito del formato binario contro quello ASCII.\n\nPrestazioni del formato DuckDB\nUna query di esempio\ntime duckdb OpenCUP.db -c \"SELECT ANNO_DECISIONE, COUNT(*) FROM OpenCUP GROUP BY ANNO_DECISIONE\" &gt; /dev/null\n\nreal    0m0,039s\nuser    0m0,156s\nsys 0m0,063s\n\n\nConversione nel formato Parquet\nCon il comando\nduckdb OpenCUP.db  -c \"COPY (SELECT * FROM OpenCUP) TO 'OpenCUP.parquet' (FORMAT 'PARQUET', CODEC 'ZSTD');\nsi ottiene un file Parquet di 2.296.193.848 byte, con una riduzione di circa 1/2 rispetto al formato nativo DuckDB.\nSe con questo comando si hanno problemi di memoria si può provare a impostare SET preserve_insertion_order=false:\nduckdb OpenCUP.db  -c \"SET preserve_insertion_order=false;COPY (SELECT * FROM OpenCUP) TO 'OpenCUP.parquet' (FORMAT 'PARQUET', CODEC 'ZSTD');\"\nLe prestazioni del formato Parquet sono notevoli\ntime duckdb -s 'SELECT ANNO_DECISIONE, COUNT (*) FROM read_parquet(\"OpenCUP.parquet\") GROUP BY ANNO_DECISIONE;' &gt; /dev/null\n\nreal    0m0,074s\nuser    0m0,372s\nsys 0m0,043s\n\n\nNote sui tempi di esecuzione\nI tempi di esecuzione sono stati misurati su un computer abbastanza “normale”, con 16 GB di RAM e questo processore:\nVendor ID:               GenuineIntel\n  Model name:            12th Gen Intel(R) Core(TM) i7-1280P\n    CPU family:          6\n    Model:               154\n    Thread(s) per core:  2\n    Core(s) per socket:  10\nQuesta la tabella riassuntiva dei tempi di esecuzione:\n\n\n\n\n\n\n\nOperazione\nTempo (minuti:secondi)\n\n\n\n\nDownload (con wget)\n12:13.81\n\n\nDecompressione (con unzip)\n1:33.05\n\n\nEstrazione delle sole righe corrette (con ugrep)1\n2:25.63\n\n\nImportazione in DuckDB\n4:15.97\n\n\nConversione in Parquet\n0:28.811\n\n\n\n\n\nVersione più sintetica e rapida\nIn DuckDB, nell’import dei file CSV, c’è l’opzione ignore_errors=true, che permette di ignorare le righe che contengono errori (come quelle con un numero di colonne errato). E quindi si può saltare la creazione di un secondo grande CSV per applicare il filtro.\nInoltre, se non abbiamo bisogno del database DuckDB, possiamo convertire direttamente il file CSV in parquet e saltare quindi un altro passaggio.\nIl comando sintetico è:\nduckdb -c \"SET preserve_insertion_order=false;COPY(SELECT * FROM read_csv_auto('TOTALE.csv',delim='|',header = true,ignore_errors=true,dateformat='%d-%b-%y',parallel=FALSE,types={'DATA_ULTIMA_MODIFICA_SSC':'DATE','DATA_ULTIMA_MODIFICA_UTENTE':'DATE','DATA_CHIUSURA_REVOCA':'DATE','DATA_GENERAZIONE_CUP':'DATE'})) TO 'opencup.parquet' WITH (FORMAT PARQUET, COMPRESSION ZSTD,ROW_GROUP_SIZE 100000)\"\nCosì facendo, c’è un risparmio di tempo di circa il 30%."
  },
  {
    "objectID": "posts/csv-ventitre-gigabyte-senza-affanno/index.html#note-finali",
    "href": "posts/csv-ventitre-gigabyte-senza-affanno/index.html#note-finali",
    "title": "Lavorare con grandi file CSV compressi",
    "section": "Note finali",
    "text": "Note finali\nQuesta descritta è soltanto una modalità per fare soprattutto una prima esplorazione e analisi di questo nuovo e importante aggiornamento dei dati di OpenCUP. È utilissima per capire per cosa è possibile usare questi dati, quali sono le informazioni che ci interessano di più, che storia poter raccontare, che mappa creare, che dashboard realizzare, come usare l’intelligenza artificiale per arricchirli, come metterli in relazione con altri dati, ecc..\nMa sono dati grandi, e in produzione bisognerà andare un po’ oltre l’utilizzo delle eccezionali utility a riga di comando. In ogni caso DuckDB, se usato bene, fa già tanta tanta roba (partizionando il dataset originario in più file parquet, si ha disposizione tutto il dataset con una grandissima facilità di accesso e lettura)."
  },
  {
    "objectID": "posts/csv-ventitre-gigabyte-senza-affanno/index.html#footnotes",
    "href": "posts/csv-ventitre-gigabyte-senza-affanno/index.html#footnotes",
    "title": "Lavorare con grandi file CSV compressi",
    "section": "Note",
    "text": "Note\n\n\nugrep '^(?:[^|]*\\|){90}[^|]*$' TOTALE.csv &gt;TOTALEfix.csv↩︎"
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Blog",
    "section": "",
    "text": "Tutti i dati aperti del PNRR in 500 kB\n\n\n\n\n\n\nduckdb\n\n\nopen-data\n\n\ncli\n\n\nparquet\n\n\npnrr\n\n\n\nNon è un titolo soltanto per farti fare click\n\n\n\n\n\n2 gen 2025\n\n\nAndrea Borruso\n\n\n\n\n\n\n\n\n\n\n\n\nI (quasi) tre milioni di beni immobili della Pubblica Amministrazione\n\n\n\n\n\n\nduckdb\n\n\nopen-data\n\n\ncli\n\n\nparquet\n\n\n\nUn dataset “grosso”, comodo da gestire e interrogare con duckdb\n\n\n\n\n\n2 set 2024\n\n\nAndrea Borruso, Matteo Fortini\n\n\n\n\n\n\n\n\n\n\n\n\nI suggerimenti dell’intelligenza artificiale per la riga di comando\n\n\n\n\n\n\ncmd\n\n\nai\n\n\n\nIn modo che non esploda la bomba …\n\n\n\n\n\n27 mar 2024\n\n\nAndrea Borruso\n\n\n\n\n\n\n\n\n\n\n\n\nCome leggere un file Parquet\n\n\n\n\n\n\nduckdb\n\n\nparquet\n\n\nsql\n\n\n\nTanti modi a partire dai dati di OpenCoesione\n\n\n\n\n\n22 feb 2024\n\n\nAndrea Borruso, Davide Taibi\n\n\n\n\n\n\n\n\n\n\n\n\nLavorare con grandi file CSV compressi\n\n\n\n\n\n\nduckdb\n\n\ncsv\n\n\ncli\n\n\n\nL’esempio dei nuovi dati OpenCUP, da sfogliare a riga di comando\n\n\n\n\n\n15 feb 2024\n\n\nAndrea Borruso, Matteo Fortini\n\n\n\n\n\n\n\n\n\n\n\n\nUsare la nuova intelligenza artificiale di Google\n\n\n\n\n\n\nai\n\n\ngoogle\n\n\ncli\n\n\n\nFarlo a riga di comando, già oggi che non è disponibile in Italia\n\n\n\n\n\n17 dic 2023\n\n\nAndrea Borruso\n\n\n\n\n\n\n\n\n\n\n\n\nGestire file CSV grandi, brutti e cattivi\n\n\n\n\n\n\nduckdb\n\n\ncsv\n\n\nparquet\n\n\n\nTips & tricks, ispirati da DuckDB, file Parquet e OpenCoesione\n\n\n\n\n\n21 ago 2023\n\n\nAndrea Borruso\n\n\n\n\n\n\nNessun risultato\n\n Torna in cima"
  },
  {
    "objectID": "til.html",
    "href": "til.html",
    "title": "TIL (Today I Learned)",
    "section": "",
    "text": "Che vuol dire TIL?\n\n\n\nMi piace quando le persone usano il loro sito web, per prendere appunti su alcune delle cose che imparano/scoprono. Come fa il mitico Simon Willison. E Simon lo fa in modalità TIL, ovvero Today I Learned.\nI miei post qui saranno spesso “piccoli”, dei TIL in forma di appunti, con qualche possibile lacuna e/o bruttura.\n\n\n\n\n\n\n\n\n   \n     \n     \n       Ordinare per\n       Predefinito\n         \n          Data - Meno recente\n        \n         \n          Data - Più recente\n        \n         \n          Titolo\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\nData\n\n\nTitolo\n\n\n\n\n\n\n27 dic 2024\n\n\nuv: installare o lanciare un’applicazione python\n\n\n\n\n21 dic 2024\n\n\nGitHub pages e file HTML in un sito Quarto\n\n\n\n\n5 set 2024\n\n\nIl meraviglioso fzf\n\n\n\n\n29 giu 2023\n\n\nInstallazione QSV\n\n\n\n\n23 apr 2023\n\n\nDuckDB: l’estensione spaziale\n\n\n\n\n4 mar 2023\n\n\nDuckDB: creare un file parquet a partire da file di testo di grandi dimensioni\n\n\n\n\n26 feb 2023\n\n\nQuarto: applicare stile CSS\n\n\n\n\n28 gen 2023\n\n\nEstrarre la lista dei file creati più di 30 giorni fa\n\n\n\n\n7 gen 2023\n\n\nPagina con codice R, Python e utility Bash\n\n\n\n\n3 dic 2022\n\n\nNushell: installarlo con il supporto ai dataframe\n\n\n\n\n28 nov 2022\n\n\nDuckDB: creare un file parquet a partire da un CSV\n\n\n\n\n26 nov 2022\n\n\nQuarto: renderizzare una tabella a partire da un CSV\n\n\n\n\n22 nov 2022\n\n\nFare convivere una cella Observable e un grafico Altair in Quarto\n\n\n\n\n21 nov 2022\n\n\nQuarto: leggere un CSV via Obeservable e visualizzare i dati\n\n\n\n\n20 nov 2022\n\n\nCome filtrare un file di testo a partire da una lista di stringhe\n\n\n\n\n19 nov 2022\n\n\nIl mio primo blog post\n\n\n\n\n\nNessun risultato\n\n Torna in cima"
  },
  {
    "objectID": "til/duckdb-creare-parquet-csv/index.html",
    "href": "til/duckdb-creare-parquet-csv/index.html",
    "title": "DuckDB: creare un file parquet a partire da un CSV",
    "section": "",
    "text": "DuckDB ha una cli molto comoda e potente.\nSe si vuole ad esempio creare il file parquet del file CSV degli Indicatori di rischio idrogeologico pubblicati da ISPRA, questo è il comando da lanciare:\nduckdb -c \"CREATE TABLE comuni_pir AS SELECT * FROM comuni_pir.csv;EXPORT DATABASE '.' (FORMAT PARQUET);\"\n\nviene creata una tabella comuni_pir in un db temporaneo, a partire dal file CSV;\nviene esportato il db in formato parquet (che conterrà una sola tabella), nella directory corrente;\n-c per eseguire i due comandi, separati da ; e poi uscire.\n\n\n\n\n\n\n\nAttenzione all’inferencing dei tipi di campo\n\n\n\nI campi di un file CSV non sono associati a una definizione di tipo di campo. DuckDB in import farà il cosiddetto inferencing, ovvero proverà a dedurlo.Non è detto che lo faccia correttamente ed è bene sempre fare un check (celle con valori come 08, 09, ecc. sono ad esempio spesso mappate come numeri e non come stringhe).\n\n\n\n\n\n Torna in cima"
  },
  {
    "objectID": "til/html-pages-with-quarto/index.html",
    "href": "til/html-pages-with-quarto/index.html",
    "title": "GitHub pages e file HTML in un sito Quarto",
    "section": "",
    "text": "Avevo la necessità di pubblicare un file HTML in un sito Quarto, ma senza che fosse gestito da Quarto. Volevo che il file fosse pubblicato così com’era, tramite GitHub Pages. Il sito era questo e qui il rendering dei file viene fatto nella cartella docs, che fa da sorgente alle GitHub Pages. Di default, se si lancia il rendering del sito, la cartella docs viene cancellata e riscritta. Quindi, se metto un file HTML nella cartella docs, questo viene cancellato.\nPer evitarlo si può usare l’opzione resources in _quarto.toml e specificare quali file e/o cartelle non devono essere cancellati e copiati (in questo caso) nella cartella docs.\nSotto un esempio di configurazione, in cui ho una cartella garage, impostata con un opzione glob (/*/), che fa in modo di pubblicare tutti i contenuti che contiene al suo interno.\nproject:\n  type: website\n  output-dir: docs\n  resources:\n    -  \"garage/*/\"\n\n\n\n Torna in cima"
  },
  {
    "objectID": "til/nushell-installare-supporto-dataframe/index.html",
    "href": "til/nushell-installare-supporto-dataframe/index.html",
    "title": "Nushell: installarlo con il supporto ai dataframe",
    "section": "",
    "text": "Dalla release 0.72 di nushell il supporto ai dataframe non è abilitato di default.\nQuesta una modalità di compilarlo, con il supporto abilitato.\n# clona il repository\ngit clone https://github.com/nushell/nushell.git\n\ncd nushell\n\ncargo install --path=. --all-features\nVerrà installato in /home/username/.cargo/bin/nu.\n\n\n\n Torna in cima"
  },
  {
    "objectID": "til/quarto-applicare-stili-span/index.html",
    "href": "til/quarto-applicare-stili-span/index.html",
    "title": "Quarto: applicare stile CSS",
    "section": "",
    "text": "Se voglio applicare ad esempio a una sola parola uno stile definito inline, basterà fare come sotto:\nL'arancia è [arancione]{style=\"color:#ffa500\"}.\nL’arancia è arancione.\nSe invece voglio associare a una frase uno stile di bootstrap (come quelli sui pulsanti), potrò fare in questo modo:\n[☝️ Partecipa]{.btn .btn-success}\n☝️ Partecipa\nSe infine voglio applicare una determinata classe, seguita da un attributo personalizzato, in modo da poter associare uno stile personalizzato tramite uno specifico CSS Selector, potrò fare così:\n[Lorem ipsum]{.class key=\"val\"}\nIl codice HTML generato sarà:\n&lt;p&gt;\n  &lt;span class=\"class\" data-key=\"val\"&gt;Lorem ipsum&lt;/span&gt;\n&lt;/p&gt;\n\n\n\n Torna in cima"
  },
  {
    "objectID": "til/uv_tips/index.html",
    "href": "til/uv_tips/index.html",
    "title": "uv: installare o lanciare un’applicazione python",
    "section": "",
    "text": "uv è un gestore di pacchetti e strumenti per Python, estremamente veloce e scritto in Rust. È progettato per sostituire strumenti comuni come pip, poetry e virtualenv, offrendo un’interfaccia familiare ma con prestazioni da 10 a 100 volte superiori. uv gestisce le versioni di Python, le dipendenze dei progetti e supporta script singoli con metadati inline, tutto in modo efficiente e scalabile. Disponibile su macOS, Linux e Windows, uv rappresenta una soluzione innovativa per la gestione dei progetti Python.\nPer installare un pacchetto e renderlo disponibile a tutto l’ambiente senza dover attivare un virtualenv, è possibile lanciare questo comando:\nuv tool install nome_pacchetto\nPer lanciarlo invece in modo temporaneo, è possibile usare il comando:\nuvx pycowsay hello from uv\nuvx è un alias del comando uv tool run. In questo caso verrà installato ed eseguito temporaneamente pycowsay, senza doverlo installare in modo permanente.\nSe si vuole aggiungere una dipendenza a un progetto, è possibile farlo con il comando come:\nuv add torch torchvision open-webui\nQuesto comando aggiungerà le dipendenze torch di torchvision al modulo open-webui già installato.\nSe si vuole fare l’update di un pacchetto già installato:\nuv tool upgrade nome_pacchetto\nSe si vuole disinstallare un pacchetto:\nuv tool uninstall nome_pacchetto\n\n\n\n Torna in cima"
  },
  {
    "objectID": "til/fzf_base/index.html",
    "href": "til/fzf_base/index.html",
    "title": "Il meraviglioso fzf",
    "section": "",
    "text": "fzf is an interactive filter program for any kind of list; files, command history, processes, hostnames, bookmarks, git commits, etc. It implements a “fuzzy” matching algorithm, so you can quickly type in patterns with omitted characters and still get the results you want.\n\n\nCTRL+R per cercare nella history dei comandi;\nALT+C per avere la lista delle cartelle nella cartella corrente, selezionarne una e al click fare cd su di essa;\nCTRL+T per avere la lista dei file della cartella corrente;\nALT+P per avere, da dentro fzf, la preview del file selezionato;\nzi in accoppiata con zoxide per navigare velocemente tra le cartelle più “usate”.\n\nA proposito della preview, è comoda questa configurazione, in cui si usa bat per la preview:\nexport FZF_DEFAULT_OPTS=\"--bind 'alt-p:toggle-preview' --preview-window=hidden --preview 'batcat --style=numbers --color=always --line-range :500 {}'\"\nInoltre, quando si scrive un comando si può attivare un auto completamento globale inserendo ** e poi TAB. Qualche esempio:\n\nls ** e poi TAB per avere la lista di tutti i file e cartelle;\ncat ** e poi TAB per avere la lista di tutti i file.\n\nSe vuoi vedere fzf in azione:\n\n\n\n\n Torna in cima"
  },
  {
    "objectID": "posts/accedere-google-ai-riga-di-comando/index.html",
    "href": "posts/accedere-google-ai-riga-di-comando/index.html",
    "title": "Usare la nuova intelligenza artificiale di Google",
    "section": "",
    "text": "😉 Questo post è per Cesare Gerbino."
  },
  {
    "objectID": "posts/accedere-google-ai-riga-di-comando/index.html#introduzione",
    "href": "posts/accedere-google-ai-riga-di-comando/index.html#introduzione",
    "title": "Usare la nuova intelligenza artificiale di Google",
    "section": "Introduzione",
    "text": "Introduzione\nGoogle ha lanciato a inizio dicembre del 2023 Gemini, il suo modello di intelligenza artificiale migliore. Può comprendere e combinare diversi tipi di informazioni, come testo, codice, audio, immagini e video.\nDa poco sono disponibili le API e ho voluto fare qualche test di base, usando la riga di comando.\n\n\n\n\n\n\nNota\n\n\n\nAl momento le API sono accessibili soltanto dagli Stati Uniti, quindi bisogna usare un VPN. Io ho usato quella gratuita di Proton VPN (grazie a Francesco Passantino per il suggerimento di anni fa).\n\n\nA seguire una mini guida per testarle"
  },
  {
    "objectID": "posts/accedere-google-ai-riga-di-comando/index.html#connessione-alla-vpn",
    "href": "posts/accedere-google-ai-riga-di-comando/index.html#connessione-alla-vpn",
    "title": "Usare la nuova intelligenza artificiale di Google",
    "section": "Connessione alla VPN",
    "text": "Connessione alla VPN\nPer prima cosa bisogna connettersi alla VPN e scegliere come paese di connessione gli Stati Uniti.\n\n\n\nEsempio connessione usando Proton VPN"
  },
  {
    "objectID": "posts/accedere-google-ai-riga-di-comando/index.html#generare-una-chiave-api",
    "href": "posts/accedere-google-ai-riga-di-comando/index.html#generare-una-chiave-api",
    "title": "Usare la nuova intelligenza artificiale di Google",
    "section": "Generare una chiave API",
    "text": "Generare una chiave API\nUna volta connessi dagli Stati Uniti è necessario generare una chiave API, per autenticarsi. Si può fare da questa pagina: https://makersuite.google.com/app/apikey.\nUna volta generata - è una lunga stringa - è da archiviare da qualche parte."
  },
  {
    "objectID": "posts/accedere-google-ai-riga-di-comando/index.html#accesso-alle-api-in-rest-via-curl",
    "href": "posts/accedere-google-ai-riga-di-comando/index.html#accesso-alle-api-in-rest-via-curl",
    "title": "Usare la nuova intelligenza artificiale di Google",
    "section": "Accesso alle API in REST, via cURL",
    "text": "Accesso alle API in REST, via cURL\nÈ il modo più immediato e diretto. Si apre la shell e si manda una richiesta come questa, in cui si definisce prima una variabile con la chiave API e poi si lancia la chiamata.\n# Una variabile dove inserire la chiave API\nAPI_KEY=\"AIxxSyCnBOUyPuDLtjWY11HOwxxxxxxx\"\n\n# Lanciare la chiamata\ncurl -s \"https://generativelanguage.googleapis.com/v1beta/models/gemini-pro:generateContent?key=$API_KEY\" \\\n-H 'Content-Type: application/json' \\\n-X POST -d '{\"contents\": [{\"parts\":[{\"text\": \"Creami tre nomi buffi per un gatto siamese con le orecchie molto grandi\"}]}]}'\n\n\n\n\n\n\nQui un esempio di output JSON\n\n\n\n\n\n{\n  \"candidates\": [\n    {\n      \"content\": {\n        \"parts\": [\n          {\n            \"text\": \"1. Dumbo\\n2. Flitzer\\n3. Elicottero\"\n          }\n        ],\n        \"role\": \"model\"\n      },\n      \"finishReason\": \"STOP\",\n      \"index\": 0,\n      \"safetyRatings\": [\n        {\n          \"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\",\n          \"probability\": \"NEGLIGIBLE\"\n        },\n        {\n          \"category\": \"HARM_CATEGORY_HATE_SPEECH\",\n          \"probability\": \"NEGLIGIBLE\"\n        },\n        {\n          \"category\": \"HARM_CATEGORY_HARASSMENT\",\n          \"probability\": \"NEGLIGIBLE\"\n        },\n        {\n          \"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\",\n          \"probability\": \"NEGLIGIBLE\"\n        }\n      ]\n    }\n  ],\n  \"promptFeedback\": {\n    \"safetyRatings\": [\n      {\n        \"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\",\n        \"probability\": \"NEGLIGIBLE\"\n      },\n      {\n        \"category\": \"HARM_CATEGORY_HATE_SPEECH\",\n        \"probability\": \"NEGLIGIBLE\"\n      },\n      {\n        \"category\": \"HARM_CATEGORY_HARASSMENT\",\n        \"probability\": \"NEGLIGIBLE\"\n      },\n      {\n        \"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\",\n        \"probability\": \"NEGLIGIBLE\"\n      }\n    ]\n  }\n}\n\n\n\nSe si espande l’esempio di JSON qui sopra, la parte con la risposta alla chiamata è quella contenuta in .candidates[0].content.parts[0].text. Si può modifcare il comando di sopra e usare jq per estrarla:\ncurl -s \"https://generativelanguage.googleapis.com/v1beta/models/gemini-pro:generateContent?key=$API_KEY\" \\\n-H 'Content-Type: application/json' \\\n-X POST -d '{\"contents\": [{\"parts\":[{\"text\": \"Creami tre nomi buffi per un gatto siamese con le orecchie molto grandi\"}]}]}' | \\\njq '.candidates[0].content.parts[0].text' -r\nIn output si avrà qualcosa come:\n1. Dumbo\n2. Flitzer\n3. Elicottero\nNon vi resta che testare e divertirvi, con esempi migliori del mio. La cosa interessante è che è un’API REST, quindi si può usare da qualsiasi linguaggio di programmazione."
  },
  {
    "objectID": "posts/accedere-google-ai-riga-di-comando/index.html#utilizzare-leccezionale-llm-via-cli",
    "href": "posts/accedere-google-ai-riga-di-comando/index.html#utilizzare-leccezionale-llm-via-cli",
    "title": "Usare la nuova intelligenza artificiale di Google",
    "section": "Utilizzare l’eccezionale LLM via cli",
    "text": "Utilizzare l’eccezionale LLM via cli\nLLM è un’utility a riga di comando e una libreria Python per interagire con Large Language Models (LLM), ovvero modelli di linguaggio avanzati. Permette di utilizzare sia API remote per accedere a modelli ospitati su server esterni, sia modelli installati e eseguiti localmente sul proprio computer.\nEd è possibile quindi usarlo per connettersi con il Large Language Models di Google Gemini.\n🙏 L’autore della cli LLM è quel genio di Simon Willison.\n\nInstallazione\nPer installarlo è sufficiente usare pip:\npip3 install llm\nPer usare Gemini, è necessario instalare il plug-in dedicato, llm-gemini:\nllm install llm-gemini\nO anche\npip3 install llm-gemini\n\n\nUtilizzo\nLa prima cosa da fare è impostare la propria chiave API (quella richiesta sopra). Si apre la shell:\nllm keys set gemini\nSi incolla la chiave API e si preme Invio.\nUna volta fatto, si può testare il funzionamento con un esempio:\nllm -m gemini-pro \"Creami tre nomi buffi per un gatto siamese con le orecchie molto grandi\"\nIn output si avrà qualcosa come:\n- Dumbo\n- Elio\n- Pipistrello\nLa cosa bella è che llm, come tutte le buone cli, può ricevere input dallo stdin e quindi può utilizzare l’output di altri comandi.\nAd esempio l’output di echo:\necho 'Dieci nomi per un blog che parla della riga di comando' | llm -m gemini-pro\nE avrò in output qualcosa come:\n1. Il Comando Centrale\n2. Padronanza del Terminale\n3. Il Mago della Riga di Comando\n4. Il Domatore di Terminale\n5. Il Maestro del Prompt\n6. La Guida alla Riga di Comando\n7. Il Tutorial del Terminale\n8. Il Manuale del Terminale\n9. Le Avventure di un SysAdmin\n10. La Linea di Comando per Tutti\nUn esempio più carino, che mostra le info sul sistema operativo che sto usando adesso per testare Gemini. Il comando da cui parto è uname -a, che mi restituisce:\nLinux MSI 5.15.133.1-microsoft-standard-WSL2 #1 SMP Thu Oct 5 21:02:42 UTC 2023 x86_64 GNU/Linux\nVoglio creare un’informazione più leggile, e la voglio in formato markdown, per scriverla qui (il markdown è il formato in cui è scritto questo sito), e lo chiedo a Gemini, via llm:\nllm -m gemini-pro \"Questo è il mio sistema operativo: $(uname -a). Dimmi qualcosa di più, e dimmelo in markdown.\"\nIn output avrò qualcosa come:\n\nOutput del comandoOutput in HTML\n\n\n* **Kernel**: Linux 5.15.133.1-microsoft-standard-WSL2\n    * Questo è il kernel Linux utilizzato per eseguire WSL2. È una versione modificata del kernel Linux 5.15 che è stato ottimizzato per l'esecuzione in un ambiente Windows.\n* **Nome Host**: MSI\n    * Questo è il nome del computer WSL2.\n* **Versione**: #1 SMP Thu Oct 5 21:02:42 UTC 2023\n    * Questa è la versione della build del kernel Linux utilizzata per eseguire WSL2.\n* **Architettura**: x86_64\n    * Questa è l'architettura del processore del computer WSL2.\n* **Sistema Operativo**: GNU/Linux\n    * Questo è il sistema operativo utilizzato per eseguire WSL2. GNU/Linux è una distribuzione Linux basata sul kernel Linux.\n\n\n\nKernel: Linux 5.15.133.1-microsoft-standard-WSL2\n\nQuesto è il kernel Linux utilizzato per eseguire WSL2. È una versione modificata del kernel Linux 5.15 che è stato ottimizzato per l’esecuzione in un ambiente Windows.\n\nNome Host: MSI\n\nQuesto è il nome del computer WSL2.\n\nVersione: #1 SMP Thu Oct 5 21:02:42 UTC 2023\n\nQuesta è la versione della build del kernel Linux utilizzata per eseguire WSL2.\n\nArchitettura: x86_64\n\nQuesta è l’architettura del processore del computer WSL2.\n\nSistema Operativo: GNU/Linux\n\nQuesto è il sistema operativo utilizzato per eseguire WSL2. GNU/Linux è una distribuzione Linux basata sul kernel Linux.\n\n\n\n\n\n\n\nChattare in modo interattivo\nPer attivare una modalità interattiva, domande e risposte, come una chat, il comando è:\nllm chat -m gemini-pro\nUna volta attivato, si potrà attivare il dialogo in modalità chat (vedi Figura 1).\n\n\n\n\n\n\nFigura 1: llm in modalità chat"
  },
  {
    "objectID": "posts/accedere-google-ai-riga-di-comando/index.html#il-costo-di-gemini-pro",
    "href": "posts/accedere-google-ai-riga-di-comando/index.html#il-costo-di-gemini-pro",
    "title": "Usare la nuova intelligenza artificiale di Google",
    "section": "Il costo di Gemini Pro",
    "text": "Il costo di Gemini Pro\n\nRight now, developers have free access to Gemini Pro and Gemini Pro Vision through Google AI Studio, with up to 60 requests per minute, making it suitable for most app development needs. Vertex AI developers can try the same models, with the same rate limits, at no cost until general availability early next year, after which there will be a charge per 1,000 characters or per image across Google AI Studio and Vertex AI.\n\nFonte: https://blog.google/technology/ai/gemini-api-developers-cloud"
  },
  {
    "objectID": "posts/accedere-google-ai-riga-di-comando/index.html#conclusioni",
    "href": "posts/accedere-google-ai-riga-di-comando/index.html#conclusioni",
    "title": "Usare la nuova intelligenza artificiale di Google",
    "section": "Conclusioni",
    "text": "Conclusioni\nIl bello di questo tipo di accesso, è quello di poter creare in modo diretto e semplici, un utilizzo programmatico di questi strumenti. E la cosa è applicabile alla gran parte dei “Large Language Model” (LLM), ovvero questi tipi di AI che si concentrano sulla comprensione e generazione del linguaggio naturale umano.\nQuesto post ha lo scopo soltanto di farvi due passi - non di più - nel nuovo motore di AI di Google, Gemini. L’utility llm è un gioiellino e consente di fare molto, ma molto di più.\n😉 Su entrambi lascio a chi legge tutti i necessari e divertenti approfondimenti del caso."
  },
  {
    "objectID": "til/mescolare-r-python-bash/index.html",
    "href": "til/mescolare-r-python-bash/index.html",
    "title": "Pagina con codice R, Python e utility Bash",
    "section": "",
    "text": "Ad esempio voglio usare Miller per calcolare la somma di un campo di un file CSV.\nUso system in r, per lanciare un comando di sistema (in questo caso sono in ambiente Linux), e associo l’output a una variabile.\n\n```{r}\nsum &lt;- system('mlr --c2n stats1 -a sum -f a input.csv', intern = TRUE)\n```\n\nCosì facendo posso usare un’opzione comodissima dell’engine knitr, che mi consente di inserire il riferimento a una variabile r (o un comando r) all’interno di un testo markdown.\nSe scrivo ad esempio\n\nLa somma è `r sum`.\n\nAvrò restituito\nLa somma è 9.\nE tramite il package di r reticulate (qui un tutorial a tema), posso passare la variabile r a un blocco di codice python:\n\n```{python}\nsum_py = r.sum\nprint(sum_py)\n```\n\n9\n\n\n\n\n\n Torna in cima"
  },
  {
    "objectID": "til/renderizzare-csv-quarto-observable/index.html",
    "href": "til/renderizzare-csv-quarto-observable/index.html",
    "title": "Quarto: leggere un CSV via Obeservable e visualizzare i dati",
    "section": "",
    "text": "Si può usare semplicemente il metodo FileAttachment di Observable, per data.csv\n```{ojs}\ndata = FileAttachment(\"data.csv\").csv({ typed: true })\nInputs.table(data)\n```\nper ottenere\n\ndata = FileAttachment(\"data.csv\").csv({ typed: true })\nInputs.table(data)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSe vuoi applicare la formattazione della localizzazione italiana, con la , come separatore decimale e il . come separatore delle migliaia, basta modifcarlo in\n```{ojs}\ndata = FileAttachment(\"data.csv\").csv({ typed: true })\nInputs.table(data,{ locale: \"it-IT\" })\n```\nper ottenere\n\nInputs.table(data,{ locale: \"it-IT\" })\n\n\n\n\n\n\nO in alternativa con arquero (ma vale la pena usarlo anche per trasformare i dati)\n\n```{ojs}\n//| echo: fenced\nimport { aq, op } from '@uwdata/arquero'\ndati_aquero = aq.loadCSV(\"data.csv\")\n\ndati_aquero.view()\n```\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNota\n\n\n\nIl Quarto del titolo è lui https://quarto.org/\n\n\n\n\n\n Torna in cima"
  },
  {
    "objectID": "til/quarto-renderizzare-tabelle-r/index.html",
    "href": "til/quarto-renderizzare-tabelle-r/index.html",
    "title": "Quarto: renderizzare una tabella a partire da un CSV",
    "section": "",
    "text": "Per prima cosa carico delle librerie per leggere il CSV e per renderizzare la tabella, e carico la tabella:\n```{r}\nlibrary(tidyverse)\nlibrary(lubridate)\nlibrary(knitr)\n\nt = read_csv(\"input.csv\")\n```\nE poi la renderizzo in vari modi.\n\n```{r}\nkable(t)\n```\n\n\n\n\nyear\ni\nv\n\n\n\n\n2016\nF\n0.9599717\n\n\n2016\nG\n0.0382419\n\n\n2016\nNA\n0.0012658\n\n\n2016\nW\n0.0000122\n\n\n2016\nS\n0.0000454\n\n\n2016\nO\n0.0004631\n\n\n2017\nF\n0.9598036\n\n\n2017\nG\n0.0384042\n\n\n2017\nC\n0.0012674\n\n\n2017\nW\n0.0000153\n\n\n2017\nS\n0.0000486\n\n\n2017\nO\n0.0004608\n\n\n2018\nF\n0.9598013\n\n\n\n\n\n\n```{r}\n#| tbl-cap: \"Soltanto le prime righe\"\nkable(head(t))\n```\n\n\nSoltanto le prime righe\n\n\nyear\ni\nv\n\n\n\n\n2016\nF\n0.9599717\n\n\n2016\nG\n0.0382419\n\n\n2016\nNA\n0.0012658\n\n\n2016\nW\n0.0000122\n\n\n2016\nS\n0.0000454\n\n\n2016\nO\n0.0004631\n\n\n\n\n\n\n```{r}\n#| tbl-cap: \"Selezionare righe e colonne\"\nkable(t[1:4, 1:2])\n```\n\n\nSelezionare righe e colonne\n\n\nyear\ni\n\n\n\n\n2016\nF\n\n\n2016\nG\n\n\n2016\nNA\n\n\n2016\nW\n\n\n\n\n\n\n```{r}\n#| tbl-cap: \"Paginazione\"\n\nrmarkdown::paged_table(t)\n```\n\n Paginazione\n  \n\n\n\n\n\n\n Torna in cima"
  }
]